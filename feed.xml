<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml" rel="self" type="application/atom+xml" /><link href="https://davidrpugh.github.io/stochastic-expatriate-descent/" rel="alternate" type="text/html" /><updated>2020-04-09T08:24:41-05:00</updated><id>https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml</id><title type="html">Stochastic Expatriate Descent</title><subtitle>An expat scientist's musings about machine learning, deep learning, and living abroad.</subtitle><entry><title type="html">Getting familiar with deep Q-networks</title><link href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html" rel="alternate" type="text/html" title="Getting familiar with deep Q-networks" /><published>2020-04-03T00:00:00-05:00</published><updated>2020-04-03T00:00:00-05:00</updated><id>https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks</id><content type="html" xml:base="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-03-deep-q-networks.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Colab-specific-environment-setup&quot;&gt;Colab specific environment setup&lt;a class=&quot;anchor-link&quot; href=&quot;#Colab-specific-environment-setup&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install gym&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;box2d&lt;span class=&quot;o&quot;&gt;]==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;.17.*
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;matplotlib&lt;/span&gt; inline
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Motivation&quot;&gt;Motivation&lt;a class=&quot;anchor-link&quot; href=&quot;#Motivation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I am currently using my COVID-19 imposed quarantine to expand my deep learning skills by completing the &lt;a href=&quot;https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893&quot;&gt;&lt;em&gt;Deep Reinforcement Learning Nanodegree&lt;/em&gt;&lt;/a&gt; from &lt;a href=&quot;https://www.udacity.com/&quot;&gt;Udacity&lt;/a&gt;. This past week I have been working my way through the seminal 2015 &lt;em&gt;Nature&lt;/em&gt; paper from researchers at &lt;a href=&quot;https://deepmind.com/&quot;&gt;Deepmind&lt;/a&gt; &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&quot;&gt;&lt;em&gt;Human-level control through deep reinforcement learning&lt;/em&gt;&lt;/a&gt; (Minh et al 2015).&lt;/p&gt;
&lt;h3 id=&quot;Why-is-Minh-et-al-2015-important?&quot;&gt;Why is Minh et al 2015 important?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-is-Minh-et-al-2015-important?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;While Minh et al 2015 was not the first paper to use neural networks to approximate the action-value function, this paper was the first to demonstrate that the same neural network architecture could be trained in a computationally efficient manner to &quot;solve&quot; a large number or different tasks.&lt;/p&gt;
&lt;p&gt;The paper also contributed several practical &quot;tricks&quot; for getting deep neural networks to consistently converge during training. This was a non-trivial contribution as issues with training convergence had plaugued previous attempts to use neural networks as function approximators in reinforcement learning tasks and were blocking widespread adoption of deep learning techniques within the reinforcemnt learning community.&lt;/p&gt;
&lt;h2 id=&quot;Summary-of-the-paper&quot;&gt;Summary of the paper&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary-of-the-paper&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Minh et al 2015 uses deep (convolutional) neural network to approximate the optimal action-value function&lt;/p&gt;
&lt;p&gt;
$$ Q^*(s, a) = \max_{\pi} \mathbb{E}\Bigg[\sum_{s=0}^{\infty} \gamma^s r_{t+s} | s_t=s, a_t=a, \pi \Bigg] $$
&lt;/p&gt;
&lt;p&gt;which is the maximum sum of rewards $r_t$ discounted by $\gamma$ at each time-step $t$ achievable by a behaviour policy $\pi = P(a|s)$, after making an observation of the state $s$ and taking an action $a$.&lt;/p&gt;
&lt;p&gt;Prior to this seminal paper it was well known that standard reinforcement learning algorithms were unstable or even diverged when a non-linear function approximators such as a neural networks were used to represent the action-value function $Q$. Why?&lt;/p&gt;
&lt;p&gt;Minh et al 2015 discuss several reasons.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Correlations present in the sequence of observations of the state $s$. In reinforcement learning applications the sequence state observations is a time-series which will almost surely be auto-correlated. But surely this would also be true of any application of deep neural networks to model time series data. &lt;/li&gt;
&lt;li&gt;Small updates to $Q$ may significantly change the policy, $\pi$ and therefore change the data distribution.&lt;/li&gt;
&lt;li&gt;Correlations between the action-values, $Q$, and the target values $r + \gamma \max_{a'} Q(s', a')$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the paper the authors address these issues by using...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a biologically inspired mechanism they refer to as &lt;em&gt;experience replay&lt;/em&gt; that randomizes over the data which removes correlations in the sequence of observations of the state $s$ and smoothes over changes in the data distribution (issues 1 and 2 above).&lt;/li&gt;
&lt;li&gt;an iterative update rule that adjusts the action-values, $Q$, towards target values, $Q'$ that are only periodically updated thereby reducing correlations with the target (issue 3 above).&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Approximating-the-action-value-function,-$Q(s,a)$&quot;&gt;Approximating the action-value function, $Q(s,a)$&lt;a class=&quot;anchor-link&quot; href=&quot;#Approximating-the-action-value-function,-$Q(s,a)$&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;There are several possible ways of approximating the action-value function $Q$ using a neural network. The only input to the DQN architecture is the state representation and the output layer has a separate output for each possible action. The output units correspond to the predicted $Q$-values of the individual actions for the input state. A representaion of the DQN architecture from the paper is reproduced in the figure below.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
   &lt;img alt=&quot;Deep Q-Network Architecture&quot; src=&quot;assets/img/q-network-architecture.jpg&quot; width=&quot;750&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map $\phi$. The first hidden layer combines a convolutional layer with 32 filters (each of which uses an 8 x 8 kernel and a stride of 4) and a ReLU activation function. The second hidden layer convolves 64 filters (each of which using a 4 x 4 kernel with stride of 2) followed by a ReLU activation function. This is followed by a third convolutional layer that combines 64 filters (each of which uses a 3 x 3 kernel and a stride of 1) with yet anotehr ReLU activation. The final hidden layer is fully-connected (i.e., dense) linear layer with 512 neurons followed by a ReLU activation. The output layer is a fully-connected layer with a single output for each action.&lt;/p&gt;
&lt;p&gt;A PyTorch implementation of the DQN architecture would look something like the following.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;deep_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;deep_q_network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deep_q_network&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;The-Loss-Function&quot;&gt;The Loss Function&lt;a class=&quot;anchor-link&quot; href=&quot;#The-Loss-Function&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The $Q$-learning update at iteration $i$ uses the following loss function&lt;/p&gt;
&lt;p&gt;
$$ \mathcal{L_i}(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Bigg[\bigg(r + \gamma \max_{a'} Q\big(s', a'; \theta_i^{-}\big) - Q\big(s, a; \theta_i\big)\bigg)^2\Bigg] $$
&lt;/p&gt;
&lt;p&gt;where $\gamma$ is the discount factor determining the agentâ€™s horizon, $\theta_i$ are the parameters of the $Q$-network at iteration $i$ and $\theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $\theta_i^{-}$ are only updated with the $Q$-network parameters $\theta_i$ every $C$ steps and are held fixed between individual updates.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Experience-Replay&quot;&gt;Experience Replay&lt;a class=&quot;anchor-link&quot; href=&quot;#Experience-Replay&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To perform &lt;em&gt;experience replay&lt;/em&gt; the authors store the agent's experiences $e_t$ as represented by the tuple&lt;/p&gt;
&lt;p&gt;
$$ e_t = (s_t, a_t, r_t, s_{t+1}) $$
&lt;/p&gt;
&lt;p&gt;consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences.&lt;/p&gt;
&lt;p&gt;
$$ D_t = \{e1, e2, ..., e_t \} $$
&lt;/p&gt;
&lt;p&gt;Depending on the task it may note be feasible for the agent to store the entire history of past experiences.&lt;/p&gt;
&lt;p&gt;During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s')$, drawn uniformly at random from the pool of stored samples $D_t$.&lt;/p&gt;
&lt;p&gt;The following is my Python implmentation of these ideas.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_field_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;state&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;action&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;reward&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;next_state&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;done&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Experience&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;namedtuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Experience&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;field_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_field_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ExperienceReplayBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Fixed-size buffer to store experience tuples.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Initialize an ExperienceReplayBuffer object.&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;        Parameters:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        -----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        buffer_size (int): maximum size of buffer&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        batch_size (int): size of each training batch&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        seed (int): random seed&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deque&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_random_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;nd&quot;&gt;@property&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_batch_size&lt;/span&gt;
    
    &lt;span class=&quot;nd&quot;&gt;@property&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;buffer_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer_size&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer_size&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Add a new experience to memory.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;typing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Randomly sample a batch of experiences from memory.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;The-Deep-Q-Network-Algorithm&quot;&gt;The Deep Q-Network Algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#The-Deep-Q-Network-Algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The following is Python pseudo-code for the Deep Q-Network (DQN) algorithm. For more fine-grained details of the DQN algorithm see the methods section of &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&quot;&gt;Minh et al 2015&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# hyper-parameters&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# number of experience tuples used in computing the gradient descent parameter update.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# number of experience tuples stored in the replay buffer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# discount factor used in the Q-learning update&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_network_update_frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# frequency (measured in parameter updates) with which target network is updated.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;update_frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# frequency (measured in number of timesteps) with which q-network parameters are updated.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# initilizing the various data structures&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;replay_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExperienceReplayBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;local_q_network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_q_network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;synchronize_q_networks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# initialize the environment state&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# simulate a single training episode&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parameter_updates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# greedy action based on Q(s, a; theta)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choose_epsilon_greedy_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

        &lt;span class=&quot;c1&quot;&gt;# update the environment based on the chosen action&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# agent records experience in its replay buffer&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replay_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# agent samples a mini-batch of experiences from its replay buffer&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replay_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# agent learns every update_frequency timesteps&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# compute the Q^- values using the Q-learning formula&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;target_q_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_learning_update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# compute the Q values&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;local_q_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# agent updates the parameters theta using gradient descent&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_q_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_q_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;gradient_descent_update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;parameter_updates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# every target_network_update_frequency timesteps set theta^- = theta&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameter_updates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_network_update_frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;synchronize_q_networks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Solving-the-LunarLander-v2-environment&quot;&gt;Solving the &lt;code&gt;LunarLander-v2&lt;/code&gt; environment&lt;a class=&quot;anchor-link&quot; href=&quot;#Solving-the-LunarLander-v2-environment&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the rest of this blog post I will use the DQN algorithm to train an agent to solve the &lt;a href=&quot;https://gym.openai.com/envs/LunarLander-v2/&quot;&gt;LunarLander-v2&lt;/a&gt; environment from &lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land &quot;gracefully&quot; (and not crash in the landing area!).&lt;/p&gt;
&lt;p&gt;A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered &quot;solved&quot; if the lander is able to achieve 200 points (I will actually be more stringent and define &quot;solved&quot; as achieving over 200 points on average in the most recent 100 training episodes).&lt;/p&gt;
&lt;h3 id=&quot;Action-Space&quot;&gt;Action Space&lt;a class=&quot;anchor-link&quot; href=&quot;#Action-Space&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;There are four discrete actions available:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Do nothing.&lt;/li&gt;
&lt;li&gt;Fire the left orientation engine.&lt;/li&gt;
&lt;li&gt;Fire main engine.&lt;/li&gt;
&lt;li&gt;Fire the right orientation engine.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;LunarLander-v2&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;/Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: &lt;span class=&quot;ansi-yellow-fg&quot;&gt;WARN: Box bound precision lowered by casting to float32&lt;/span&gt;
  warnings.warn(colorize(&amp;#39;%s: %s&amp;#39;%(&amp;#39;WARN&amp;#39;, msg % args), &amp;#39;yellow&amp;#39;))
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Defining-a-generic-Agent-and-train-loop&quot;&gt;Defining a generic &lt;code&gt;Agent&lt;/code&gt; and &lt;code&gt;train&lt;/code&gt; loop&lt;a class=&quot;anchor-link&quot; href=&quot;#Defining-a-generic-Agent-and-train-loop&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In the cell below I define a fairly generic training loop for training and &lt;code&gt;Agent&lt;/code&gt; to solve a task in a given &lt;code&gt;gym.Env&lt;/code&gt; environment. In working through the hands-on portions of the &lt;a href=&quot;https://www.udacity.com/&quot;&gt;Udacity&lt;/a&gt; &lt;a href=&quot;https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893&quot;&gt;&lt;em&gt;Deep Reinforcement Learning Nanodegree&lt;/em&gt;&lt;/a&gt; I found myself writing similar code over and over again to train the agent to solve a task. This is my first attempt to write something that I might be able to reuse on the course going forward.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;choose_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Rule for choosing an action given the current state of the environment.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;NotImplementedError&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Save any important agent state to a file.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;NotImplementedError&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Update agent&amp;#39;s state after observing the effect of its action on the environment.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NotImplmentedError&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_train_for_at_most&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Train agent for a maximum number of timesteps.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choose_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;

                
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_train_until_done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Train the agent until the current episode is complete.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choose_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;checkpoint_filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;target_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;number_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;maximum_timesteps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;typing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Reinforcement learning training loop.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Parameters:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    -----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    agent (Agent): an agent to train.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    env (gym.Env): an environment in which to train the agent.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    checkpoint_filepath (str): filepath used to save the state of the trained agent.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    number_episodes (int): maximum number of training episodes.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    maximum_timsteps (int): maximum number of timesteps per episode.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Returns:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    --------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    scores (list): collection of episode scores from training.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;most_recent_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deque&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maximum_timesteps&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_train_until_done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_train_for_at_most&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maximum_timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;         
        &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;most_recent_scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;average_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_recent_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_recent_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;average_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Environment solved in &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; episodes!&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Average Score: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average_score&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.2f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Episode &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Average Score: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average_score&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.2f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Creating-a-DeepQAgent&quot;&gt;Creating a &lt;code&gt;DeepQAgent&lt;/code&gt;&lt;a class=&quot;anchor-link&quot; href=&quot;#Creating-a-DeepQAgent&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The code in the cell below encapsulates much of the logic of the DQN algorithm in a &lt;code&gt;DeepQAgent&lt;/code&gt; class. Since the &lt;code&gt;LunarLander-v2&lt;/code&gt; task is not well suited for convolutional neural networks, the agent uses a simple three layer dense neural network with ReLU activation functions to approximate the action-value function $Q$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DeepQAgent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;state_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;optimizer_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;typing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Callable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;typing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Iterable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;epsilon_decay_schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;typing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Callable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;update_frequency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Initialize a DeepQAgent.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Parameters:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        -----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        state_size (int): the size of the state space.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        action_size (int): the size of the action space.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        number_hidden_units (int): number of units in the hidden layers.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        batch_size (int): number of experience tuples in each mini-batch.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        buffer_size (int): maximum number of experience tuples stored in the replay buffer.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        epsilon_decay_schdule (callable): function that takes episode number and returns epsilon.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        alpha (float): rate at which the target q-network parameters are updated.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        gamma (float): Controls how much that agent discounts future rewards (0 &amp;lt; gamma &amp;lt;= 1).&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        seed (int): random seed&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_state_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_action_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cuda&amp;quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# set seeds for reproducibility&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_random_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backends&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cudnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backends&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cudnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# initialize agent hyperparameters&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_experience_replay_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExperienceReplayBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_epsilon_decay_schedule&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon_decay_schedule&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# initialize Q-Networks&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_frequency&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_initialize_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_target_q_network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_initialize_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_synchronize_q_networks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# send the networks to the device&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_target_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# initialize the optimizer&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# initialize some counters&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_number_episodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_number_timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_number_parameter_updates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_initialize_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Create a neural network for approximating the action-value function.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;q_network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_state_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_network&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_learn_from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;typing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Heart of the Deep Q-learning algorithm.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vs&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# get max predicted Q values (for next states) from target model&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;next_target_q_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_target_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                                       &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                                       &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# compute the new Q&amp;#39; values using the Q-learning formula&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;target_q_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_target_q_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# get expected Q values from local model&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                         &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;expected_q_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                                 &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# compute the mean squared loss&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_q_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_q_values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# agent updates the parameters theta of Q using gradient descent&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_soft_update_target_q_network_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                 
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_soft_update_target_q_network_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Soft-update of target q-network parameters with the local q-network parameters.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_target_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;target_param&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_param&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_param&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_synchronize_q_networks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Synchronize the target_q_network and the local_q_network.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_target_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
           
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_uniform_random_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Choose an action uniformly at random.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_greedy_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Choose an action that maximizes the action_values given the current state.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# evaluate the network to compute the action values&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;action_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# choose the greedy action&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# action_values might reside on the GPU!&lt;/span&gt;
                               &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                               &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_epsilon_greedy_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_uniform_random_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_greedy_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;choose_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Return the action for given state as per current policy.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Parameters:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        -----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        state (np.array): current state of the environment.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Return:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        --------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        action (int): an integer representing the chosen action.&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# need to reshape state array and convert to tensor&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                             &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                             &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            
        &lt;span class=&quot;c1&quot;&gt;# choose uniform at random if agent has insufficient experience&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;has_sufficient_experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_uniform_random_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_epsilon_decay_schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_number_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_epsilon_greedy_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;has_sufficient_experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;True if agent has enough experience to train on a batch of samples; False otherwise.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_experience_replay_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_experience_replay_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Saves the state of the DeepQAgent.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Parameters:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        -----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        filepath (str): filepath where the serialized state should be saved.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Notes:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        ------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        The method uses `torch.save` to serialize the state of the q-network, &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        the optimizer, as well as the dictionary of agent hyperparameters.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;checkpoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;q-network-state&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_q_network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;optimizer-state&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;agent-hyperparameters&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;alpha&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;batch_size&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_experience_replay_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;buffer_size&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_experience_replay_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buffer_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;gamma&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;update_frequency&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_frequency&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Updates the agent&amp;#39;s state based on feedback received from the environment.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Parameters:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        -----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        state (np.array): the previous state of the environment.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        action (int): the action taken by the agent in the previous state.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        reward (float): the reward received from the environment.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        next_state (np.array): the resulting state of the environment following the action.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        done (bool): True is the training episode is finised; false otherwise.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# save experience in the experience replay buffer&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_experience_replay_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_number_episodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_number_timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            
            &lt;span class=&quot;c1&quot;&gt;# every so often the agent should learn from experiences&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_number_timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;has_sufficient_experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_experience_replay_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_learn_from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Epsilon-decay-schedule&quot;&gt;Epsilon decay schedule&lt;a class=&quot;anchor-link&quot; href=&quot;#Epsilon-decay-schedule&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In the DQN algorithm the agent chooses its action using an $\epsilon$-greedy policy. When using an $\epsilon$-greedy policy, with probability $\epsilon$, the agent explores the state space by choosing an action uniformly at random from the set of feasible actions; with probability $1-\epsilon$, the agent exploits its current knowledge by choosing the optimal action given that current state.&lt;/p&gt;
&lt;p&gt;As the agent learns and acquires additional knowledge about it environment it makes sense to &lt;em&gt;decrease&lt;/em&gt; exploration and &lt;em&gt;increase&lt;/em&gt; exploitation by decreasing $\epsilon$. In practice, it isn't a good idea to decrease $\epsilon$ to zero; instead one typically decreases $\epsilon$ over time according to some schedule until it reaches some minimum value.&lt;/p&gt;
&lt;p&gt;The Deepmind researchers used a simple linear decay schedule and set a minimum value of $\epsilon=0.1$. In the cell below I code up a linear decay schedule as well as a power decay schedule that I have seen used in many other practical applications.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear_decay_schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episode_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;minimum_epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Simple linear decay schedule used in the Deepmind paper.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;episode_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimum_epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;power_decay_schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episode_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;decay_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;minimum_epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Power decay schedule found in other practical applications.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decay_factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episode_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimum_epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;_epsilon_decay_schedule_kwargs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;decay_factor&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.995&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;minimum_epsilon&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epsilon_decay_schedule&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;power_decay_schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_epsilon_decay_schedule_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Choosing-an-optimizer&quot;&gt;Choosing an optimizer&lt;a class=&quot;anchor-link&quot; href=&quot;#Choosing-an-optimizer&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;As is the case in training any neural network, the choice of optimizer and the tuning of its hyper-parameters (in particular the learning rate) is important. Here I am going to more or less follow the Minh et al 2015 paper and use the &lt;a href=&quot;https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop&quot;&gt;RMSProp&lt;/a&gt; optimizer.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_optimizer_kwargs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;lr&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;alpha&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;eps&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;weight_decay&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;momentum&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;centered&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RMSprop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_optimizer_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;At this point I am ready to create an instance of the &lt;code&gt;DeepQAgent&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_agent_kwargs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;state_size&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;action_size&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;number_hidden_units&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;optimizer_fn&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;epsilon_decay_schedule&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon_decay_schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;batch_size&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;buffer_size&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;alpha&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;gamma&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;update_frequency&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;seed&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deep_q_agent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DeepQAgent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_agent_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Training-the-DeepQAgent&quot;&gt;Training the &lt;code&gt;DeepQAgent&lt;/code&gt;&lt;a class=&quot;anchor-link&quot; href=&quot;#Training-the-DeepQAgent&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Now I am finally ready to train the &lt;code&gt;deep_q_agent&lt;/code&gt;. The target score for the &lt;code&gt;LunarLander-v2&lt;/code&gt; environment is 200 points on average for at least 100 consecutive episodes. If the &lt;code&gt;deep_q_agent&lt;/code&gt; is able to &quot;solve&quot; the environment, then training will terminate early.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deep_q_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;checkpoint.pth&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;number_episodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Episode 100	Average Score: -174.38
Episode 200	Average Score: -101.55
Episode 300	Average Score: -110.48
Episode 400	Average Score: -62.51
Episode 500	Average Score: -28.77
Episode 600	Average Score: 56.29
Episode 700	Average Score: 132.25
Episode 800	Average Score: 102.05
Episode 900	Average Score: 88.39
Episode 1000	Average Score: 103.97
Episode 1100	Average Score: 148.96
Episode 1200	Average Score: 167.50
Episode 1300	Average Score: 175.88
Episode 1400	Average Score: 156.95
Episode 1500	Average Score: 183.58
Episode 1600	Average Score: 184.38
Episode 1700	Average Score: 142.57
Episode 1800	Average Score: 155.47

Environment solved in 1886 episodes!	Average Score: 202.91
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Analyzing-DeepQAgent-performance&quot;&gt;Analyzing &lt;code&gt;DeepQAgent&lt;/code&gt; performance&lt;a class=&quot;anchor-link&quot; href=&quot;#Analyzing-DeepQAgent-performance&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;Plotting-the-time-series-of-scores&quot;&gt;Plotting the time series of scores&lt;a class=&quot;anchor-link&quot; href=&quot;#Plotting-the-time-series-of-scores&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;I can use &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;Pandas&lt;/a&gt; to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;scores&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;count    1887.000000
mean       79.280025
std       182.558296
min      -629.814823
25%       -57.980604
50%        62.985922
75%       251.261991
max       314.992618
Name: scores, dtype: float64&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Scores&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rolling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
           &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Rolling Average&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linestyle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;dashed&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Target Score&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Episode Number&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Score&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Kernel-density-plot-of-the-scores&quot;&gt;Kernel density plot of the scores&lt;a class=&quot;anchor-link&quot; href=&quot;#Kernel-density-plot-of-the-scores&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Kernel density plot of scores is bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent &quot;solved&quot; the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;kde&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Score&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Where-to-go-from-here?&quot;&gt;Where to go from here?&lt;a class=&quot;anchor-link&quot; href=&quot;#Where-to-go-from-here?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I am a bit frustrated by lack of stability that I am seeing in my implmentation of the Deep Q algorithm: sometimes the algorithm converges and sometimes not. Perhaps more tuning of hyper-parameters or use of a different optimization algorithm would exhibit better convergence. I have already spent more time than I had allocated on playing around with this agorithm so I am not going to try and fine-tune the hyperparamters or explore alternative optimization algorithms for now.&lt;/p&gt;
&lt;p&gt;Rather than spending time tuning hyperparameters I think it would be better use of my time to explore algorithmic improvements. In future posts I plan to cover the following extensions of the DQN algorithm: &lt;a href=&quot;https://arxiv.org/abs/1509.06461&quot;&gt;Double Q-Learning&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1509.06461&quot;&gt;Prioritized Experience Replay&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1511.06581&quot;&gt;Dueling Network Architectures&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>David R. Pugh</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/q-network-architecture.jpg" /><media:content medium="image" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/q-network-architecture.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Conda (+ pip) and Docker FTW!</title><link href="https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html" rel="alternate" type="text/html" title="Conda (+ pip) and Docker FTW!" /><published>2020-03-31T00:00:00-05:00</published><updated>2020-03-31T00:00:00-05:00</updated><id>https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker</id><content type="html" xml:base="https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html">&lt;h1 id=&quot;conda--pip-and-docker-ftw&quot;&gt;Conda (+ pip) and Docker FTW!&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.conda.io/en/latest/&quot;&gt;Conda&lt;/a&gt; is an open source package and 
environment management system that runs on Windows, Mac OS and Linux.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Conda can quickly install, run, and update packages and their dependencies.&lt;/li&gt;
  &lt;li&gt;Conda can create, save, load, and switch between project specific software 
environments on your local computer.&lt;/li&gt;
  &lt;li&gt;Although Conda was created for Python programs, Conda can package and 
distribute software for any language such as R, Ruby, Lua, Scala, Java, 
JavaScript, C, C++, FORTRAN.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Conda as a package manager helps you find and install packages. If you need a 
package that requires a different version of Python, you do not need to switch 
to a different environment manager, because Conda is also an environment 
manager. With just a few commands, you can set up a totally separate 
environment to run that different version of Python, while continuing to run 
your usual version of Python in your normal environment.&lt;/p&gt;

&lt;p&gt;While Conda is my default package and environment management solution, not 
every Python package that I might need to use is available via Conda. 
Fortunately, Conda plays nicely with &lt;a href=&quot;https://pip.pypa.io/en/stable/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pip&lt;/code&gt;&lt;/a&gt; 
which is the default Python package management tool.&lt;/p&gt;

&lt;h2 id=&quot;why-not-just-use-conda--pip&quot;&gt;Why not just use Conda (+ pip)?&lt;/h2&gt;

&lt;p&gt;While Conda (+ pip) solves most of my day-to-day data science environment and 
package management issues, incorporating &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; 
into my Conda (+ pip) development workflow has made it much easier to port my 
data science workflows from from my laptop/workstation to remote cloud 
computing resources. Getting Conda (+ pip) to work as expected inside Docker 
containers turned out to be much more challenging that I expected.&lt;/p&gt;

&lt;p&gt;This blog post shows how I eventually combined Conda (+ pip) and Docker. 
In the following I assume that you have organized your project directory 
similar to my 
&lt;a href=&quot;https://github.com/kaust-vislab/python-data-science-project&quot;&gt;Python data science project template&lt;/a&gt;. 
In particular, I will assume that you store all Docker related files in a 
&lt;code class=&quot;highlighter-rouge&quot;&gt;docker&lt;/code&gt; sub-directory within your project root directory.&lt;/p&gt;

&lt;h2 id=&quot;writing-the-dockerfile&quot;&gt;Writing the &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;The trick to getting Conda (+ pip) and Docker to work smoothly together is to 
write a good &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;. In this section I will take you step by step 
through the various pieces of the &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; that I developed. Hopefully you 
can use this &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; without modification on you next data science 
project.&lt;/p&gt;

&lt;h3 id=&quot;use-a-standard-base-image&quot;&gt;Use a standard base image&lt;/h3&gt;

&lt;p&gt;Every &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockefile&lt;/code&gt; has a base or parent image. For the parent image I use 
&lt;a href=&quot;http://releases.ubuntu.com/16.04/&quot;&gt;Ubuntu 16.04&lt;/a&gt; 
which is one of the most commonly used flavor of Linux in the data science 
community (and also happens to be the same OS installed on my workstation).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:16.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;make-bash-the-default-shell&quot;&gt;Make &lt;code class=&quot;highlighter-rouge&quot;&gt;bash&lt;/code&gt; the default shell&lt;/h3&gt;

&lt;p&gt;The default shell used to run &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; commands when building Docker 
images is &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/sh&lt;/code&gt;. Unfortunately &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/sh&lt;/code&gt; is currently not one of the shells 
supported by the &lt;code class=&quot;highlighter-rouge&quot;&gt;conda init&lt;/code&gt; command. Fortunately it is possible to change the 
default shell used to run &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; commands using the 
&lt;a href=&quot;https://docs.docker.com/engine/reference/builder/#shell&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SHELL&lt;/code&gt;&lt;/a&gt; instruction.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SHELL [ &quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the use of the &lt;code class=&quot;highlighter-rouge&quot;&gt;--login&lt;/code&gt; flag which insures that both &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.profile&lt;/code&gt; and 
&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt; are sourced properly. Proper sourcing of both &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.profile&lt;/code&gt; and 
&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt; is necessary in order to use various &lt;code class=&quot;highlighter-rouge&quot;&gt;conda&lt;/code&gt; commands to build the 
Conda environment inside the Docker image.&lt;/p&gt;

&lt;h3 id=&quot;create-a-non-root-user&quot;&gt;Create a non-root user&lt;/h3&gt;

&lt;p&gt;It is a 
&lt;a href=&quot;https://snyk.io/blog/10-docker-image-security-best-practices/&quot;&gt;Docker security â€œbest practiceâ€&lt;/a&gt; 
to create a non-root user inside your Docker images. My preferred approach to 
create a non-root user uses build arguments to customize the &lt;code class=&quot;highlighter-rouge&quot;&gt;username&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;uid&lt;/code&gt;, 
and &lt;code class=&quot;highlighter-rouge&quot;&gt;gid&lt;/code&gt;the non-root user. I use standard defaults for the &lt;code class=&quot;highlighter-rouge&quot;&gt;uid&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;gid&lt;/code&gt;; 
the default username is set to 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Muhammad_ibn_Musa_al-Khwarizmi&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;al-khawarizmi&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Create a non-root user
ARG username=al-khawarizmi
ARG uid=1000
ARG gid=100
ENV USER $username
ENV UID $uid
ENV GID $gid
ENV HOME /home/$USER

RUN adduser --disabled-password \
    --gecos &quot;Non-root user&quot; \
    --uid $UID \
    --gid $GID \
    --home $HOME \
    $USER
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;copy-over-the-config-files-for-the-conda-environment&quot;&gt;Copy over the config files for the Conda environment&lt;/h3&gt;

&lt;p&gt;After creating the non-root user I copy over all of the config files that I 
will need to create the Conda environment (i.e., &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt;, 
&lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;postBuild&lt;/code&gt;). I also copy over a Bash script that I will 
use as the Docker &lt;code class=&quot;highlighter-rouge&quot;&gt;ENTRYPOINT&lt;/code&gt; (more on this below).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;COPY environment.yml requirements.txt /tmp/
RUN chown $UID:$GID /tmp/environment.yml /tmp/requirements.txt

COPY postBuild /usr/local/bin/postBuild.sh
RUN chown $UID:$GID /usr/local/bin/postBuild.sh &amp;amp;&amp;amp; \
    chmod u+x /usr/local/bin/postBuild.sh

COPY docker/entrypoint.sh /usr/local/bin/
RUN chown $UID:$GID /usr/local/bin/entrypoint.sh &amp;amp;&amp;amp; \
    chmod u+x /usr/local/bin/entrypoint.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Newer versions of Docker support copying files as a non-root user, however 
the version of Docker available on DockerHub does not yet support copying 
as a non-root user so if you want to setup 
&lt;a href=&quot;https://docs.docker.com/docker-hub/builds/&quot;&gt;automated builds&lt;/a&gt; for your Git 
repositories you will need to copy everything as root.&lt;/p&gt;

&lt;h3 id=&quot;install-miniconda-as-the-non-root-user&quot;&gt;Install Miniconda as the non-root user.&lt;/h3&gt;

&lt;p&gt;After copying over the config files as root, I switch over to the non-root 
user and install &lt;a href=&quot;https://docs.conda.io/en/latest/miniconda.html&quot;&gt;Miniconda&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;USER $USER

# install miniconda
ENV MINICONDA_VERSION 4.8.2
ENV CONDA_DIR $HOME/miniconda3
RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-$MINICONDA_VERSION-Linux-x86_64.sh -O ~/miniconda.sh &amp;amp;&amp;amp; \
    chmod +x ~/miniconda.sh &amp;amp;&amp;amp; \
    ~/miniconda.sh -b -p $CONDA_DIR &amp;amp;&amp;amp; \
    rm ~/miniconda.sh

# make non-activate conda commands available
ENV PATH=$CONDA_DIR/bin:$PATH

# make conda activate command available from /bin/bash --login shells
RUN echo &quot;. $CONDA_DIR/etc/profile.d/conda.sh&quot; &amp;gt;&amp;gt; ~/.profile

# make conda activate command available from /bin/bash --interative shells
RUN conda init bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;create-a-project-directory&quot;&gt;Create a project directory&lt;/h3&gt;

&lt;p&gt;Next I create a project directory inside the non-root user home directory. The 
Conda environment will be created in a &lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt; sub-directory inside the project 
directory and all other project files and directories can then be mounted into 
this directory.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# create a project directory inside user home
ENV PROJECT_DIR $HOME/app
RUN mkdir $PROJECT_DIR
WORKDIR $PROJECT_DIR
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;build-the-conda-environment&quot;&gt;Build the Conda environment&lt;/h3&gt;

&lt;p&gt;Now I am ready to build the Conda environment. Note that I can use nearly the 
same sequence of &lt;code class=&quot;highlighter-rouge&quot;&gt;conda&lt;/code&gt; commands that I would use to build a Conda environment 
for a project on my laptop or workstation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# build the conda environment
ENV ENV_PREFIX $PWD/env
RUN conda update --name base --channel defaults conda &amp;amp;&amp;amp; \
    conda env create --prefix $ENV_PREFIX --file /tmp/environment.yml --force &amp;amp;&amp;amp; \
    conda clean --all --yes

# run the postBuild script to install any JupyterLab extensions
RUN conda activate $ENV_PREFIX &amp;amp;&amp;amp; \
    /usr/local/bin/postBuild.sh &amp;amp;&amp;amp; \
    conda deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;insure-conda-environment-is-properly-activated-at-runtime&quot;&gt;Insure Conda environment is properly activated at runtime&lt;/h3&gt;

&lt;p&gt;Almost finished! Second to last step is to use an 
&lt;a href=&quot;https://docs.docker.com/engine/reference/builder/#entrypoint&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ENTRYPOINT&lt;/code&gt;&lt;/a&gt; 
script to insure that the Conda environment is properly activated at runtime.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ENTRYPOINT [ &quot;/usr/local/bin/entrypoint.sh&quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/bin/entrypoint.sh&lt;/code&gt; script for reference.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash --login&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;

conda activate &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$@&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;specify-a-default-command-for-the-docker-container&quot;&gt;Specify a default command for the Docker container&lt;/h3&gt;

&lt;p&gt;Finally, I use the &lt;a href=&quot;https://docs.docker.com/engine/reference/builder/#cmd&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD&lt;/code&gt;&lt;/a&gt; 
instruction to specify a default command to run when a Docker container is 
launched. Since I install 
&lt;a href=&quot;https://jupyterlab.readthedocs.io/en/stable/&quot;&gt;JupyerLab&lt;/a&gt; in all of my Conda 
environments I tend to launch a JupyterLab server by default when executing 
containers.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# default command will be to launch JupyterLab server for development
CMD [ &quot;jupyter&quot;, &quot;lab&quot;, &quot;--no-browser&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;building-the-docker-image&quot;&gt;Building the Docker image&lt;/h2&gt;

&lt;p&gt;The following command builds a new image for your project with a custom &lt;code class=&quot;highlighter-rouge&quot;&gt;$USER&lt;/code&gt; 
(and associated &lt;code class=&quot;highlighter-rouge&quot;&gt;$UID&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;$GID&lt;/code&gt;) as well as a particular &lt;code class=&quot;highlighter-rouge&quot;&gt;$IMAGE_NAME&lt;/code&gt; and 
&lt;code class=&quot;highlighter-rouge&quot;&gt;$IMAGE_TAG&lt;/code&gt;. This command should be run within the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker&lt;/code&gt; sub-directory of 
the project as the Docker build context is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;../&lt;/code&gt; which should be the 
project root directory.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker image build &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$UID&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;gid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$GID&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; Dockerfile &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$IMAGE_TAG&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  ../
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-a-docker-container&quot;&gt;Running a Docker container&lt;/h2&gt;

&lt;p&gt;Once the image is built, the following command will run a container based on 
the image &lt;code class=&quot;highlighter-rouge&quot;&gt;$IMAGE_NAME:$IMAGE_TAG&lt;/code&gt;. This command should be run from within the 
projectâ€™s root directory.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker container run &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--tty&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/bin:/home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/app/bin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/data:/home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/app/data &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/doc:/home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/app/doc &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/notebooks:/home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/app/notebooks &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/results:/home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/app/results &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/src:/home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/app/src &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--publish&lt;/span&gt; 8888:8888 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$IMAGE_TAG&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;using-docker-compose&quot;&gt;Using Docker Compose&lt;/h2&gt;

&lt;p&gt;It is quite easy to make typos whilst writing the above docker commands by hand. 
A less error-prone approach is to use 
&lt;a href=&quot;https://docs.docker.com/compose/&quot;&gt;Docker Compose&lt;/a&gt;. The above docker commands can 
be encapsulated into the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; configuration file as follows.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3.7&quot;&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;jupyterlab-server&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;username=${USER}&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;uid=${UID}&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gid=${GID}&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;dockerfile&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker/Dockerfile&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;8888:8888&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../bin:/home/${USER}/app/bin&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../data:/home/${USER}/app/data&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../doc:/home/${USER}/app/doc&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../notebooks:/home/${USER}/app/notebooks&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../results:/home/${USER}/app/results&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../src:/home/${USER}/app/src&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;stdin_open&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tty&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file relies on 
&lt;a href=&quot;https://docs.docker.com/compose/environment-variables/#the-env-file&quot;&gt;variable substitution&lt;/a&gt;.
to obtain the values for &lt;code class=&quot;highlighter-rouge&quot;&gt;$USER&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;$UID&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;$GID&lt;/code&gt;. These values can be 
stored in an a file called &lt;code class=&quot;highlighter-rouge&quot;&gt;.env&lt;/code&gt; as follows.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;USER=$USER
UID=$UID
GID=$GID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can test your &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file by running the following command in 
the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker&lt;/code&gt; sub-directory of the project.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker-compose config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command takes the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file and substitutes the values 
provided in the &lt;code class=&quot;highlighter-rouge&quot;&gt;.env&lt;/code&gt; file and then returns the result.&lt;/p&gt;

&lt;p&gt;Once you are confident that values in the &lt;code class=&quot;highlighter-rouge&quot;&gt;.env&lt;/code&gt; file are being substituted 
properly into the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file, the following command can be used 
to bring up a container based on your projectâ€™s Docker image and launch the 
JupyterLab server. This command should also be run from within the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker&lt;/code&gt; 
sub-directory of the project.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker-compose up &lt;span class=&quot;nt&quot;&gt;--build&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you are done developing and have shutdown the JupyterLab server, the 
following command tears down the networking infrastructure for the running 
container.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker-compose down
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this post I walked through a &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; that can be used to inject a 
Conda (+ pip) environment into into a Docker image. I also detailed how to 
build the resulting image and launch containers using Docker Compose.&lt;/p&gt;

&lt;p&gt;If you are looking for a production-quality solution that generalizes the 
approach outlined above, then I would encourage you to check out 
&lt;a href=&quot;https://repo2docker.readthedocs.io/en/latest/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jupyter-repo2docker&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jupyter-repo2docker&lt;/code&gt; is a tool to build, run, and push Docker images from 
source code repositories. &lt;code class=&quot;highlighter-rouge&quot;&gt;repo2docker&lt;/code&gt; fetches a repository (from GitHub, 
GitLab, Zenodo, Figshare, Dataverse installations, a Git repository or a local 
directory) and builds a container image in which the code can be executed. The 
image build process is based on the configuration files found in the repository.&lt;/p&gt;

&lt;p&gt;The Conda (+ pip) and Docker combination has significantly increased my data 
science development velocity while at the same time increasing the portability 
and reproducibility of my data science workflows.&lt;/p&gt;

&lt;p&gt;Hopefully this post can help you combine these three great tools together on 
your next data science project!&lt;/p&gt;</content><author><name></name></author><summary type="html">Conda (+ pip) and Docker FTW!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/conda-docker.png" /><media:content medium="image" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/conda-docker.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Building a Conda environment for Horovod</title><link href="https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html" rel="alternate" type="text/html" title="Building a Conda environment for Horovod" /><published>2020-03-30T00:00:00-05:00</published><updated>2020-03-30T00:00:00-05:00</updated><id>https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env</id><content type="html" xml:base="https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html">&lt;h1 id=&quot;what-is-horovod&quot;&gt;What is Horovod?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;&quot;&gt;Horovod&lt;/a&gt; is an open-source distributed training framework for 
&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;, &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt;, 
&lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt;, and 
&lt;a href=&quot;https://mxnet.incubator.apache.org/&quot;&gt;Apache MXNet&lt;/a&gt;. Horovod improves the speed, 
scale, and resource utilization of deep learning training.&lt;/p&gt;

&lt;p&gt;In this post I describe how I build Conda environments for my deep learning 
projects where I plan to use Horovod to enable distributed training across 
multiple GPUs (either on the same node or spread across multuple nodes). If 
you like my approach then you can make use of the template repository on 
&lt;a href=&quot;https://github.com/kaust-vislab/horovod-gpu-data-science-project&quot;&gt;GitHub&lt;/a&gt; to 
get started with you rnext Horovod data science project!&lt;/p&gt;

&lt;h1 id=&quot;installing-the-nvidia-cuda-toolkit&quot;&gt;Installing the NVIDIA CUDA Toolkit&lt;/h1&gt;

&lt;p&gt;First thing you need to do is to install the 
&lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot;&gt;appropriate version&lt;/a&gt; 
of the NVIDIA CUDA Toolkit on your workstation. For this blog post I am using 
&lt;a href=&quot;https://developer.nvidia.com/cuda-10.1-download-archive-update2&quot;&gt;NVIDIA CUDA Toolkit 10.1&lt;/a&gt; 
&lt;a href=&quot;https://docs.nvidia.com/cuda/archive/10.1/&quot;&gt;(documentation)&lt;/a&gt; which works with 
all three deep learning frameworks that are currently supported by Horovod.&lt;/p&gt;

&lt;h2 id=&quot;why-not-just-use-the-cudatoolkit-package&quot;&gt;Why not just use the &lt;code class=&quot;highlighter-rouge&quot;&gt;cudatoolkit&lt;/code&gt; package?&lt;/h2&gt;

&lt;p&gt;Typically when installing PyTorch, TensorFlow, or Apache MXNet with GPU support 
using Conda you simply add the appropriate version 
&lt;a href=&quot;https://anaconda.org/anaconda/cudatoolkit&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cudatoolkit&lt;/code&gt;&lt;/a&gt; package to your 
&lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Unfortunately, the &lt;code class=&quot;highlighter-rouge&quot;&gt;cudatoolkit&lt;/code&gt; package available from 
&lt;a href=&quot;https://conda-forge.org/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;conda-forge&lt;/code&gt;&lt;/a&gt; does not include 
&lt;a href=&quot;https://docs.nvidia.com/cuda/archive/10.1/cuda-compiler-driver-nvcc/index.html&quot;&gt;NVCC&lt;/a&gt; 
and in order to use Horovod with either PyTorch, TensorFlow, or MXNet you need 
to compile extensions.&lt;/p&gt;

&lt;h2 id=&quot;what-about-the-cudatoolkit-dev-package&quot;&gt;What about the &lt;code class=&quot;highlighter-rouge&quot;&gt;cudatoolkit-dev&lt;/code&gt; package?&lt;/h2&gt;

&lt;p&gt;While there are 
&lt;a href=&quot;https://anaconda.org/conda-forge/cudatoolkit-dev&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cudatoolkit-dev&lt;/code&gt;&lt;/a&gt; packages 
available from &lt;code class=&quot;highlighter-rouge&quot;&gt;conda-forge&lt;/code&gt; that do include NVCC, I have had difficult getting 
these packages to consistently install properly.&lt;/p&gt;

&lt;h2 id=&quot;use-the-nvcc_linux-64-meta-pacakge&quot;&gt;Use the &lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc_linux-64&lt;/code&gt; meta-pacakge!&lt;/h2&gt;

&lt;p&gt;The most robust approach to obtain NVCC and still use Conda to manage all the 
other dependencies is to install the NVIDIA CUDA Toolkit on your system and then 
install a meta-package 
&lt;a href=&quot;https://anaconda.org/nvidia/nvcc_linux-64&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc_linux-64&lt;/code&gt;&lt;/a&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;conda-forge&lt;/code&gt; 
which configures your Conda environment to use the NVCC installed on the system 
together with the other CUDA Toolkit components installed inside the Conda 
environment.&lt;/p&gt;

&lt;h1 id=&quot;the-environmentyml-file&quot;&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file&lt;/h1&gt;

&lt;p&gt;I prefer to specify as many dependencies as possible in the Conda 
&lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file and only specify dependencies in &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; 
that are not available via Conda channels. Check the 
&lt;a href=&quot;https://horovod.readthedocs.io/en/latest/install_include.html&quot;&gt;official Horovod installation guide&lt;/a&gt; 
for details of required dependencies.&lt;/p&gt;

&lt;h2 id=&quot;channel-priority&quot;&gt;Channel Priority&lt;/h2&gt;

&lt;p&gt;I use the recommended channel priorities. Note that &lt;code class=&quot;highlighter-rouge&quot;&gt;conda-forge&lt;/code&gt; has priority over &lt;code class=&quot;highlighter-rouge&quot;&gt;defaults&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;null&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pytorch&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;conda-forge&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;defaults&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h2&gt;

&lt;p&gt;There are a few things worth noting about the dependencies.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Even though I have installed the NVIDIA CUDA Toolkit manually I still use 
Conda to manage the other required CUDA components such as &lt;code class=&quot;highlighter-rouge&quot;&gt;cudnn&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nccl&lt;/code&gt; 
(and the optional &lt;code class=&quot;highlighter-rouge&quot;&gt;cupti&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;I use two meta-pacakges, &lt;code class=&quot;highlighter-rouge&quot;&gt;cxx-compiler&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc_linux-64&lt;/code&gt;, to make sure 
that suitable C, and C++ compilers are installed and that the resulting 
Conda environment is aware of the manually installed CUDA Toolkit.&lt;/li&gt;
  &lt;li&gt;Horovod requires some controller library to coordinate work between the 
various Horovod processes. Typically this will be some MPI implementation 
such as &lt;a href=&quot;https://www.open-mpi.org/&quot;&gt;OpenMPI&lt;/a&gt;. However, rather than 
specifying the &lt;code class=&quot;highlighter-rouge&quot;&gt;openmpi&lt;/code&gt; package directly I instead opt for 
&lt;a href=&quot;https://mpi4py.readthedocs.io/en/stable/&quot;&gt;mpi4py&lt;/a&gt; Conda package which 
provides a cuda-aware build of OpenMPI (where possible).&lt;/li&gt;
  &lt;li&gt;Horovod also support that &lt;a href=&quot;https://github.com/facebookincubator/gloo&quot;&gt;Gloo&lt;/a&gt; 
collective communications library that can be used in place of MPI. I 
include &lt;code class=&quot;highlighter-rouge&quot;&gt;cmake&lt;/code&gt; in order to insure that the Horovod extensions for Gloo are 
built.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Below are the core required dependencies. The complete &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file 
is available on 
&lt;a href=&quot;https://github.com/kaust-vislab/horovod-gpu-data-science-project/blob/master/environment.yml&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bokeh=1.4&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cmake=3.16&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# insures that the Gloo library extensions will be built&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cudnn=7.6&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cupti=10.1&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cxx-compiler=1.0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# meta-pacakge that insures suitable C and C++ compilers are available&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;jupyterlab=1.2&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mpi4py=3.0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# installs cuda-aware openmpi&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nccl=2.5&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nodejs=13&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvcc_linux-64=10.1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# meta-package that configures environment to be &quot;cuda-aware&quot;&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pip=20.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mxnet-cu101mkl==1.6.*&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# makes sure MXNET is installed prior to horovod&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-r file:requirements.txt&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;python=3.7&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pytorch=1.4&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tensorboard=2.1&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tensorflow-gpu=2.1&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;torchvision=0.5&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;the-requirementstxt-file&quot;&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; File&lt;/h1&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file is where all of the &lt;code class=&quot;highlighter-rouge&quot;&gt;pip&lt;/code&gt; dependencies, including 
Horovod itself, are listed for installation. In addition to Horovod I 
typically will also use &lt;code class=&quot;highlighter-rouge&quot;&gt;pip&lt;/code&gt; to install JupyterLab extensions to enable GPU and 
CPU resource monitoring via &lt;a href=&quot;https://github.com/rapidsai/jupyterlab-nvdashboard&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jupyterlab-nvdashboard&lt;/code&gt;&lt;/a&gt; 
and Tensorboard support via &lt;a href=&quot;https://github.com/lspvic/jupyter_tensorboard&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jupyter-tensorboard&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;horovod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.19.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
jupyterlab-nvdashboard&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.2.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# server-side component; client-side component installed in postBuild&lt;/span&gt;
jupyter-tensorboard&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.2.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# make sure horovod is re-compiled if environment is re-built&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--no-binary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;horovod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the use of the &lt;code class=&quot;highlighter-rouge&quot;&gt;--no-binary&lt;/code&gt; option at the end of the file. Including this 
option insures that Horovod will be re-built whenever the Conda environment is 
re-built.&lt;/p&gt;

&lt;p&gt;The complete &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file is available on &lt;a href=&quot;https://github.com/kaust-vislab/horovod-gpu-data-science-project/blob/master/requirements.txt&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;building-conda-environment&quot;&gt;Building Conda Environment&lt;/h1&gt;

&lt;p&gt;After adding any necessary dependencies that should be downloaded via &lt;code class=&quot;highlighter-rouge&quot;&gt;conda&lt;/code&gt; 
to the &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file and any dependencies that should be downloaded 
via &lt;code class=&quot;highlighter-rouge&quot;&gt;pip&lt;/code&gt; to the &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file you create the Conda environment in a 
sub-directory &lt;code class=&quot;highlighter-rouge&quot;&gt;./env&lt;/code&gt;of your project directory by running the following 
commands.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ENV_PREFIX&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;/env
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_CUDA_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$CUDA_HOME&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_NCCL_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_GPU_ALLREDUCE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NCCL
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_GPU_BROADCAST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NCCL
conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;--prefix&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; environment.yml &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By default Horovod will try and build extensions for all detected frameworks. 
See the Horovod documentation on 
&lt;a href=&quot;https://horovod.readthedocs.io/en/latest/install_include.html#environment-variables&quot;&gt;environment variables&lt;/a&gt; 
for the details on additional environment variables that can be set prior to 
building Horovod.&lt;/p&gt;

&lt;p&gt;Once the new environment has been created you can activate the environment 
with the following command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda activate &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-postbuild-file&quot;&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;postBuild&lt;/code&gt; File&lt;/h2&gt;

&lt;p&gt;If you wish to use any JupyterLab extensions included in the &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; 
and &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; files then you need to rebuild the JupyterLab 
application using the following commands to source the &lt;code class=&quot;highlighter-rouge&quot;&gt;postBuild&lt;/code&gt; script.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda activate &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# optional if environment already active&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; postBuild
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;wrapping-it-all-up-in-a-bash-script&quot;&gt;Wrapping it all up in a Bash script&lt;/h2&gt;

&lt;p&gt;I typically wrap these commands into a shell script &lt;code class=&quot;highlighter-rouge&quot;&gt;./bin/create-conda-env.sh&lt;/code&gt;. 
Running the shell script will set the Horovod build variables, create the 
Conda environment, activate the Conda environment, and built JupyterLab with 
any additional extensions.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash --login&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ENV_PREFIX&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;/env
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_CUDA_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$CUDA_HOME&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_NCCL_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_GPU_ALLREDUCE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NCCL
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOROVOD_GPU_BROADCAST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NCCL

conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;--prefix&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; environment.yml &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt;
conda activate &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; postBuild
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I typically put scripts inside a &lt;code class=&quot;highlighter-rouge&quot;&gt;./bin&lt;/code&gt; directory in my project root directory. 
The script should be run from the project root directory as follows.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./bin/create-conda-env.sh &lt;span class=&quot;c&quot;&gt;# assumes that $CUDA_HOME is set properly&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;verifying-the-conda-environment&quot;&gt;Verifying the Conda environment&lt;/h1&gt;

&lt;p&gt;After building the Conda environment you can check that Horovod has been built 
with support for the deep learning frameworks TensorFlow, PyTorch, Apache 
MXNet, and the contollers MPI and Gloo with the following command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda activate &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# optional if environment already active&lt;/span&gt;
horovodrun &lt;span class=&quot;nt&quot;&gt;--check-build&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should see output similar to the following.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Horovod v0.19.1:

Available Frameworks:
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] TensorFlow
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] PyTorch
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] MXNet

Available Controllers:
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] MPI
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] Gloo

Available Tensor Operations:
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] NCCL
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; DDL
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; CCL
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] MPI
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;X] Gloo  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;listing-the-contents-of-the-conda-environment&quot;&gt;Listing the contents of the Conda environment&lt;/h2&gt;

&lt;p&gt;To see the full list of packages installed into the environment run the 
following command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda activate &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# optional if environment already active&lt;/span&gt;
conda list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;updating-the-conda-environment&quot;&gt;Updating the Conda environment&lt;/h1&gt;

&lt;p&gt;If you add (remove) dependencies to (from) the &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file or the 
&lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file after the environment has already been created, then 
you can re-create the environment with the following command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;--prefix&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENV_PREFIX&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; environment.yml &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, whenever I add new dependencies I prefer to re-run the Bash script 
which will re-build both the Conda environment and JupyterLab.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./bin/create-conda-env.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Finding a reproducible process for building Horovod extensions for my deep 
learning projects was tricky. Key to my solution is the use of meta-packages 
from &lt;code class=&quot;highlighter-rouge&quot;&gt;conda-forge&lt;/code&gt; to insure that the appropriate compilers are installed and 
that the resulting Conda environment is aware of the system installed NVIDIA 
CUDA Toolkit. The second key is to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;--no-binary&lt;/code&gt; flag in the 
&lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file to insure that Horovod is re-built whenever the Conda 
environment is re-built.&lt;/p&gt;

&lt;p&gt;If you like my approach then you can make use of the template repository on 
&lt;a href=&quot;https://github.com/kaust-vislab/horovod-gpu-data-science-project&quot;&gt;GitHub&lt;/a&gt; to 
get started with your next Horovod data science project!&lt;/p&gt;</content><author><name></name></author><summary type="html">What is Horovod?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/horovod-nvidia.jpg" /><media:content medium="image" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/horovod-nvidia.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Managing Project-Specific Environments With Conda</title><link href="https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html" rel="alternate" type="text/html" title="Managing Project-Specific Environments With Conda" /><published>2020-03-29T00:00:00-05:00</published><updated>2020-03-29T00:00:00-05:00</updated><id>https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda</id><content type="html" xml:base="https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html">&lt;h1 id=&quot;getting-started-with-conda&quot;&gt;Getting Started with Conda&lt;/h1&gt;

&lt;p&gt;Conda is an open source package and environment management system that runs on 
Windows, Mac OS and Linux.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Conda can quickly install, run, and update packages and their dependencies.&lt;/li&gt;
  &lt;li&gt;Conda can create, save, load, and switch between project specific software environments on your local computer.&lt;/li&gt;
  &lt;li&gt;Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Conda as a &lt;em&gt;package manager&lt;/em&gt; helps you find and install packages. If you need a 
package that requires a different version of Python, you do not need to switch to 
a different environment manager, because Conda is also an &lt;em&gt;environment manager&lt;/em&gt;. 
With just a few commands, you can set up a totally separate environment to run 
that different version of Python, while continuing to run your usual version of 
Python in your normal environment.&lt;/p&gt;

&lt;h2 id=&quot;conda-miniconda-anaconda-whats-the-difference&quot;&gt;Conda? Miniconda? Anaconda? Whatâ€™s the difference?&lt;/h2&gt;

&lt;p&gt;Users are often confused about the differences between Conda, Miniconda, and 
Anaconda.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img alt=&quot;Conda vs. Miniconda vs. Anaconda&quot; src=&quot;/stochastic-expatriate-descent/images/miniconda-vs-anaconda.png&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;I suggest installing Miniconda which combines Conda with Python 3 (and a small 
number of core systems packages) instead of the full Anaconda distribution. 
Installing only Miniconda will encourage you to create separate environments 
for each project (and to install only those packages that you actually need for each 
project!) which will enhance portability and reproducibility of your research and 
workflows.&lt;/p&gt;

&lt;p&gt;Besides, if you &lt;em&gt;really&lt;/em&gt; want a particular version of the full Anaconda 
distribution you can always create an new conda environment and install it 
using the following command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; anaconda202002 &lt;span class=&quot;nv&quot;&gt;anaconda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2020.02
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;installing-miniconda&quot;&gt;Installing Miniconda&lt;/h2&gt;

&lt;p&gt;Download the 64-bit, Python 3 version of the 
&lt;a href=&quot;https://docs.conda.io/en/latest/miniconda.html&quot;&gt;appropriate Miniconda installer&lt;/a&gt; 
for your operating system from and follow the instructions. I will walk through 
the steps for installing on Linux systems below as installing on Linux systems 
is slightly more involved.&lt;/p&gt;

&lt;p&gt;Download the 64-bit Python 3 install script for Miniconda.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget &lt;span class=&quot;nt&quot;&gt;--quiet&lt;/span&gt; https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run the Miniconda install script. The &lt;code class=&quot;highlighter-rouge&quot;&gt;-b&lt;/code&gt; runs the install script in batch mode 
which doesnâ€™t require manual intervention (and assumes that the user agrees to 
the terms of the license).&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash Miniconda3-latest-Linux-x86_64.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remove the install script.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;rm &lt;/span&gt;Miniconda3-latest-Linux-x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;initializing-your-shell-for-conda&quot;&gt;Initializing your shell for Conda&lt;/h2&gt;

&lt;p&gt;After installing Miniconda you next need to configure your preferred shell to be 
â€œconda-awareâ€.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda init bash
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# now the prompt indicates that the base environment is active!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;updating-conda&quot;&gt;Updating Conda&lt;/h2&gt;

&lt;p&gt;It is a good idea to keep your &lt;code class=&quot;highlighter-rouge&quot;&gt;conda&lt;/code&gt; installation updated to the most recent 
version.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda update &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; base conda &lt;span class=&quot;nt&quot;&gt;--yes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;uninstalling-miniconda&quot;&gt;Uninstalling Miniconda&lt;/h2&gt;

&lt;p&gt;Whenever installing new software it is always a good idea to understand how to 
&lt;em&gt;uninstall&lt;/em&gt; the software (just in case you have second thoughts!). Uninstalling 
Miniconda is fairly straighforward.&lt;/p&gt;

&lt;p&gt;Uninitialize your shell to remove Conda related content from &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda init &lt;span class=&quot;nt&quot;&gt;--reverse&lt;/span&gt; bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remove the entire &lt;code class=&quot;highlighter-rouge&quot;&gt;~/miniconda3&lt;/code&gt; directory.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; ~/miniconda3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remove the entire &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.conda&lt;/code&gt; directory.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; ~/.conda
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If present, remove your Conda configuration file.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; ~/.condarc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;conda-best-practices&quot;&gt;Conda â€œBest Practicesâ€&lt;/h1&gt;

&lt;p&gt;In the following section I detail a minimal set of best practices for using 
Conda to manage data science environments that I use in my own work.&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TLDR;&lt;/h2&gt;

&lt;p&gt;Here is the basic recipe for using Conda to manage a project specific software 
stack.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;project-dir
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;project-dir
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;nano environment.yml &lt;span class=&quot;c&quot;&gt;# create the environment file&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;--prefix&lt;/span&gt; ./env &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; environment.yml
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda activate ./env &lt;span class=&quot;c&quot;&gt;# activate the environment&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;/path/to/env&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;nano environment.yml &lt;span class=&quot;c&quot;&gt;# forgot to add some deps&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;/path/to/env&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;update &lt;span class=&quot;nt&quot;&gt;--prefix&lt;/span&gt; ./env &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; environment.yml &lt;span class=&quot;nt&quot;&gt;--prune&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# update the environment&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;/path/to/env&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda deactivate &lt;span class=&quot;c&quot;&gt;# done working on project (for now!)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;new-project-new-directory&quot;&gt;New project, new directory&lt;/h2&gt;

&lt;p&gt;Every new project (no matter how small!) should live in its own directory. 
A good reference to get started with organizing your project directory is 
&lt;a href=&quot;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510&quot;&gt;Good Enough Practices for Scientific Computing&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;project-dir
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;project-dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;new-project-new-environment&quot;&gt;New project, new environment&lt;/h2&gt;

&lt;p&gt;Now that you have a new project directory you are ready to create a new 
environment for your project. We will do this in two steps.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create an environment file that describes the software dependencies 
(including specific version numbers!) for the project.&lt;/li&gt;
  &lt;li&gt;Use the newly created environment file to build the software environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is an example of a typical environment file that could be used to run GPU 
accelerated, distributed training of deep learning models developed using 
&lt;a href=&quot;https://www.pytorch.org&quot;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;null&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pytorch&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;conda-forge&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;defaults&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cudatoolkit=10.1&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;jupyterlab=1.2&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pip=20.0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;python=3.7&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pytorch=1.4&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;torchvision=0.5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you have created an &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file inside your project directory 
you can use the following commands to create the environment as a sub-directory 
called &lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt; inside your project directory.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda env create --prefix ./env --file environment.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;activating-an-environment&quot;&gt;Activating an environment&lt;/h2&gt;

&lt;p&gt;Activating environments is essential to making the software in environments work 
well (or sometimes at all!). Activation of an environment does two things.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Adds entries to &lt;code class=&quot;highlighter-rouge&quot;&gt;PATH&lt;/code&gt; for the environment.&lt;/li&gt;
  &lt;li&gt;Runs any activation scripts that the environment may contain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Step 2 is particularly important as activation scripts are how packages can set 
arbitrary environment variables that may be necessary for their operation.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda activate ./env &lt;span class=&quot;c&quot;&gt;# activate the environment&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;/path/to/env&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# now the prompt indicates which environment is active!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;updating-an-environment&quot;&gt;Updating an environment&lt;/h2&gt;

&lt;p&gt;You are unlikely to know ahead of time which packages (and version numbers!) you 
will need to use for your research project. For example it may be the case thatâ€¦&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;one of your core dependencies just released a new version (dependency version 
number update).&lt;/li&gt;
  &lt;li&gt;you need an additional package for data analysis (add a new dependency).&lt;/li&gt;
  &lt;li&gt;you have found a better visualization package and no longer need to old 
visualization package (add new dependency and remove old dependency).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If any of these occurs during the course of your research project, all you need 
to do is update the contents of your &lt;code class=&quot;highlighter-rouge&quot;&gt;environment.yml&lt;/code&gt; file accordingly and then 
run the following command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;update &lt;span class=&quot;nt&quot;&gt;--prefix&lt;/span&gt; ./env &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; environment.yml &lt;span class=&quot;nt&quot;&gt;--prune&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# update the environment&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Alternatively, you can simply rebuild the environment from scratch with the following 
command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;--prefix&lt;/span&gt; ./env &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; environment.yml &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;deactivating-an-environment&quot;&gt;Deactivating an environment&lt;/h2&gt;

&lt;p&gt;When you are done working on your project it is a good idea to deactivate the 
current environment. To deactivate the currently active environment use the 
&lt;code class=&quot;highlighter-rouge&quot;&gt;deactivate&lt;/code&gt; command as follows.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda deactivate # done working on project (for now!)
(base) $ # now you are back to the base environment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;interested-in-learning-more&quot;&gt;Interested in Learning More?&lt;/h1&gt;

&lt;p&gt;For more details on using Conda to manage software stacks for you data science projects, 
checkout the 
&lt;a href=&quot;https://carpentries-incubator.github.io/introduction-to-conda-for-data-scientists/&quot;&gt;Introduction to Conda for (Data) Scientists&lt;/a&gt; 
training materials that I have contributed to &lt;a href=&quot;https://carpentries.org/involved-lessons/&quot;&gt;The Carpentries Incubator&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Getting Started with Conda</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/conda-logo.svg" /><media:content medium="image" url="https://davidrpugh.github.io/stochastic-expatriate-descent/images/conda-logo.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>