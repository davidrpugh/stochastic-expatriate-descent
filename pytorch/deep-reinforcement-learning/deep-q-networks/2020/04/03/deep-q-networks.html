<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Getting familiar with deep Q-networks | Stochastic Expatriate Descent</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Getting familiar with deep Q-networks" />
<meta name="author" content="David R. Pugh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on the famous Deep Q-Networks paper from Deepmind." />
<meta property="og:description" content="Notes on the famous Deep Q-Networks paper from Deepmind." />
<link rel="canonical" href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html" />
<meta property="og:url" content="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html" />
<meta property="og:site_name" content="Stochastic Expatriate Descent" />
<meta property="og:image" content="https://davidrpugh.github.io/stochastic-expatriate-descent/images/q-network-architecture.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-03T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"David R. Pugh"},"datePublished":"2020-04-03T00:00:00-05:00","headline":"Getting familiar with deep Q-networks","image":"https://davidrpugh.github.io/stochastic-expatriate-descent/images/q-network-architecture.jpg","description":"Notes on the famous Deep Q-Networks paper from Deepmind.","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html"},"@type":"BlogPosting","url":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html","dateModified":"2020-04-03T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/stochastic-expatriate-descent/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml" title="Stochastic Expatriate Descent" /><link rel="shortcut icon" type="image/x-icon" href="/stochastic-expatriate-descent/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Getting familiar with deep Q-networks | Stochastic Expatriate Descent</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Getting familiar with deep Q-networks" />
<meta name="author" content="David R. Pugh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on the famous Deep Q-Networks paper from Deepmind." />
<meta property="og:description" content="Notes on the famous Deep Q-Networks paper from Deepmind." />
<link rel="canonical" href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html" />
<meta property="og:url" content="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html" />
<meta property="og:site_name" content="Stochastic Expatriate Descent" />
<meta property="og:image" content="https://davidrpugh.github.io/stochastic-expatriate-descent/images/q-network-architecture.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-03T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"David R. Pugh"},"datePublished":"2020-04-03T00:00:00-05:00","headline":"Getting familiar with deep Q-networks","image":"https://davidrpugh.github.io/stochastic-expatriate-descent/images/q-network-architecture.jpg","description":"Notes on the famous Deep Q-Networks paper from Deepmind.","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html"},"@type":"BlogPosting","url":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html","dateModified":"2020-04-03T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml" title="Stochastic Expatriate Descent" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/stochastic-expatriate-descent/">Stochastic Expatriate Descent</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/stochastic-expatriate-descent/about/">About Me</a><a class="page-link" href="/stochastic-expatriate-descent/search/">Search</a><a class="page-link" href="/stochastic-expatriate-descent/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Getting familiar with deep Q-networks</h1><p class="page-description">Notes on the famous Deep Q-Networks paper from Deepmind.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-03T00:00:00-05:00" itemprop="datePublished">
        Apr 3, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">David R. Pugh</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      20 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#deep-reinforcement-learning">deep-reinforcement-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#deep-q-networks">deep-q-networks</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/davidrpugh/stochastic-expatriate-descent/tree/2020-04-03-deep-q-networks/_notebooks/2020-04-03-deep-q-networks.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/davidrpugh/stochastic-expatriate-descent/2020-04-03-deep-q-networks?filepath=_notebooks%2F2020-04-03-deep-q-networks.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/davidrpugh/stochastic-expatriate-descent/blob/2020-04-03-deep-q-networks/_notebooks/2020-04-03-deep-q-networks.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-03-deep-q-networks.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Colab-specific-environment-setup">Colab specific environment setup<a class="anchor-link" href="#Colab-specific-environment-setup"> </a></h2><p>If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required dependencies into the environment.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>conda install --yes --channel pytorch --channel conda-forge <span class="nv">pytorch</span><span class="o">=</span><span class="m">1</span>.4 <span class="nv">pandas</span><span class="o">=</span><span class="m">1</span>.0 <span class="nv">scipy</span><span class="o">=</span><span class="m">1</span>.4 <span class="nv">swig</span><span class="o">=</span><span class="m">4</span>.0
<span class="o">!</span>pip install gym<span class="o">[</span>box2d<span class="o">]==</span><span class="m">0</span>.17.*
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting package metadata (current_repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /Users/pughdr/Research/stochastic-expatriate-descent/env

  added / updated specs:
    - pytorch=1.4
    - scipy=1.4
    - swig=4.0


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    openssl-1.1.1f             |       h0b31af3_0         1.9 MB  conda-forge
    pcre-8.44                  |       h4a8c4bd_0         222 KB  conda-forge
    scipy-1.4.1                |   py37hce1b9e5_2        18.9 MB  conda-forge
    swig-4.0.1                 |       h6de7cb9_0         1.1 MB  conda-forge
    ------------------------------------------------------------
                                           Total:        22.1 MB

The following NEW packages will be INSTALLED:

  pcre               conda-forge/osx-64::pcre-8.44-h4a8c4bd_0
  scipy              conda-forge/osx-64::scipy-1.4.1-py37hce1b9e5_2
  swig               conda-forge/osx-64::swig-4.0.1-h6de7cb9_0

The following packages will be SUPERSEDED by a higher-priority channel:

  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --&gt; conda-forge::ca-certificates-2019.11.28-hecc5488_0
  certifi              pkgs/main::certifi-2019.11.28-py37_1 --&gt; conda-forge::certifi-2019.11.28-py37hc8dfbb8_1
  openssl              pkgs/main::openssl-1.1.1f-h1de35cc_0 --&gt; conda-forge::openssl-1.1.1f-h0b31af3_0



Downloading and Extracting Packages
pcre-8.44            | 222 KB    | ##################################### | 100% 
swig-4.0.1           | 1.1 MB    | ##################################### | 100% 
scipy-1.4.1          | 18.9 MB   | ##################################### | 100% 
openssl-1.1.1f       | 1.9 MB    | ##################################### | 100% 
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
Requirement already satisfied: gym==0.17.* in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (0.17.1)
Requirement already satisfied: pyvirtualdisplay==0.2.* in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (0.2.5)
Collecting box2d==2.3.*
  Using cached Box2D-2.3.2.tar.gz (427 kB)
Requirement already satisfied: cloudpickle&lt;1.4.0,&gt;=1.2.0 in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (from gym==0.17.*) (1.3.0)
Requirement already satisfied: scipy in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (from gym==0.17.*) (1.4.1)
Requirement already satisfied: numpy&gt;=1.10.4 in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (from gym==0.17.*) (1.18.1)
Requirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (from gym==0.17.*) (1.5.0)
Requirement already satisfied: six in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (from gym==0.17.*) (1.14.0)
Requirement already satisfied: EasyProcess in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (from pyvirtualdisplay==0.2.*) (0.2.10)
Requirement already satisfied: future in /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym==0.17.*) (0.18.2)
Building wheels for collected packages: box2d
  Building wheel for box2d (setup.py) ... done
  Created wheel for box2d: filename=Box2D-2.3.2-cp37-cp37m-macosx_10_9_x86_64.whl size=522295 sha256=e591bdf01666a4cf6a2410096282a50a0ff2e40a366db1b6a4fc2b13de9744dc
  Stored in directory: /Users/pughdr/Library/Caches/pip/wheels/89/b0/a7/84885e448a1d3ca7b9779a7aba7d71121b1b11baec80027885
Successfully built box2d
Installing collected packages: box2d
Successfully installed box2d-2.3.2
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">ion</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Motivation">Motivation<a class="anchor-link" href="#Motivation"> </a></h2><p>I am currently using my COVID-19 imposed quarantine to expand my deep learning skills by completing the <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893"><em>Deep Reinforcement Learning Nanodegree</em></a> from <a href="https://www.udacity.com/">Udacity</a>. This past week I have been working my way through <em>Human-level control through deep reinforcement learning</em> <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">(Minh et al 2015)</a>.</p>
<p>While Minh et al 2015 were not the first to use neural networks to approximate the action-value function, this paper was the first to demonstrate that the same neural network architecture could be trained in a computationally efficient manner to "solve" a large number or different tasks.</p>
<p>The Minh et al 2015 paper also contributed several practical "tricks" for getting deep neural networks to consistently converge. This in and of itself was a non-trivial contribution as issues with convergence of neural networks had previsouly been blocking widespread adoption of deep learning techniques in the reinforcemnt learning community.</p>
<p>Minh et al 2015 uses deep (convolutional) neural network to approximate the optimal action-value function</p>
<p>
$$ Q^*(s, a) = \max_{\pi} \mathbb{E}\Bigg[\sum_{s=0}^{\infty} \gamma^s r_{t+s} | s_t=s, a_t=a, \pi \Bigg] $$
</p>
<p>which is the maximum sum of rewards $r_t$ discounted by $\gamma$ at each time-step $t$ achievable by a behaviour policy $\pi = P(a|s)$, after making an observation of the state $s$ and taking an action $a$.</p>
<p>Prior to this seminal paper it was well known that standard reinforcement learning algorithms were unstable or even diverged when a non-linear function approximators such as a neural networks were used to represent the action-value function $Q$. Why?</p>
<p>Minh et al 2015 discuss several reasons.</p>
<ol>
<li>Correlations present in the sequence of observations of the state $s$. In reinforcement learning applications the sequence state observations is a time-series which will almost surely be auto-correlated. But surely this would also be true of any application of deep neural networks to model time series data. </li>
<li>Small updates to $Q$ may significantly change the policy, $\pi$ and therefore change the data distribution.</li>
<li>Correlations between the action-values, $Q$, and the target values $r + \gamma \max_{a'} Q(s', a')$</li>
</ol>
<p>In the paper the authors address these issues by using...</p>
<ul>
<li>a biologically inspired mechanism they refer to as <em>experience replay</em> that randomizes over the data which removes correlations in the sequence of observations of the state $s$ and smoothes over changes in the data distribution (issues 1 and 2 above).</li>
<li>an iterative update rule that adjusts the action-values, $Q$, towards target values, $Q'$ that are only periodically updated thereby reducing correlations with the target (issue 3 above).</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Approximating-the-action-value-function,-$Q(s,a)$">Approximating the action-value function, $Q(s,a)$<a class="anchor-link" href="#Approximating-the-action-value-function,-$Q(s,a)$"> </a></h2><p>There are several possible ways of approximating the action-value function $Q$ using a neural network. The only input to the DQN architecture is the state representation and the output layer has a separate output for each possible action. The output units correspond to the predicted $Q$-values of the individual actions for the input state. A representaion of the DQN architecture from the paper is reproduced in the figure below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p align="center">
   <img alt="Deep Q-Network Architecture" src="../images/q-network-architecture.jpg" width="750" />
</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map $\phi$. The first hidden layer combines a convolutional layer with 32 filters (each of which uses an 8 x 8 kernel and a stride of 4) and a ReLU activation function. The second hidden layer convolves 64 filters (each of which using a 4 x 4 kernel with stride of 2) followed by a ReLU activation function. This is followed by a third convolutional layer that combines 64 filters (each of which uses a 3 x 3 kernel and a stride of 1) with yet anotehr ReLU activation. The final hidden layer is fully-connected (i.e., dense) linear layer with 512 neurons followed by a ReLU activation. The output layer is a fully-connected layer with a single output for each action.</p>
<p>A PyTorch implementation of the DQN architecture would look something like the following.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">deep_q_network</span><span class="p">(</span><span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">deep_q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">action_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">deep_q_network</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Loss-Function">The Loss Function<a class="anchor-link" href="#The-Loss-Function"> </a></h2><p>The $Q$-learning update at iteration $i$ uses the following loss function</p>
<p>
$$ \mathcal{L_i}(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Bigg[\bigg(r + \gamma \max_{a'} Q\big(s', a'; \theta_i^{-}\big) - Q\big(s, a; \theta_i\big)\bigg)^2\Bigg] $$
</p>
<p>where $\gamma$ is the discount factor determining the agent’s horizon, $\theta_i$ are the parameters of the $Q$-network at iteration $i$ and $\theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $\theta_i^{-}$ are only updated with the $Q$-network parameters $\theta_i$ every $C$ steps and are held fixed between individual updates.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Experience-Replay">Experience Replay<a class="anchor-link" href="#Experience-Replay"> </a></h2><p>To perform <em>experience replay</em> the authors store the agent's experiences $e_t$ as represented by the tuple</p>
<p>
$$ e_t = (s_t, a_t, r_t, s_{t+1}) $$
</p>
<p>consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences.</p>
<p>
$$ D_t = \{e1, e2, ..., e_t \} $$
</p>
<p>Depending on the task it may note be feasible for the agent to store the entire history of past experiences.</p>
<p>During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s')$, drawn uniformly at random from the pool of stored samples $D_t$.</p>
<p>The following is my Python implmentation of these ideas.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_field_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;action&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reward&quot;</span><span class="p">,</span>
    <span class="s2">&quot;next_state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;done&quot;</span>
<span class="p">]</span>
<span class="n">Experience</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;Experience&quot;</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="n">_field_names</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ExperienceReplayBuffer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Fixed-size buffer to store experience tuples.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize an ExperienceReplayBuffer object.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        buffer_size (int): maximum size of buffer</span>
<span class="sd">        batch_size (int): size of each training batch</span>
<span class="sd">        seed (int): random seed</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span> <span class="o">=</span> <span class="n">buffer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span> <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">random_state</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">buffer_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span>
    
    <span class="k">def</span> <span class="nf">has_sufficient_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
    
    <span class="k">def</span> <span class="nf">is_full</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span>
    
    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">:</span> <span class="n">Experience</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">Experience</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Randomly sample a batch of experiences from memory.&quot;&quot;&quot;</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">)</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">experiences</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Deep-Q-Network-Algorithm">The Deep Q-Network Algorithm<a class="anchor-link" href="#The-Deep-Q-Network-Algorithm"> </a></h2><p>The following is Python pseudo-code for the Deep Q-Network (DQN) algorithm. For more fine-grained details of the DQN algorithm see the methods section of <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">Minh et al 2015</a>.</p>
<div class="highlight"><pre><span></span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ExperienceReplayBuffer</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">random_state</span><span class="p">)</span>
<span class="n">local_q_network</span> <span class="o">=</span> <span class="n">initialize_q_network</span><span class="p">()</span>
<span class="n">target_q_network</span> <span class="o">=</span> <span class="n">initialize_q_network</span><span class="p">()</span>
<span class="n">synchronize_q_networks</span><span class="p">(</span><span class="n">target_q_network</span><span class="p">,</span> <span class="n">local_q_network</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_episodes</span><span class="p">)</span>

    <span class="c1"># initialize the environment state</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="c1"># simulate a single training episode</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">timesteps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>

        <span class="c1"># greedy action based on Q(s, a; theta)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_epsilon_greedy_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> 

        <span class="c1"># update the environment based on the chosen action</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># agent records experience in its replay buffer</span>
        <span class="n">experience</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

        <span class="c1"># agent samples a mini-batch of experiences from its replay buffer</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="n">experiences</span>

        <span class="c1"># agent learns every update_frequency timesteps</span>
        <span class="k">if</span> <span class="n">timesteps</span> <span class="o">%</span> <span class="n">update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>

            <span class="c1"># compute the Q^- values using the Q-learning formula</span>
            <span class="n">target_q_values</span> <span class="o">=</span> <span class="n">q_learning_update</span><span class="p">(</span><span class="n">target_q_network</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span><span class="p">)</span>

            <span class="c1"># compute the Q values</span>
            <span class="n">local_q_values</span> <span class="o">=</span> <span class="n">local_q_network</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

            <span class="c1"># agent updates the parameters theta of Q using gradient descent</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">target_q_values</span><span class="p">,</span> <span class="n">local_q_values</span><span class="p">)</span>
            <span class="n">gradient_descent_update</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="c1"># every target_network_update_frequency timesteps set theta^- = theta</span>
        <span class="k">if</span> <span class="n">timesteps</span> <span class="o">%</span> <span class="n">target_network_update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">synchronize_q_networks</span><span class="p">(</span><span class="n">target_q_network</span><span class="p">,</span> <span class="n">local_q_network</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Solving-the-LunarLander-v2-environment">Solving the <code>LunarLander-v2</code> environment<a class="anchor-link" href="#Solving-the-LunarLander-v2-environment"> </a></h2><p>In the rest of this blog post I will use the DQN algorithm to train an agent to solve the <a href="https://gym.openai.com/envs/LunarLander-v2/">LunarLander-v2</a> environment from <a href="https://openai.com/">OpenAI</a>.</p>
<p>In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land "gracefully" (and not crash in the landing area!).</p>
<p>A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered "solved" if the lander is able to achieve 200 points (I will actually be more stringent and define "solved" as achieving over 200 points on average in the most recent 100 training episodes).</p>
<h3 id="Action-Space">Action Space<a class="anchor-link" href="#Action-Space"> </a></h3><p>There are four discrete actions available:</p>
<ol>
<li>Do nothing.</li>
<li>Fire the left orientation engine.</li>
<li>Fire main engine.</li>
<li>Fire the right orientation engine.</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: <span class="ansi-yellow-fg">WARN: Box bound precision lowered by casting to float32</span>
  warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;))
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Defining-a-generic-Agent-and-train-loop">Defining a generic <code>Agent</code> and <code>train</code> loop<a class="anchor-link" href="#Defining-a-generic-Agent-and-train-loop"> </a></h3><p>In the cell below I define a fairly generic training loop for training and <code>Agent</code> to solve a task in a given <code>gym.Env</code> environment. In working through the hands-on portions of the <a href="https://www.udacity.com/">Udacity</a> <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893"><em>Deep Reinforcement Learning Nanodegree</em></a> I found myself writing similar code over and over again to train the agent to solve a task. This is my first attempt to write something that I might be able to reuse on the course going forward.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        
    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
             <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
             <span class="n">next_state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
             <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update agent&#39;s state after observing the effect of its action on the environment.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="n">NotImplmentedError</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">_train_for_at_most</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">max_timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_timesteps</span><span class="p">):</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">score</span>

                
<span class="k">def</span> <span class="nf">_train_until_done</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="k">return</span> <span class="n">score</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span>
          <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span>
          <span class="n">checkpoint_filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
          <span class="n">target_score</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">number_episodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">maximum_timesteps</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reinforcement learning training loop.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    agent (Agent): an agent to train.</span>
<span class="sd">    env (gym.Env): an environment in which to train the agent.</span>
<span class="sd">    checkpoint_filepath (str): filepath used to save the state of the trained agent.</span>
<span class="sd">    number_episodes (int): maximum number of training episodes.</span>
<span class="sd">    maximum_timsteps (int): maximum number of timesteps per episode.</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">most_recent_scores</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_episodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">maximum_timesteps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">_train_until_done</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">_train_for_at_most</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">maximum_timesteps</span><span class="p">)</span>         
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">most_recent_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
        <span class="n">average_score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">most_recent_scores</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">most_recent_scores</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">average_score</span> <span class="o">&gt;=</span> <span class="n">target_score</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Environment solved in </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">d</span><span class="si">}</span><span class="s2"> episodes!</span><span class="se">\t</span><span class="s2">Average Score: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint_filepath</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">Episode </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="se">\t</span><span class="s2">Average Score: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scores</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Creating-a-DeepQAgent">Creating a <code>DeepQAgent</code><a class="anchor-link" href="#Creating-a-DeepQAgent"> </a></h3><p>The code in the cell below encapsulates much of the logic of the DQN algorithm in a <code>DeepQAgent</code> class. Since the <code>LunarLander-v2</code> task is not well suited for convolutional neural networks, the agent uses a simple three layer dense neural network with ReLU activation functions to approximate the action-value function $Q$.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DeepQAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">state_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">action_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">number_hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">optimizer_fn</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">epsilon_decay_schedule</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
                 <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">target_network_update_frequency</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">update_frequency</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize a DeepQAgent.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        </span>
<span class="sd">        state_size (int): the size of the state space.</span>
<span class="sd">        action_size (int): the size of the action space.</span>
<span class="sd">        discount_factor (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1).</span>
<span class="sd">        epsilon_decay_schdule (callable): function that takes episode number and returns epsilon.</span>
<span class="sd">        seed (int): random seed</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_action_size</span> <span class="o">=</span> <span class="n">action_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        
        <span class="c1"># set seeds for reproducibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span> <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># initialize agent hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_experience_replay_buffer</span> <span class="o">=</span> <span class="n">ExperienceReplayBuffer</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_decay_schedule</span> <span class="o">=</span> <span class="n">epsilon_decay_schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_network_update_frequency</span> <span class="o">=</span> <span class="n">target_network_update_frequency</span>
        
        <span class="c1"># initialize Q-Networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span> <span class="o">=</span> <span class="n">update_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_q_network</span><span class="p">(</span><span class="n">number_hidden_units</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_q_network</span><span class="p">(</span><span class="n">number_hidden_units</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_synchronize_q_networks</span><span class="p">()</span>
        
        <span class="c1"># send the networks to the device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        
        <span class="c1"># initialize the optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

        <span class="c1"># initialize some counters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_initialize_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">number_hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_action_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">q_network</span>
        
    <span class="k">def</span> <span class="nf">_learn_from</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiences</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">Experience</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">vs</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span> <span class="k">for</span> <span class="n">vs</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">experiences</span><span class="p">))</span>
        
        <span class="c1"># get max predicted Q values (for next states) from target model</span>
        <span class="n">next_target_q_values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span>
                                       <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                                       <span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># compute the new Q&#39; values using the Q-learning formula</span>
        <span class="n">target_q_values</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="n">next_target_q_values</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">))</span>
        
        <span class="c1"># get expected Q values from local model</span>
        <span class="n">_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">actions</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
                         <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">expected_q_values</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                                 <span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">_index</span><span class="p">))</span>
        <span class="c1"># compute the mean squared loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">expected_q_values</span><span class="p">,</span> <span class="n">target_q_values</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># agent updates the parameters theta of Q using gradient descent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                 
    <span class="k">def</span> <span class="nf">_soft_update_target_q_network_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">local_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
            <span class="n">target_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> <span class="n">local_param</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_synchronize_q_networks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
           
    <span class="k">def</span> <span class="nf">_uniform_random_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_action_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_greedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_values</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Choose an action that maximizes the action_values&quot;&quot;&quot;</span>
        <span class="n">action</span> <span class="o">=</span> <span class="p">(</span><span class="n">action_values</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>  <span class="c1"># action_values might reside on the GPU!</span>
                               <span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
                               <span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns actions for given state as per current policy.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        state (np.array): current state of the environment.</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">        --------</span>
<span class="sd">        action (int): an integer representing the chosen action.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># choose uniform at random if agent has insufficient experience</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experience_replay_buffer</span><span class="o">.</span><span class="n">has_sufficient_experience</span><span class="p">():</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uniform_random_policy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            
            <span class="c1"># with probability epsilon explore randomly</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_decay_schedule</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uniform_random_policy</span><span class="p">()</span>

            <span class="c1"># with probability 1 - epsilon, exploit what you have learned by choosing the greedy action</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># need to reshape the numpy tensor (and maybe send it to the GPU)</span>
                <span class="n">state_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                                     <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                                     <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>

                <span class="c1"># evaluate the local_q_network to get action values</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">action_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

                <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_policy</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span>
    
    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves the state of the DeepQAgent.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        filepath (str): filepath where the serialized state should be saved.</span>
<span class="sd">        </span>
<span class="sd">        Notes:</span>
<span class="sd">        ------</span>
<span class="sd">        The method uses `torch.save` to serialize the state of the q-network, </span>
<span class="sd">        the optimizer, as well as the dictionary of agent hyperparameters.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;q-network-state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;optimizer-state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;agent-hyperparameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">,</span>
                <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">,</span>
                <span class="s2">&quot;target_network_update_frequency&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_network_update_frequency</span><span class="p">,</span>
                <span class="s2">&quot;update_frequency&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">next_state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the agent&#39;s state based on feedback received from the environment.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        state (np.array): the previous state of the environment.</span>
<span class="sd">        action (int): the action taken by the agent in the previous state.</span>
<span class="sd">        reward (float): the reward received from the environment.</span>
<span class="sd">        next_state (np.array): the resulting state of the environment following the action.</span>
<span class="sd">        done (bool): True is the training episode is finised; false otherwise.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># save experience in the experience replay buffer</span>
        <span class="n">experience</span> <span class="o">=</span> <span class="n">Experience</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_experience_replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>
            
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># every so often the agent should learn from experiences</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">_experience_replay_buffer</span><span class="o">.</span><span class="n">has_sufficient_experience</span><span class="p">()):</span>
                <span class="n">experiences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experience_replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_learn_from</span><span class="p">(</span><span class="n">experiences</span><span class="p">)</span>
            
            <span class="c1"># every so often the agent should update the target_q_network parameters</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_network_update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1">#self._synchronize_q_networks()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_soft_update_target_q_network_parameters</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Epsilon-decay-schedule">Epsilon decay schedule<a class="anchor-link" href="#Epsilon-decay-schedule"> </a></h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">linear_decay_schedule</span><span class="p">(</span><span class="n">episode_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                          <span class="n">slope</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                          <span class="n">minimum_epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Simple linear decay schedule used in the Deepmind paper.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">episode_number</span><span class="p">,</span> <span class="n">minimum_epsilon</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">power_decay_schedule</span><span class="p">(</span><span class="n">episode_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">decay_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                         <span class="n">minimum_epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">decay_factor</span><span class="o">**</span><span class="n">episode_number</span><span class="p">,</span> <span class="n">minimum_epsilon</span><span class="p">)</span>

<span class="n">_epsilon_decay_schedule_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;decay_factor&quot;</span><span class="p">:</span> <span class="mf">0.995</span><span class="p">,</span>
    <span class="s2">&quot;minimum_epsilon&quot;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">epsilon_decay_schedule</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">power_decay_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="o">**</span><span class="n">_epsilon_decay_schedule_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Choosing-an-optimizer">Choosing an optimizer<a class="anchor-link" href="#Choosing-an-optimizer"> </a></h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_optimizer_kwargs</span> <span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="mf">1e-08</span><span class="p">,</span>
    <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;momentum&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;centered&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># gradient is normalized by an estimation of its variance (default is False)</span>
<span class="p">}</span>
<span class="n">optimizer_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">_optimizer_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_agent_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;state_size&quot;</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;action_size&quot;</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> 
    <span class="s2">&quot;number_hidden_units&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;optimizer_fn&quot;</span><span class="p">:</span> <span class="n">optimizer_fn</span><span class="p">,</span>
    <span class="s2">&quot;epsilon_decay_schedule&quot;</span><span class="p">:</span> <span class="n">epsilon_decay_schedule</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;update_frequency&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;target_network_update_frequency&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">deep_q_agent</span> <span class="o">=</span> <span class="n">DeepQAgent</span><span class="p">(</span><span class="o">**</span><span class="n">_agent_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Performance-of-an-un-trained-DeepQAgent">Performance of an un-trained <code>DeepQAgent</code><a class="anchor-link" href="#Performance-of-an-un-trained-DeepQAgent"> </a></h3><p>The function <code>simulate</code> defined in the cell below can be used to simuate an agent interacting with and environment for one episode.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">simulate</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">))</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">deep_q_agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">img</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">))</span> 
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        <span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">())</span>
        <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>       
    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The untrained agent behaves erratically (not quite randomly!) and performs poorly. Lots of room for improvement!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">simulate</span><span class="p">(</span><span class="n">deep_q_agent</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-the-DeepQAgent">Training the <code>DeepQAgent</code><a class="anchor-link" href="#Training-the-DeepQAgent"> </a></h3><p>Now I am finally ready to train the <code>deep_q_agent</code>. The target score for the <code>LunarLander-v2</code> environment is 200 points on average for at least 100 consecutive episodes. If the <code>deep_q_agent</code> is able to "solve" the environment, then training will terminate early.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">deep_q_agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">,</span> <span class="n">number_episodes</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">target_score</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Episode 100	Average Score: -152.22
Episode 200	Average Score: -99.00
Episode 300	Average Score: -50.58
Episode 400	Average Score: 18.95
Episode 500	Average Score: 46.88
Episode 600	Average Score: 90.79
Episode 700	Average Score: 114.65
Episode 800	Average Score: 148.66
Episode 900	Average Score: 168.82
Episode 1000	Average Score: 164.13
Episode 1100	Average Score: 137.74
Episode 1200	Average Score: 176.16
Episode 1300	Average Score: 181.91
Episode 1400	Average Score: 131.38
Episode 1500	Average Score: 182.73

Environment solved in 1518 episodes!	Average Score: 200.97
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Analyzing-DeepQAgent-performance">Analyzing <code>DeepQAgent</code> performance<a class="anchor-link" href="#Analyzing-DeepQAgent-performance"> </a></h3><h4 id="Simulating-agent-behavior">Simulating agent behavior<a class="anchor-link" href="#Simulating-agent-behavior"> </a></h4><p>First step to analyzing agent performance is to re-run the simulation above with the trained agent to see the performance improvement on the task at hand. Assuming that the <code>deep_q_agent</code> was able to "solve" the task, running the simulation should show the lander gracefully landing in the designated landing area.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">simulate</span><span class="p">(</span><span class="n">deep_q_agent</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Plotting-the-time-series-of-scores">Plotting the time series of scores<a class="anchor-link" href="#Plotting-the-time-series-of-scores"> </a></h4><p>I can use <a href="https://pandas.pydata.org/">Pandas</a> to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scores&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">scores</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>count    942.000000
mean      35.992930
std      171.546124
min     -480.904674
25%      -81.925562
50%       -5.676505
75%      226.328608
max      313.855280
Name: scores, dtype: float64</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Scores&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
           <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
           <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;Rolling Average&quot;</span><span class="p">)</span>
           <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Target Score&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Episode Number&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Kernel-density-plot-of-the-scores">Kernel density plot of the scores<a class="anchor-link" href="#Kernel-density-plot-of-the-scores"> </a></h4><p>Kernel density plot of scores is bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent "solved" the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a very high score).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Text(0.5, 0, &#39;Score&#39;)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Outstanding-Issues">Outstanding Issues<a class="anchor-link" href="#Outstanding-Issues"> </a></h2><p>I am a bit frustrated by lack of stability that I am seeing in my implmentation of the Deep Q algorithm. Sometimes the algorithm converges, sometimes not, seems to depend on the randomness. Not good. Tuning of hyper-parameters and optimization algorithm is required for convergence. These settings will be task dependent. Can't help feeling that there must be more "dark magic" in the Deepmind implementation than was discussed in the paper.</p>
<p>Possible extensions to pursue in future posts...</p>
<ul>
<li><a href="https://arxiv.org/abs/1509.06461">Double Q-Learning</a></li>
<li><a href="https://arxiv.org/abs/1509.06461">Prioritized Experience Replay</a></li>
<li><a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures</a></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="davidrpugh/stochastic-expatriate-descent"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/stochastic-expatriate-descent/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/stochastic-expatriate-descent/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An expat scientist&#39;s musings about machine learning, deep learning, and living abroad.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/davidrpugh" title="davidrpugh"><svg class="svg-icon grey"><use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/TheSandyCoder" title="TheSandyCoder"><svg class="svg-icon grey"><use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
