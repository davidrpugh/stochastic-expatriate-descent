<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Dueling Network Architectures | Stochastic Expatriate Descent</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Dueling Network Architectures" />
<meta name="author" content="David R. Pugh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="???" />
<meta property="og:description" content="???" />
<link rel="canonical" href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html" />
<meta property="og:url" content="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html" />
<meta property="og:site_name" content="Stochastic Expatriate Descent" />
<meta property="og:image" content="https://davidrpugh.github.io/stochastic-expatriate-descent/images/dueling-network-architectures.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-20T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"David R. Pugh"},"datePublished":"2020-04-20T00:00:00-05:00","headline":"Dueling Network Architectures","image":"https://davidrpugh.github.io/stochastic-expatriate-descent/images/dueling-network-architectures.jpeg","description":"???","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html"},"@type":"BlogPosting","url":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html","dateModified":"2020-04-20T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/stochastic-expatriate-descent/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml" title="Stochastic Expatriate Descent" /><link rel="shortcut icon" type="image/x-icon" href="/stochastic-expatriate-descent/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Dueling Network Architectures | Stochastic Expatriate Descent</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Dueling Network Architectures" />
<meta name="author" content="David R. Pugh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="???" />
<meta property="og:description" content="???" />
<link rel="canonical" href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html" />
<meta property="og:url" content="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html" />
<meta property="og:site_name" content="Stochastic Expatriate Descent" />
<meta property="og:image" content="https://davidrpugh.github.io/stochastic-expatriate-descent/images/dueling-network-architectures.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-20T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"David R. Pugh"},"datePublished":"2020-04-20T00:00:00-05:00","headline":"Dueling Network Architectures","image":"https://davidrpugh.github.io/stochastic-expatriate-descent/images/dueling-network-architectures.jpeg","description":"???","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html"},"@type":"BlogPosting","url":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html","dateModified":"2020-04-20T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml" title="Stochastic Expatriate Descent" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/stochastic-expatriate-descent/">Stochastic Expatriate Descent</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/stochastic-expatriate-descent/about/">About Me</a><a class="page-link" href="/stochastic-expatriate-descent/search/">Search</a><a class="page-link" href="/stochastic-expatriate-descent/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Dueling Network Architectures</h1><p class="page-description">???</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-20T00:00:00-05:00" itemprop="datePublished">
        Apr 20, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">David R. Pugh</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      25 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#deep-reinforcement-learning">deep-reinforcement-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#deep-q-networks">deep-q-networks</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/davidrpugh/stochastic-expatriate-descent/tree/2020-04-20-dueling-network-architectures/_notebooks/2020-04-20-dueling-network-architectures.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/davidrpugh/stochastic-expatriate-descent/2020-04-20-dueling-network-architectures?filepath=_notebooks%2F2020-04-20-dueling-network-architectures.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/davidrpugh/stochastic-expatriate-descent/blob/2020-04-20-dueling-network-architectures/_notebooks/2020-04-20-dueling-network-architectures.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-20-dueling-network-architectures.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I am continuing to work my way through the <a href="https://www.udacity.com/">Udacity</a> <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893"><em>Deep Reinforcement Learning Nanodegree</em></a>. In this blog post I discuss and implement an the dueling network architecture from <a href="https://arxiv.org/abs/1511.06581"><em>Dueling Network Architectures for Deep Reinforcement Learning</em></a> (Wang et al 2016).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Background">Background<a class="anchor-link" href="#Background"> </a></h2><p>The overall setup in <a href="https://arxiv.org/pdf/1511.06581.pdf">Wang et al 2016</a> is similar to 
previous work in the deep RL literature. The agent seeks maximize the expected discounted 
return, where we define the discounted return as</p>
<p>
$$ R_t = \sum_{s=0}^{\infty} \gamma^s r_{t+s} $$
</p>
<p>where $\gamma \in [0, 1]$ is a discount factor that determines how the RL agent should value 
immediate versus long-term rewards. For an RL agent behaving according to a stochastic policy 
$\pi$, the values of the state-action pair $(s, a)$ and the state $s$ are defined as follows.</p>
\begin{align}
Q^{\pi}(s, a) =&amp; \mathbb{E} \big[R_t \big| s_t = s, a_t = a, \pi\big] \\
V^{\pi}(s) =&amp; \mathbb{E}_{a\sim \pi(s)}\big[Q^{\pi}(s, a)\big]
\end{align}<p>The preceding state-action value function, or Q function, can be computed recursively with 
dynamic programming.</p>
<p>
$$ Q^{\pi}(s, a) = \mathbb{E}_{s'}\bigg[r + \gamma \mathbb{E}_{a'\sim \pi(s')}\big[Q^{\pi}(s', a')\big] \bigg| s, a, \pi \bigg] $$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Advantage-Function">Advantage Function<a class="anchor-link" href="#Advantage-Function"> </a></h3><p>At this point, <a href="https://arxiv.org/pdf/1511.06581.pdf">Wang et al 2016</a>, begin to deviate 
a bit previous work from previous work by defining an <em>advantage function</em> which relates 
the $V$ and $Q$ functions as follows.</p>
<p>
$$ A^{\pi}(s, a) = Q^{\pi}(s, a) − V^{\pi}(s) $$
</p>
<p>Note that assuming that the agent chooses its actions using policy $\pi$ it follows that</p>
<p>
$$ \mathbb{E}_{a\sim\pi(s)}\big[A^{\pi}(s, a)\big] = 0. $$
</p>
<p>Intuitively, the value function $V$ measures how "good" it is to be in a particular state $s$. The 
$Q$ function measures the the value of choosing a particular action when in state $s$. The 
advantage function, $A$, subtracts the value of being in state $s$ from the $Q$ function to obtain 
a relative measure of the importance of each action in state $s$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Deep-Q-Networks">Deep Q-Networks<a class="anchor-link" href="#Deep-Q-Networks"> </a></h3><p><a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">Minh et al 2015</a> approximate the value functions defined above using deep neural networks which they refere to as deep Q-networks. In particular Minh et al 2015 parameterize the Q-function using $Q(s, a; \theta)$ and then seek to find parameters $\theta$ of some neural network that minimize the follwing sequence of loss functions at iteration $i$:</p>
<p>
$$ L_i(\theta_i) = \mathbb{E}_{s,a,r,s'} \bigg[y_i^{DQN} − Q(s, a; \theta_i)\bigg]^2 $$
</p>
<p>with</p>
<p>
$$ y_i^{DQN} = r + \gamma \underset{a'}{max}\ Q(s', a'; \theta^{-}) $$
</p>
<p>where $\theta^{-}$ represents the parameters of a separate, target network whose parameters, because of convergence issues, are only occasionally updated. If you are interested in more details about deep Q-networks check out my <a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html">previous post</a> (or even better read the original paper).</p>
<p>In the cell below I implement several useful type annotations and functions that come from Minh et al 2015 that will be used later in this post.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">QNetwork</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span>

<span class="k">def</span> <span class="nf">synchronize_q_networks</span><span class="p">(</span><span class="n">q_network_1</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">,</span> <span class="n">q_network_2</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;In place, synchronization of q_network_1 and q_network_2.&quot;&quot;&quot;</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">q_network_1</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">q_network_2</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">select_greedy_actions</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">q_network</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Select the greedy action for the current state given some Q-network.&quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">actions</span> <span class="o">=</span> <span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">actions</span>


<span class="k">def</span> <span class="nf">evaluate_selected_actions</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">q_network</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Compute the Q-values by evaluating the actions given the current states and Q-network.&quot;&quot;&quot;</span>
    <span class="n">next_q_values</span> <span class="o">=</span> <span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="p">)</span>        
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q_values</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">q_values</span>


<span class="k">def</span> <span class="nf">q_learning_update</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                      <span class="n">q_network</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Q-Learning uses a q-network to select and evaluate actions.&quot;&quot;&quot;</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">select_greedy_actions</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">q_network</span><span class="p">)</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">evaluate_selected_actions</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">q_network</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_values</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Double-Deep-Q-Networks">Double Deep Q-Networks<a class="anchor-link" href="#Double-Deep-Q-Networks"> </a></h3><p><a href="https://arxiv.org/abs/1509.06461">Van Hasselt et al (2015)</a> combined double Q-learning and deep 
Q-networks to obtain a much improved algorithm called double deep Q-networks (DDQN). For more 
detailed discussion of the DDQN algorithm see either my 
<a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/11/double-dqn.html">previous blog post</a> 
(or better yet read the <a href="https://arxiv.org/pdf/1509.06461.pdf">original paper</a>).</p>
<p>The DDQN algorithm uses the online Q-network parameterized by $\theta$ to choose actions but uses 
the target Q-network parameterized by $\theta^{-}$ to evaluated the chosen actions and compute the 
q-values. DDQN uses the following rule for computing the target.</p>
<p>
$$ y_i^{DDQN} = r + \gamma Q\big(s', \underset{a'}{\mathrm{argmax}}Q(s', a'; \theta_i); \theta^{-}\big) $$
</p>
<p>In the cell below I provide my Python implementation of this update rule.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">double_q_learning_update</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                             <span class="n">q_network_1</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">,</span>
                             <span class="n">q_network_2</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Double Q-Learning uses Q-network 1 to select actions and Q-network 2 to evaluate the selected actions.&quot;&quot;&quot;</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">select_greedy_actions</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">q_network_1</span><span class="p">)</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">evaluate_selected_actions</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">q_network_2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_values</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prioritized-Experience-Replay">Prioritized Experience Replay<a class="anchor-link" href="#Prioritized-Experience-Replay"> </a></h3><p><a href="https://arxiv.org/abs/1511.05952">Schaul et al (2016)</a> introduced prioritized experience replay 
which increased the replay probability of experience tuples that have a high expected learning 
progress (as measured via the proxy of absolute temporal difference (TD) error). For more details 
see my 
<a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html">previous blog post</a> 
(or better yet read the <a href="https://arxiv.org/pdf/1511.05952.pdf">original paper</a>).</p>
<p>In the cells below I provide and implementation of the TD errors for both Q and double Q-learning 
as well a an implementation of a <code>PrioritizedExperienceReplayBuffer</code> that encapsulates the key 
ideas from Schaul et al (2016).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">q_learning_error</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">next_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                     <span class="n">q_network</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Q-learning temporal-difference (TD) error.&quot;&quot;&quot;</span>
    <span class="n">expected_q_values</span> <span class="o">=</span> <span class="n">q_learning_update</span><span class="p">(</span><span class="n">next_states</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">q_network</span><span class="p">)</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">expected_q_values</span> <span class="o">-</span> <span class="n">q_values</span>
    <span class="k">return</span> <span class="n">delta</span>


<span class="k">def</span> <span class="nf">double_q_learning_error</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">next_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                            <span class="n">q_network_1</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">,</span>
                            <span class="n">q_network_2</span><span class="p">:</span> <span class="n">QNetwork</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Double Q-learning temporal-difference (TD) error.&quot;&quot;&quot;</span>
    <span class="n">expected_q_values</span> <span class="o">=</span> <span class="n">double_q_learning_update</span><span class="p">(</span><span class="n">next_states</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">q_network_1</span><span class="p">,</span> <span class="n">q_network_2</span><span class="p">)</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_network_1</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">expected_q_values</span> <span class="o">-</span> <span class="n">q_values</span>
    <span class="k">return</span> <span class="n">delta</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="n">_field_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;action&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reward&quot;</span><span class="p">,</span>
    <span class="s2">&quot;next_state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;done&quot;</span>
<span class="p">]</span>
<span class="n">Experience</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;Experience&quot;</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="n">_field_names</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PrioritizedExperienceReplayBuffer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Fixed-size buffer to store priority, Experience tuples.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">beta_annealing_schedule</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize an ExperienceReplayBuffer object.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        buffer_size (int): maximum size of buffer</span>
<span class="sd">        batch_size (int): size of each training batch</span>
<span class="sd">        alpha (float): Strength of prioritized sampling. Default to 0.0 (i.e., uniform sampling).</span>
<span class="sd">        beta (float): Strength of the sampling correction. Default to 0.0 (i.e., no correction).</span>
<span class="sd">        random_state (np.random.RandomState): random number generator.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span> <span class="o">=</span> <span class="n">buffer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># current number of prioritized experience tuples in buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;priority&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;experience&quot;</span><span class="p">,</span> <span class="n">Experience</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">beta</span>
        
        <span class="c1"># if not provided, assume constant beta annealing schedule</span>
        <span class="k">if</span> <span class="n">beta_annealing_schedule</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_beta_annealing_schedule</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_beta_annealing_schedule</span> <span class="o">=</span> <span class="n">beta_annealing_schedule</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span> <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">random_state</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Current number of prioritized experience tuple stored in buffer.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span>
    
    <span class="k">def</span> <span class="nf">_unpack</span><span class="p">(</span><span class="n">experiences</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">_states</span><span class="p">,</span> <span class="n">_actions</span><span class="p">,</span> <span class="n">_rewards</span><span class="p">,</span> <span class="n">_next_states</span><span class="p">,</span> <span class="n">_dones</span> <span class="o">=</span> <span class="p">(</span><span class="n">_</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">experiences</span><span class="p">))</span>  
        <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">_actions</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">long</span><span class="p">()</span>
                        <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">_rewards</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">_next_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">_rewards</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">alpha</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Strength of prioritized sampling.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Number of experience samples per training batch.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">buffer_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Maximum number of prioritized experience tuples stored in buffer.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">:</span> <span class="n">Experience</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot;</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_empty</span><span class="p">()</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_full</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">priority</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">():</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmin</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">experience</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span> <span class="c1"># low priority experiences should not be included in buffer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">experience</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">is_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;True if the buffer is empty; False otherwise.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">==</span> <span class="mi">0</span>
    
    <span class="k">def</span> <span class="nf">is_full</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;True if the buffer is full; False otherwise.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span>
    
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">episode_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Sample a batch of experiences from memory.&quot;&quot;&quot;</span>
        <span class="c1"># use sampling scheme to determine which experiences to use for learning</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span><span class="p">][</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span>
        <span class="n">sampling_probs</span> <span class="o">=</span> <span class="n">ps</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ps</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">ps</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                                         <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">,</span>
                                         <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">p</span><span class="o">=</span><span class="n">sampling_probs</span><span class="p">)</span>
        
        <span class="c1"># select the experiences and compute sampling weights</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;experience&quot;</span><span class="p">][</span><span class="n">idxs</span><span class="p">]</span>
        
        <span class="c1"># compute the sampling weights</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_annealing_schedule</span><span class="p">(</span><span class="n">episode_number</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">*</span> <span class="n">sampling_probs</span><span class="p">[</span><span class="n">idxs</span><span class="p">])</span><span class="o">**-</span><span class="n">beta</span>
        <span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">weights</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">idxs</span><span class="p">,</span> <span class="n">experiences</span><span class="p">,</span> <span class="n">normalized_weights</span>

    <span class="k">def</span> <span class="nf">update_priorities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">priorities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update the priorities associated with particular experiences.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">][</span><span class="n">idxs</span><span class="p">]</span> <span class="o">=</span> <span class="n">priorities</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dueling-Network-Architecture">Dueling Network Architecture<a class="anchor-link" href="#Dueling-Network-Architecture"> </a></h2><p>The motivation for the new Q-network architecture developed by Wang et al (2016) is the observation that in many practical applications it is unnecessary to estimate the value of each action choice in every state. In some states, it is obviously critical to know which action to take (think of an autonomous vehicle about to hit a pedistrian!), but in many other states the choice of action has no real, observable impact. Bootstrapping-based RL algorithms, however, estimate the value of each action for every state.</p>
<p><img src="/stochastic-expatriate-descent/images/copied_from_nb/my_icons/dueling-network-architectures.jpeg" alt="" /></p>
<p>The lower layers of the network are shared and can be though of as a kind of feature extractor. Instead of following these layers with a single sequence of fully-connected, dense layers, the dueling network architecture splits into two streams of fully-connected, dense layers. One stream of fully connected layers is used to estimate the value function directly, while the second stream of layers is used to estimate the advantage function. The two streams are then re-combined using equation ??? above to estimate the Q-function.</p>
<p>There are some technical issues given any Q-function it is not possible to recover both V and A functions uniquely (technically, this means that the parameters $\theta^{F}, \theta^{A}, \theta^{V}$ are not identifiable from the data). In order to solve this issue Wang et al (2016) force the advantage function estimator to have zero advantage for the chosen action. That is, we let the last module of the network implement the forward mapping</p>
$$ Q(s, a; \theta^{F}, \theta^{A}, \theta^{V}) = V (s; \theta^{F}, \theta^{V}) + \bigg(A(s, a; \theta^{F}, \theta^{A}) − \underset{a'}{\max} A(s, a'; \theta^{F}, \theta^{A})\bigg) $$<p></p>
<p>trying to An alternative module replaces the max operator with an
average:</p>
$$ Q(s, a; \theta^{F}, \theta^{A}, \theta^{V}) = V (s; \theta^{F}, \theta^{V}) + \bigg(A(s, a; \theta^{F}, \theta^{A}) − \frac{1}{|A|}\sum_{a'}A(s, a'; \theta^{F}, \theta^{A})\bigg) $$<p></p>
<p>Since the output of the dueling network architecture is a Q-function, it can be trained with either the DQN or DDQN training algorithms and can also take advantage
of other advances such as better replay memories, better exploration policies, etc.</p>
<p>In the cell below I wrap up these ideas into a PyTorch <code>nn.Module</code>. All the action is in the implementation of the <code>forward</code> method of the <code>DuelingDeepQNetwork</code> module which will be used to compute the forward pass during training.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">DuelingDeepQNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">advantage_q_network_fn</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">QNetwork</span><span class="p">],</span>
                 <span class="n">feature_extractor_q_network_fn</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">QNetwork</span><span class="p">],</span>
                 <span class="n">value_q_network_fn</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">QNetwork</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_advantage_q_network_fn</span> <span class="o">=</span> <span class="n">advantage_q_network_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feature_extractor_q_network_fn</span> <span class="o">=</span> <span class="n">feature_extractor_q_network_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_value_q_network_fn</span> <span class="o">=</span> <span class="n">value_q_network_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">_initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create the various Q-network instances.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feature_extractor_q_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_extractor_q_network_fn</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_advantage_q_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_advantage_q_network_fn</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_value_q_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_q_network_fn</span><span class="p">()</span>
        
        
    <span class="k">def</span> <span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DuelingDeepQNetwork&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Return a DuelingDeepQNetwork with the same network architecture as self.&quot;&quot;&quot;</span>
        <span class="n">q_network</span> <span class="o">=</span> <span class="n">DuelingDeepQNetwork</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_advantage_q_network_fn</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">_feature_extractor_q_network_fn</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">_value_q_network_fn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">q_network</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Forward pass combines the three Q-networks.&quot;&quot;&quot;</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_extractor_q_network</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_advantage_q_network</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_q_network</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span> <span class="o">+</span> <span class="n">advantage</span> <span class="o">-</span> <span class="n">advantage</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">synchronize_with</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;DuelingDeepQNetwork&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Synchronize the weights of self with those of other.&quot;&quot;&quot;</span>
        <span class="n">synchronize_q_networks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Refactoring-the-DeepQAgent-class">Refactoring the <code>DeepQAgent</code> class<a class="anchor-link" href="#Refactoring-the-DeepQAgent-class"> </a></h2><p>Other than continuing to clean up internal implementation details, nothing really changed from the 
implementation of the <code>DeepQAgent</code> from my previous posts.  I added two additional parameters to 
the constructor: <code>alpha</code> which controls the strength of the prioritization sampling and 
<code>beta_annealing_schedule</code> (discussed in detail below) which allows the strength of the sampling 
bias correction (i.e., the importance sampling weights) to increase as training progresses.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>GymState</code> is represented by an <code>np.ndarray</code> with shape <code>(210, 160, 3)</code> and dtype <code>np.uint8</code>. Need to convert the <code>GymState</code> into a <code>DeepQAgent</code> internal representation of state which is a <code>torch.Tensor</code> with shape <code>(1, 3, 210, 160)</code> and dtype <code>torch.float32</code>. We can accomplish this by defining a pre-processing function that takes an <code>ndarray</code> input and returns a <code>torch.Tensor</code>. This function can encapsulate what ever pre-processing steps that need to be included to convert a raw <code>GymState</code> to a suitable <code>torch.Tensor</code>.  Here I make use of the <code>torchvision.transforms</code> module which contains exactly the transformation that I need!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>


<span class="n">GymState</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>


<span class="k">def</span> <span class="nf">preprocessing_fn</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">GymState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Converts a Gym state with shape (H, W, C) to a torch.Tensor with shape (1, C, H, W).&quot;&quot;&quot;</span>
    <span class="n">state_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">state</span><span class="p">)</span>
                              <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">state_tensor</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>


<span class="n">Action</span> <span class="o">=</span> <span class="nb">int</span>
<span class="n">Reward</span> <span class="o">=</span> <span class="nb">float</span>
<span class="n">Done</span> <span class="o">=</span> <span class="nb">bool</span>


<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">GymState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
    
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load an Agent state from a saved checkpoint.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        
    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">state</span><span class="p">:</span> <span class="n">GymState</span><span class="p">,</span>
             <span class="n">action</span><span class="p">:</span> <span class="n">Action</span><span class="p">,</span>
             <span class="n">reward</span><span class="p">:</span> <span class="n">Reward</span><span class="p">,</span>
             <span class="n">next_state</span><span class="p">:</span> <span class="n">GymState</span><span class="p">,</span>
             <span class="n">done</span><span class="p">:</span> <span class="n">Done</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update internal state after observing effect of action on the environment.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="n">NotImplmentedError</span>


<span class="k">class</span> <span class="nc">DeepQAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dueling_dqn</span><span class="p">:</span> <span class="n">DuelingDeepQNetwork</span><span class="p">,</span>
                 <span class="n">replay_buffer</span><span class="p">:</span> <span class="n">PrioritizedExperienceReplayBuffer</span><span class="p">,</span>
                 <span class="n">preprocessing_fn</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="n">GymState</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">optimizer_fn</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="n">typing</span><span class="o">.</span><span class="n">Iterable</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]],</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span>
                 <span class="n">number_actions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">epsilon_decay_schedule</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
                 <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">update_frequency</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize a DeepQAgent.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        dueling_dqn (DuelingDeepQNetwork):</span>
<span class="sd">        optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer.</span>
<span class="sd">        epsilon_decay_schdule (callable): function that takes episode number and returns 0 &lt;= epsilon &lt; 1.</span>
<span class="sd">        alpha (float): rate at which the target q-network parameters are updated.</span>
<span class="sd">        gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1).</span>
<span class="sd">        update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated.</span>
<span class="sd">        seed (int): random seed</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        
        <span class="c1"># set seeds for reproducibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span> <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># initialize agent hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span> <span class="o">=</span> <span class="n">replay_buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_actions</span> <span class="o">=</span> <span class="n">number_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_decay_schedule</span> <span class="o">=</span> <span class="n">epsilon_decay_schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span> <span class="o">=</span> <span class="n">update_frequency</span>
        
        <span class="c1"># initialize Q-Networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_preprocessing_fn</span> <span class="o">=</span> <span class="n">preprocessing_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span> <span class="o">=</span> <span class="n">dueling_dqn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span>
                                      <span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">synchronize_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">)</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        
        <span class="c1"># initialize the optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

        <span class="c1"># initialize some counters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">GymState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Epsilon-greedy action given the current state of the environment.&quot;&quot;&quot;</span>
        <span class="n">_state</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_preprocessing_fn</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                      <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
            
        <span class="c1"># choose uniform at random if agent has insufficient experience</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_sufficient_experience</span><span class="p">():</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uniform_random_policy</span><span class="p">(</span><span class="n">_state</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_decay_schedule</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_greedy_policy</span><span class="p">(</span><span class="n">_state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>
           
    <span class="k">def</span> <span class="nf">_uniform_random_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_actions</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_greedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Choose action that maximizes the Q-values given the current state.&quot;&quot;&quot;</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">select_greedy_actions</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="p">(</span><span class="n">actions</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>  <span class="c1"># actions might reside on the GPU!</span>
                         <span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="k">def</span> <span class="nf">_epsilon_greedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uniform_random_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="k">def</span> <span class="nf">_ddqn_algorithm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">idxs</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">next_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">sampling_weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Double deep Q-network (DDQN) algorithm with prioritized experience replay.&quot;&quot;&quot;</span>

        <span class="c1"># compute the temporal difference errors</span>
        <span class="n">deltas</span> <span class="o">=</span> <span class="n">double_q_learning_error</span><span class="p">(</span><span class="n">states</span><span class="p">,</span>
                                         <span class="n">actions</span><span class="p">,</span>
                                         <span class="n">rewards</span><span class="p">,</span>
                                         <span class="n">next_states</span><span class="p">,</span>
                                         <span class="n">dones</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="p">)</span>
        
        <span class="c1"># update experience priorities</span>
        <span class="n">priorities</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">update_priorities</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">priorities</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="c1"># priorities must be positive!</span>
        
        <span class="c1"># compute the mean squared loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">deltas</span> <span class="o">*</span> <span class="n">sampling_weights</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># updates the parameters of the online network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># don&#39;t forget to synchronize the target and online networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">synchronize_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">)</span> 
    
    <span class="k">def</span> <span class="nf">has_sufficient_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;True if agent has enough experience to train on a batch of samples; False otherwise.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">batch_size</span>
    
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;q-network-state&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">synchronize_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;optimizer-state&quot;</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves the state of the DeepQAgent.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        filepath (str): filepath where the serialized state should be saved.</span>
<span class="sd">        </span>
<span class="sd">        Notes:</span>
<span class="sd">        ------</span>
<span class="sd">        The method uses `torch.save` to serialize the state of the q-network, </span>
<span class="sd">        the optimizer, as well as the dictionary of agent hyperparameters.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;q-network-state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;optimizer-state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;experience_replay_buffer&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
                <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="s2">&quot;beta_annealing_schedule&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_annealing_schedule</span><span class="p">,</span>
                <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">buffer_size</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;agent-hyperparameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;epsilon_decay_schedule&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_decay_schedule</span><span class="p">,</span>
                <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">,</span>
                <span class="s2">&quot;update_frequency&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">state</span><span class="p">:</span> <span class="n">GymState</span><span class="p">,</span>
             <span class="n">action</span><span class="p">:</span> <span class="n">Action</span><span class="p">,</span>
             <span class="n">reward</span><span class="p">:</span> <span class="n">Reward</span><span class="p">,</span>
             <span class="n">next_state</span><span class="p">:</span> <span class="n">GymState</span><span class="p">,</span>
             <span class="n">done</span><span class="p">:</span> <span class="n">Done</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update internal state after observing effect of action on the environment.&quot;&quot;&quot;</span>
        <span class="n">state_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocessing_fn</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocessing_fn</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
        <span class="n">experience</span> <span class="o">=</span> <span class="n">Experience</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state_tensor</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span> 

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># every so often the agent should learn from experiences</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_sufficient_experience</span><span class="p">():</span>
                <span class="n">idxs</span><span class="p">,</span> <span class="n">_experiences</span><span class="p">,</span> <span class="n">_sampling_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span><span class="p">)</span>
                
                <span class="c1"># unpack the experiences</span>
                <span class="n">_states</span><span class="p">,</span> <span class="n">_actions</span><span class="p">,</span> <span class="n">_rewards</span><span class="p">,</span> <span class="n">_next_states</span><span class="p">,</span> <span class="n">_dones</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">_experiences</span><span class="p">))</span>  
                <span class="n">states</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                               <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
                <span class="n">actions</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">_actions</span><span class="p">)</span>
                                <span class="o">.</span><span class="n">long</span><span class="p">()</span>
                                <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                                <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
                <span class="n">rewards</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">_rewards</span><span class="p">)</span>
                                <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                                <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
                <span class="n">next_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">_next_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                                    <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
                <span class="n">dones</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">_rewards</span><span class="p">)</span>
                               <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                               <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
                
                <span class="c1"># reshape sampling weights</span>
                <span class="n">sampling_weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">_sampling_weights</span><span class="p">)</span>
                                         <span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                                         <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
                
                <span class="bp">self</span><span class="o">.</span><span class="n">_ddqn_algorithm</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">sampling_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Training-Loop">The Training Loop<a class="anchor-link" href="#The-Training-Loop"> </a></h2><p>The code for the training loop remains unchanged from previous posts.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">import</span> <span class="nn">gym</span>


<span class="n">Score</span> <span class="o">=</span> <span class="nb">int</span>


<span class="k">def</span> <span class="nf">_train_for_at_most</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">max_timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Score</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_timesteps</span><span class="p">):</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">score</span>

                
<span class="k">def</span> <span class="nf">_train_until_done</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Score</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="k">return</span> <span class="n">score</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span>
          <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span>
          <span class="n">checkpoint_filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
          <span class="n">target_score</span><span class="p">:</span> <span class="n">Score</span><span class="p">,</span>
          <span class="n">number_episodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">maximum_timesteps</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">Score</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reinforcement learning training loop.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    agent (Agent): an agent to train.</span>
<span class="sd">    env (gym.Env): an environment in which to train the agent.</span>
<span class="sd">    checkpoint_filepath (str): filepath used to save the state of the trained agent.</span>
<span class="sd">    number_episodes (int): maximum number of training episodes.</span>
<span class="sd">    maximum_timesteps (int): maximum number of timesteps per episode.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    scores (list): collection of episode scores from training.</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">most_recent_scores</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_episodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">maximum_timesteps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">_train_until_done</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">_train_for_at_most</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">maximum_timesteps</span><span class="p">)</span>         
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">most_recent_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
        <span class="n">average_score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">most_recent_scores</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">most_recent_scores</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">average_score</span> <span class="o">&gt;=</span> <span class="n">target_score</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Environment solved in </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">d</span><span class="si">}</span><span class="s2"> episodes!</span><span class="se">\t</span><span class="s2">Average Score: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint_filepath</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">Episode </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="se">\t</span><span class="s2">Average Score: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scores</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Solving-atari-environments">Solving <code>atari</code> environments<a class="anchor-link" href="#Solving-atari-environments"> </a></h2><p>In the rest of this blog post I will use the Double DQN algorithm with prioritized experience 
replay to train an agent to solve the 
<a href="https://gym.openai.com/envs/LunarLander-v2/">LunarLander-v2</a> environment from 
<a href="https://openai.com/">OpenAI</a>.</p>
<p><a href="https://gym.openai.com/envs/#atari">https://gym.openai.com/envs/#atari</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Google-Colab-Preamble">Google Colab Preamble<a class="anchor-link" href="#Google-Colab-Preamble"> </a></h3><p>If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash

<span class="c1"># install required system dependencies</span>
apt-get install -y xvfb x11-utils

<span class="c1"># install required python dependencies (might need to install additional gym extras depending)</span>
pip install gym<span class="o">[</span>atari<span class="o">]==</span><span class="m">0</span>.17.* <span class="nv">pyvirtualdisplay</span><span class="o">==</span><span class="m">0</span>.2.* <span class="nv">PyOpenGL</span><span class="o">==</span><span class="m">3</span>.1.* PyOpenGL-accelerate<span class="o">==</span><span class="m">3</span>.1.*
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set <code>visible=False</code>.</p>
<p><strong>This code only needs to be run once per session to start the display.</strong></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pyvirtualdisplay</span>


<span class="n">_display</span> <span class="o">=</span> <span class="n">pyvirtualdisplay</span><span class="o">.</span><span class="n">Display</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># use False with Xvfb</span>
                                    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1400</span><span class="p">,</span> <span class="mi">900</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">_display</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Binder-Preamble">Binder Preamble<a class="anchor-link" href="#Binder-Preamble"> </a></h3><p>The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set <code>visible=False</code>.</p>
<p><em>This code only needs to be run once per session to start the display.</em></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pyvirtualdisplay</span>


<span class="n">_display</span> <span class="o">=</span> <span class="n">pyvirtualdisplay</span><span class="o">.</span><span class="n">Display</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># use False with Xvfb</span>
                                    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1400</span><span class="p">,</span> <span class="mi">900</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">_display</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;Atlantis-v0&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Creating-a-DeepQAgent">Creating a <code>DeepQAgent</code><a class="anchor-link" href="#Creating-a-DeepQAgent"> </a></h3><p>Before creating an instance of the <code>DeepQAgent</code> with prioritized experience replay I need to 
define a $\beta$-annealing schedule, an $\epsilon$-decay schedule, and choose an optimizer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="$\beta$-annealing-schedule">$\beta$-annealing schedule<a class="anchor-link" href="#$\beta$-annealing-schedule"> </a></h4><p>Due to the inherent non-stationarity of the RL training process, Schaul et al 2016 hypothesize 
that a small sampling bias can be ignored during early training episodes. Instead of fixing 
$\beta=1$ (and fully correcting for the bias throughout training) they increase the amount of 
importance sampling correction as the number of training episodes increase by defining a schedule 
for $\beta$ that reaches 1 (i.e., full bias correction) only near the end of training.</p>
<p>Note that the choice of $\beta$ interacts with choice of prioritization exponent $\alpha$: 
increasing both simultaneously prioritizes sampling more aggressively while at the same time as 
correcting for it more strongly.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># define some annealing schedue for beta</span>
<span class="n">rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">_beta_annealing_schedule</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rate</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>

<span class="n">_replay_buffer_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s2">&quot;beta_annealing_schedule&quot;</span><span class="p">:</span> <span class="n">_beta_annealing_schedule</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mi">1000000</span><span class="p">,</span>
    <span class="s2">&quot;random_state&quot;</span><span class="p">:</span> <span class="kc">None</span>
<span class="p">}</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">PrioritizedExperienceReplayBuffer</span><span class="p">(</span><span class="o">**</span><span class="n">_replay_buffer_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="$\epsilon$-decay-schedule">$\epsilon$-decay schedule<a class="anchor-link" href="#$\epsilon$-decay-schedule"> </a></h4><p>As was the case with the DQN and Double DQN algorithms, the agent chooses its action using an 
$\epsilon$-greedy policy. When using an $\epsilon$-greedy policy, with probability $\epsilon$, the 
agent explores the state space by choosing an action uniformly at random from the set of feasible 
actions; with probability $1-\epsilon$, the agent exploits its current knowledge by choosing the 
optimal action given that current state.</p>
<p>As the agent learns and acquires additional knowledge about it environment it makes sense to 
<em>decrease</em> exploration and <em>increase</em> exploitation by decreasing $\epsilon$. In practice, it isn't 
a good idea to decrease $\epsilon$ to zero; instead one typically decreases $\epsilon$ over time 
according to some schedule until it reaches some minimum value.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">power_decay_schedule</span><span class="p">(</span><span class="n">episode_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">decay_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                         <span class="n">minimum_epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">decay_factor</span><span class="o">**</span><span class="n">episode_number</span><span class="p">,</span> <span class="n">minimum_epsilon</span><span class="p">)</span>

<span class="n">_epsilon_decay_schedule_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;decay_factor&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;minimum_epsilon&quot;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">epsilon_decay_schedule</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">power_decay_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="o">**</span><span class="n">_epsilon_decay_schedule_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Choosing-an-optimizer">Choosing an optimizer<a class="anchor-link" href="#Choosing-an-optimizer"> </a></h4><p>Given the good results I achieved in my previous post using the <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam</a> optimizer I decided to continue to use that optimizer here.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>


<span class="n">_optimizer_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="s2">&quot;betas&quot;</span><span class="p">:(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="mf">1e-08</span><span class="p">,</span>
    <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;amsgrad&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">optimizer_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">_optimizer_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Creating-a-dueling-Q-network-architecture">Creating a dueling Q-network architecture<a class="anchor-link" href="#Creating-a-dueling-Q-network-architecture"> </a></h3><p>Now I am ready to create an instance of the <code>DuelingQNetwork</code> class. I need to define three, no-arg functions that return the <code>feature_extractor_q_network</code>, <code>value_q_network</code> and <code>advantage_q_network</code>, respectively. I am going to use the same network structure from the paper.</p>
<ul>
<li><code>feature_extractor_q_network</code> consists of three convolutional layers with ReLU activations. First convolutional layer has 32 filters each using a kernel size of 8 and a stride of 4. Second convolutional layer has 64 filters each using a kernel of size 4 and a stride of 2. The final convolutional layer also has 64 filters but uses a kernel of size 2 and a stride of 1.</li>
<li><code>value_q_network</code> consists of two layers. The first layer simply flattens the inputs from the <code>feature_extractor_q_network</code>. The second layer is just a dense, fully-connected layer followed by a ReLU activation function. The final layer of the <code>value_q_network</code> outputs a single number representing the value of a particular state.</li>
<li><code>advantage_q_network</code> also consists of two layers. The first layer flattens the inputs from the <code>feature_extractor_q_network</code>. The second layer is just a dense, fully-connected layer followed by a ReLU activation function. The final layer of the <code>value_q_network</code> has the same number of outputs as there are valid actions.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LambdaLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_f</span> <span class="o">=</span> <span class="n">f</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">atari_feature_extractor_q_network_fn</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">QNetwork</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Defines the feature extractor Q-network.&quot;&quot;&quot;</span>
    <span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">q_network</span>


<span class="k">def</span> <span class="nf">atari_value_q_network_fn</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">QNetwork</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Defines the value Q-network (computes the value of each state).&quot;&quot;&quot;</span>
    <span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">LambdaLayer</span><span class="p">(</span><span class="k">lambda</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">25024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">q_network</span>


<span class="k">def</span> <span class="nf">make_atari_advantage_q_network_fn</span><span class="p">(</span><span class="n">number_actions</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">QNetwork</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Return a function representing the advantage Q-network.&quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="nf">atari_advantage_q_network_fn</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">QNetwork</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Defines the advantage Q-network (computes the benefit of taking each action a given the state).&quot;&quot;&quot;</span>
        <span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">LambdaLayer</span><span class="p">(</span><span class="k">lambda</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">25024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">number_actions</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">q_network</span>
    
    <span class="k">return</span> <span class="n">atari_advantage_q_network_fn</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">NUMBER_ACTIONS</span> <span class="o">=</span> <span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span>
                     <span class="o">.</span><span class="n">n</span><span class="p">)</span>

<span class="n">dueling_dqn</span> <span class="o">=</span> <span class="n">DuelingDeepQNetwork</span><span class="p">(</span>
    <span class="n">make_atari_advantage_q_network_fn</span><span class="p">(</span><span class="n">NUMBER_ACTIONS</span><span class="p">),</span>
    <span class="n">atari_feature_extractor_q_network_fn</span><span class="p">,</span>
    <span class="n">atari_value_q_network_fn</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-the-DeepQAgent">Training the <code>DeepQAgent</code><a class="anchor-link" href="#Training-the-DeepQAgent"> </a></h3><p>Now I am finally ready to train the <code>deep_q_agent</code>. The target score for the <code>LunarLander-v2</code> environment is 200 points on average for at least 100 consecutive episodes. First, I will train an RL agent with $\alpha=0.0$ and $\beta=0$ (throught training) which will recover the uniform random sampling baseline. Then I will re-train the RL agent using prioritized sampling for comparison.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Uniform-random-sampling">Uniform random sampling<a class="anchor-link" href="#Uniform-random-sampling"> </a></h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">_agent_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;dueling_dqn&quot;</span><span class="p">:</span> <span class="n">dueling_dqn</span><span class="p">,</span>
    <span class="s2">&quot;replay_buffer&quot;</span><span class="p">:</span> <span class="n">replay_buffer</span><span class="p">,</span>
    <span class="s2">&quot;preprocessing_fn&quot;</span><span class="p">:</span> <span class="n">preprocessing_fn</span><span class="p">,</span>
    <span class="s2">&quot;number_actions&quot;</span><span class="p">:</span> <span class="n">NUMBER_ACTIONS</span><span class="p">,</span>
    <span class="s2">&quot;optimizer_fn&quot;</span><span class="p">:</span> <span class="n">optimizer_fn</span><span class="p">,</span>
    <span class="s2">&quot;epsilon_decay_schedule&quot;</span><span class="p">:</span> <span class="n">epsilon_decay_schedule</span><span class="p">,</span>
    <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;update_frequency&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">double_dqn_agent</span> <span class="o">=</span> <span class="n">DeepQAgent</span><span class="p">(</span><span class="o">**</span><span class="n">_agent_kwargs</span><span class="p">)</span>

<span class="n">uniform_sampling_scores</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">double_dqn_agent</span><span class="p">,</span>
                                <span class="n">env</span><span class="p">,</span>
                                <span class="n">checkpoint_filepath</span><span class="o">=</span><span class="s2">&quot;uniform-sampling-checkpoint.pth&quot;</span><span class="p">,</span>
                                <span class="n">number_episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">target_score</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 11min 27s, sys: 2min 18s, total: 13min 45s
Wall time: 10min 41s
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Simulating-the-behavior-of-the-trained-agent">Simulating the behavior of the trained agent<a class="anchor-link" href="#Simulating-the-behavior-of-the-trained-agent"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>


<span class="k">def</span> <span class="nf">simulate</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">ax</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">Axes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">))</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">img</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">))</span> 
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        <span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">())</span>
        <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>       
    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">simulate</span><span class="p">(</span><span class="n">double_dqn_agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Plotting-the-time-series-of-scores">Plotting the time series of scores<a class="anchor-link" href="#Plotting-the-time-series-of-scores"> </a></h4><p>I can use <a href="https://pandas.pydata.org/">Pandas</a> to quickly plot the time series of scores along with a 100 episode moving average. Most obvious difference between the two different sampling strategies is that prioritized sampling reduces, perhaps even eliminates, the significant number of large negative scores. Perhaps this is because prioritized sampling replays exactly those experiences that generate, at least initially, large losses (in magnitude). Over time the RL agent using prioritized sampling learns how to handle those awkward state transitions that led to <em>really</em> poor scores much better than an RL agent that uses uniform random sampling throughout.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">uniform_sampling_scores</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[15000.0,
 25700.0,
 12900.0,
 20100.0,
 15100.0,
 14500.0,
 21300.0,
 40900.0,
 27800.0,
 20300.0]</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ModuleNotFoundError</span>                       Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-39-7dd3504c366f&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">import</span> pandas <span class="ansi-green-fg">as</span> pd

<span class="ansi-red-fg">ModuleNotFoundError</span>: No module named &#39;pandas&#39;</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">uniform_sampling_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">uniform_sampling_scores</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scores&quot;</span><span class="p">)</span>
<span class="n">prioritized_sampling_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">prioritized_sampling_scores</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scores&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">uniform_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Uniform Sampling&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="p">(</span><span class="n">uniform_sampling_scores</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
               <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
               <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;Rolling Average&quot;</span><span class="p">)</span>
               <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">prioritized_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Double DQN Scores&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="p">(</span><span class="n">prioritized_sampling_scores</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
                      <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                      <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;Rolling Average&quot;</span><span class="p">)</span>
                      <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Episode Number&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAnMAAAFzCAYAAABVWI+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3xV5f3A8c9zR272DgmBkEUCIYwgYU8FQcFdFbW4xVX3qquuWqtd1q2o/Wm1RVutrVWKCqg4QNmyd4AwAiFkkXXH8/vjjtyb3CQ3ISEJfN+vFy9yzz3jOeee8T3PVFprhBBCCCFE92To7AQIIYQQQoi2k2BOCCGEEKIbk2BOCCGEEKIbk2BOCCGEEKIbk2BOCCGEEKIbk2BOCCGEEKIbM3V2AjpLfHy8TktL6+xkCCGEEEK0aMWKFcVa6wR/3520wVxaWhrLly/v7GQIIYQQQrRIKbWrqe+kmFUIIYQQohuTYE4IIYQQohuTYE4IIYQQohuTYE4IIQCtNTJWtegsZdVWiitrOzsZopuSYE60mtXu4MjRukbTy6qsHCyv6YQUNVZrs7Pr8FHu/sca6myODtlGcWWt3+MAsLe0mucXbm234KDO5uD77cUBzetwaN5ZUkCN1d7ivF9vOURZtbXF+Wx2B1uLKnw+u9f//bZiymuc69BaU1ReQ2WtDZv92I/7il0lHKqopdZm57GP13OoovmH3f6yaqyu7Vqb2X6dzcEv/r6SNXtKsdkdlFVbSX9gHq8t3oHd0fRvtnpPKQ6v7wuPVHHvP9dwtNZGRY3/47inpIq//eBbb1lrTcnROl79ejtfbCji/WW7m92v299bxe/mb2p0PpXXWCmtqvPs+8rdRxoteyxB6t7Saj5ff8Dz2e7QPsenzuZgbWEZz8zf1Oy1v3THYQY99hnvLCkAnPeKLzcdDCgNWmuemb+JWW/8wE+Fpc3O5309/t93O/l2q/OaqbHaeWfpLs+94PvtxXy1uX77K3cfYf2+MqrrnOd0VZ2Nqc9+zVebD3LHe6tYt7fMM++mA+W8vngHAEXlNXy0qpCD5TUUV9ZSeKQKcB631XtKWbL9MG98s4O7/7GGUU8txO7QfLetmB93lnjWV11n53BlLUMe/5z8Jxd4zi+HQ7OnpAqtnce8pd9Rax3QNd+UGqvd59x2K6+x8t6Pu1leUML2Q5UAHKyo4U9fbMFqd1Bnc/CXb3dyoKyGu/+xht2Hq9qcBoBtByv5/WeNz/WG5q87QFmV/2uu1mb3/JbethRVcMVffvTsq8OhKa2qa/La9afO5mj2HtFZ1Mn6Jpqfn69P9NasVruDrzcfYsqAxBbnrbM5KDh8lOzEiBbnveO9Vfx79T52/nY6SinP9EGPfUZFjY2Cp2c0WmbNnlKW7zrCtePS/a6zqLyGJdsPc97QXo2+e2reRj5YUcjKX53uM939AJw5vI/P9F2HjzLx91/5TFv1q9OJCQtqcp/qbA4OVdbSKzrE7/daa/aV1fh8n3b/pygFO3/ru7/e23/l56dw5qCeHCyv4Y1vd3LN2HR6RFjYXFRBenwYFpOBH3aW8NXmQ2iteWB6js+6Co9UcbiyjnNf+g6Az+6YQL+kCA6W17CmsIzhaTEUHqnmsteXkp4QTnpcKMPSYvnVv9dx86RM3l26ixHpcbx+xTCUUtz/4U/MW7ufJQ9M5oKXv2dzUQWnD0jkoek5LCsoYeP+Cj5du4+Y0CCSooIZkR7LeXm9+Mu3O3nj2508dvYApg/qyf3/WsuiTQdZ/vAU8p9cwJScRF6/Yhj9Hp5PnVcQ9ceLhhATZub3n22hrKqOO6Zkc9+HP/Hr8wZy+ahUyqqs3PvBGi4fncr4rAQcDs3yXUeIDw9i04EKbv7bSjLiw7h9Sha3v7eaabmJ3HdGfzITwlmwoYiXv9rGwYpashMjeOmyU8h5ZD5BRgMf3jSGs1/8llP7JTBzeB/Kq63M+WYH10/IYOqARF5YtI03v93Z5PkwMTuBH3eWMHt8OgaD4o4p2Xy3rZifv/EDvaJD+NVZObz1fQFLd5T4LFfw9AxW7CrhZ68sAeCb+07l6reWse1gJcsfnsIna/YxqHc0320r5k9fbGm03ecuyWNKTiJGgyLYbGRZQQlWu4PLXv8BgKcvGMQlI/pgtTsoPFLNqX9wnmfnDEnm4zX7AHjmZ4P45YdrAchNjmT9vnIAnjg3l3lr9/PEuQMJMhp4b9ke7pvWD4NB8ea3O9l9+CinpMZwbl4vVuwqoarOzv0frmVvaTUxoWaGpcayYV8Z+8pqWPHwFJRSPPrxev7r2i7g+V1f/Xo7b3yzk0fPHkBxZS2P/3eDZ55J/RKoqrXzY0EJC+6ayJIdh/lpTymr95Ty8FkD2FNSxatfb+ejm8fy9ZZDjMqIZdwzX3qW/+LOCZz+7GJmjerDP5cX0r9nJAYFQ3pH89b3Bcy5fBiZPcKZ/MevAdjwxDQGPPKZZ/mrxqTx1vcFAHx+5wT2lFRx7dv1z4JfntGfZ+ZvavTbvDrrFAb0jGLC751p6ZcYwWavFxy35y8dym1zVzWaDjAhO4HFWw4BEBpkZExmHAs2+ga17mP4xjc7ePLTjeSnxrB8l2+Q/tD0HK4Zl87G/eWkxoXyn9X7eOK/G6izO3hoeg4LNxUxPiuBw5V13DAxg1/9ex2fbygC4DfnD6RXdAgvLNqG1e7g1tOy2HawkmfmbyLEbKTaaifCYsJgUFwyIgWL0cDzi7Z5th0WZOSon0DJ24K7JvD99sOckZtEfLiFw0frCA0ycqC8hlv/voqnLhhEXko0hytriQu3sLe0ml7RIdTa7Ix9+kuKK2u5aVImNVY7D7nuiU9+upFrx6VzoLyG5OgQxj69CIC/XjOC7MQIVu0+wjfbivnNeQMZ+usvKK2ycuXoVM4ekkx+WiwA57/8Hat2l3LfGf14fuFWaqz196qU2BAW3DWRkqN12B2a3jGhOByavaXVhAYZWbTpINMH9ST3Uee5dG5eMqMz4iirtnJxfkqzz5f2opRaobXO9/udBHMnrj99vpnnF23jnWtHMD7Lb9c0Hr/69zreWbqLJQ+cRs8o/wGNW9r9nwKw9TdnYjYaWLn7CLPfXs5h11txwdMzOFprY97a/Vw4rDdKKTIe+BSHhi/vmcSWogqm5SZ51vf3H3bz4EdrPZ9fvGwoUwckMWfxdi4Z0Yf8Jxd41nuoopZb567kaK2dta635Qen9yc1Loxf/XsdBytqGZYaw4oGN7+5s0cRFWJm/b4ylmw/zL9W7eXW0/py9pBkFmwsYk9JFXN/3MPKX51OrNdFabM7mP3X5Vjtmm+3FXPjxEx+eUY/lFKe47DhiWnUWB08v3Arn68/wL4y3xyKX5+by6o9pfxr5V7AeRP4z+p9ZCSEUWt1sLe02jPvsNQYshPDeer8Qcz9cY/PcQFIiLAwPives65J/RL4avOhZn8v72XdOVvXT8hgjit34Vh4PxhP69+DRQHmtriZjQqrveV70Ij0WJ/cDH8+unkM57/8PQBXjE7lr0uabMXfaleMTmXTgYoW0/DQ9Bx+M2+j53O4xURlrc1rDo0BjQEHRhyYsGPGhgkHZmwEqzo0kJTUiycvGcOUZ79ttI2L83vzj+WFbd6XvJRoVu9x5nBFWEwMSYnm2231ub4PnNmf3/6vcTATqDevzPcJjo7VpSNSmPvjnnZbnzcjdoKpI4Q6glUtUVShAQMOgqkjWFkJxnlfs2KiDhO12kw5oWzTvTskTacPSGTqgETu/eCnDll/V+EOHK8bl84b3+7k8lGpfL7hAEXljXPfP75lLOe8+F2bt/XW1cN56cttLCtonHPdlB8fnMyIpxYGNO+Fw3rzh4uGtDV5AZNgzo+TIZi78/3VfLRqL8/OHML5Q+tvPFprFm8tZkJWvCdnbeqzX7OlqJL5d4ynf1Jkk+t0ODQZD84DYOMTZxBsNpD+wDyfeQqensE9/1zDBysK+dfNY8hOjGDgo5/5zPOvm8fw1eZD3HJqX8Y8vZDiSv/Fld5WP3I6eU98EfD+e5uYncDXWxoHPe4AJy4syBOMrnl0KmFBRm5/bzW9Y0J4rUHQ8+aV+bz1fQHfbPUt9gw0MDl2mjBqiFGVWKjzBAeh1BKEDStGLMpKijqIxvn7KjRm14PLQh0W10PKgpUwarAoK3+zTcGBQgNByoYFKybsXoGHw/O3QTn/d/6zO6d5PtcHK0blnF6pQ7jY+BVxqpxSHc5u3QPAtZTCgcKdx+tMAdgwEoSNMFVNEDY0oHyOAuzXcWx0pGJQDixGRZ3dQa0O4hBRRFKFDaNrC8712jFgw8gunUilDsGKkTrMWDFhwIFGUYeZWm1yTdMMMuykpzrs2qYzdTFU0s+wx3XMncfJiAOTcuZYGHGQpg4QQq3vsVGtOz+OagsVhFKoE9Dg+Q3cv4vJ6/gbXb9JqQ7jkI5mo+5DpQ7BpBzU6CAKdTwVhGLHQDB1hFJDuKrBipFljv5EU0mRjmEfcQ2OdNMiOUq160wxYSOeMo4QQS0dn0vROpoYKuilirnMuIhEdYQsVUgfQ2AvQv58bR/MAscp1GHGgAMbRo7oCEp1GEY0IaqGBFVGtbYAYEBTg5nvHbkYcRBONeGqmnIdRiUh2DBgwkGUqqRch5KuDjDT+CVWnOdi/fXmPAfsGLBiwoaRKiwU6gRqtZlwVcMww2bM2KnDTJ02EaGqsGDFihEbJo7ocCoIxYzzOk9RBwml1nMOARzREYRSQwkRFOgkgrAShI0gbM6XEGXjqA7mKMGee4kG7BixYSRGVRBBNQU6iSIdzQpHNrt1IjYMVGOhBkvAx3ru7FFc+vrSNv9WreO8V1pb2Q3v/Wf258aJmR2UJqfmgrmTttPgk4G7BLRh1aF5aw/wi7+vZMagnjx6zgBMBgNbipx1IX4qLKN/UiRWu4OHPlrL7PEZZCVGYLU72LS/gote+96znp+/sZSpXjlsbntKqlhb6Mw1q7HaedZPcdIFrhyU5xduDXh/pj/3TcDzNuQvkIP6R9Zhr7o2n68/QEZCOJ+u3e93maZyHdoSyIVRTawqJ4kjJKojGHAQpJwPxVRVRG91iGKiiKOcJFVCtDpKNBUEqbbXjXGr1kHUYiZaHQXgHOOSY16nPw6tMLiCmG/tuTgwEK6qXQGWwxWMajR4gk8AM3Z6xESyqcRMJSEonAFcVGgQZVV1hKtqTjOs4kzjsvptGeu31dFKdRhFOoZazK4HmAGbdt5S7RhYqnPYr+NcYZeh/n/tDmANnmDSjoFbpuTw2y8KMKCJVRVEqCrCqCFBldJTlWDVRqpdwagdg2eb47MT2VhURWFZHVpDb1VMP8MeJqi1LeyBf4d1BKU6nCBsKKWxawNVWCjVEdRixoqRSFVFhtpHgnIW4dZpo+ecdGjFQaIp0ZFUYSGYOg7oGK6z3gMojNidgQzVVGOhCgu1mNHHWIX7euN/GWdYR7wqx0IdwarOmbtGHSHKioH6G+EWRy8OEMtHtnGU61DuOGMwj/1vJwblfBmowkINQdRoMzUEEWQ0gL2OIGUjhFoeNP2dicafmGjs+Nwzm3YGPw6U63d3nkdGHESaNVZrHWHKNzfLYQqh3JxAZZUziLNhpFSHe3KCexmKUWhPoHhEh1NEDHZtxIqRcFVNinLmso9QmzDioA4TdZixKRPhoWEcqLQRbagkCBs2gwVLSCglR+tc27C7AjsDk5X/IudiHYkGVjmyMLkCz5WOvhToJEp1OAU6iYPEAPDil4E/J5pjxE62KqSPOkgwtUxPLCXj8NdEqwosWLFgw6Kc9ec2O3rzgyOHMsKI4ihFOoYywqjFTDzlVBDCfh3rSXuYPbld0thWEsydwAyuaM7RIPfV3WLq07X7WbLjMHdOyfJ8d98HP3Fxfgpr95bxj+WFfL/9MAvumsjMOUtZs8e38vHK3aWs3N24QvL439XXb3lx0Ta+3364XfanYfFlezjop0L9vR/8RP+klusONmdc33gmZMfz1LxNKBwMVAUkqRLO61WO4cAaYlUFg427CNHVTa6jWEeyX8eSRhGVBLNDJ3PEEU4p4ZToCIb0y2BnqZ2NB46iUVQTRC1BmLHhwMBBHc1RHewJkC4cmcFNk3O59YMNLNhSSn0oqzlFbSUpFMqqa1HA1RP7MTm3D5sPVREZEszfl+9l3vqD2DEw/45TuXDODxQftWH3BBb1AYtv8KIwGRRhjkqsmKgi2Gcfzx6S7Klztf2p6VjtDvr/ar7n+xdOG8qtDeof/fPS0Vz9qjPwNOBgcO9oNhVVUGN1nuc9OUy8KnMFUu5w0bmvvaKCqCg/QqoqwowdE3aCsHqOmUJjUVZmj+nN3uIyFm05zFEdTHXP4azaW0WI2UCtq5L5fmI5rX8SQ/tEc2r/Hvzi7eXsK6shyGTgshF9WLX7CGsKy7h3Wj9iQoMaFZmnxYUya1QqT366kbF940icNJL/fOabyx0daqbUVcn7hokZTMlJ5CLXvv/tupHsLa0mKj+FrMpaLnFVR3D/pkHYsGFkck4S327cTao6yO/O6Uu/xFC2HLZx67+2clQHE6/KGaB2UYWFeFXGQFVAcqidYZlJ/G/dAQw4CKOGaFVJNDWYsVNDEEscudSF9GDnUQvhqppLxvbnhW8PEEEVvVQxMaqSUGrIN25hIAWsNNzAIR1NijpEaIPgo0abKSGCb+yDWaGz2OJIYY9OoBYztQRhx0CQKyixYSSEWqb2tnF4305CqeHR6X1J+uI9jEqzI2Y864ut1DiCqCEIU3AYl47JhtA4fvHxPrbqXmzRKYCz+O5orZ2IzDieGeOg70P/80nXsoem8NzCLYzrm8CN764gJsTM9RMyOX3+UIKpdeZqY+Xft07gghe/JpoK5s7qR0SIhQtfX85hojg1K4bvtpXwi9Oy+N+iRWSo/VRh4ZGfjeTuf28lwl6KyVXUrtCUEUYIdUSqo3xhH8Z6nc6nt43j3aW7GhU3Fzw+gwVr93PP374nhgosysotE9O44PRJRBlM5DUoNfE+P5rKfb13Wj9+/9lmz2eFwxNov3jZUM4anIzDoZnwYP2637wyn8k5iYx0VT1puK00dYChahthqgYjDiKoIt2wn2DqGKB2kaxKOEI4Z3i9mNm1YrXuy3ZHMrUFZk4zmRmQksAp6QnsKqlh0bpdmLBToUOxKCuJ6gjB1BJLJUZlR6Ep12FYMWGhjoGGAiI5ikV5VXs4oviBfvxo708tZhxGC0dtRqKpYIBhFxcbv8KMLaAc9VX7qoChLc7XUSSYO4EZ6p/VPkKCjJ6/S47WscpPQLanxNkiqfBINXe8t7pRIBeoQAO5CIuJCp+6RfUs1JGkSijSMa6seU0virEoK0U6BgOaiYY12DCS37cXa7btIstQSC9VTF+1j2RV7Mn63+VIpIYgSglHoanTJioJoUjHsEsn4nDlfBwqiiZNWQihjiy1l1zDTuwYyTdsplAnsNSRg0aRog5SoiPRKGedJ+oIVbXMsESQuvcIkyJ2klK3nRDlyvk7BJWGYPbrOFTeZfz2xyoOE0mxjqRQJ3hya8p1GGWENzoWN07M5NWvtwPw3OA8bs2rbzDy2foD3PDOCgCenTmEO99f47Ps1BGDCYmM4qUrRtPvYWfAdNvkLJ5fuJWVOpt7x/Xjzwu2YLVrHho8HnpG0s9VOn93/+G8+ch8qursBCdm8smvnMUJe0qqiA41E2QyeNY5Mj2WH3aW8PEtY0mLD+Os579ldwm8c+0INh+o4MlP6+uUnTkwiTumZLFhXzlGg8JoMLLgrolM+ZOz8vr0QT3ZX1bNrFGpzFm8g+SoEPJTY/jDRUO4559rcGDgspFp3PdhfQ7JfuLYr+O4caKzAvXUAYmU19i48d0VJIdEUFhmptBVzJsaF8quBq3vjAbF72dMx3Cwknc2fs3frhvJ7+Zv4gBlvHPFCPolRvDV5kNclN/bpwHQ9w9M9lnPM/M3saawDK01ceGNix2/uvdUAH4+MhWTUfmsy+3swcm8s9RZ9+/eqf0wGZ0P1bOHJDO2b7xnvrhwC3+/biRPz9/ET4Vl4CouLnh6Bn/8fDNfbAxmk+6DNTkfS2oMgzKg5gsbh8prOKRj2KhTfbY7KCKK/84chz1jD7ERFq5+a5nP93dOyWZivwQGJkd6AqArJ0/jL1/7VqcAmB5XzqNH7meP7kGxjqI8bgjzD0ZTQSgh1BFKLb3UIdLUAWaavmImXzVahzdPTm8xeEpzFwAKrq27mzdvf4QdG4qIMCqu+r9lXDU4DU7LBWC4fSefejXGGNw72vO3+9h6S4iw8OR5g/jeVafQZDQwJCUKgBpXUeGUnB6Yo3tRqBMoJIGI3KkA3H7NAHpEBNPP9WL4/fZi/udw5oIvuGsCqkcE8/89nyp7fS57iNnIyIzYRvVgEyIsNFUjKqtHOFUEU0UwO5+qb5TW8GwKMhqoszsYnxXvU0XkszsmkBQVzLdbiymrtjI6M84TzJ3SJ9rnhd3sOkYGg2L2+HRe/2YnV41JY3JOc43sFAW6JwW6pycNANi953BWcchU+wijhkhVxTjDOkYZNjDOuBYLVkIMdkIP2mF/LdlAX4sFm81GkLLj0IpazOzXsdhC4klPjOWnwjKSbYcBjR0jBcE5RPXO4fVNZsqj+vPaNRMhLI6Zj9eXNt0+MYv/+24n5TUNn0WaJEo8VSmKdAwRVJGhDmBz5ZJe3ntMJ4ZyEsyd0Nw5c5+s3c95Q3thdj0wGtaT/NeqvT6fj9baOFpbf6XN9+qaoDk5PSPZuL+8TWmdkJ3AT+tWc0PaIex7lhGuqkmgjGBVx1DjTszaGQzVaDNWZSYC5wPYrhUOZcSM6+LbjecGX6vNrNaZfOMYjAZiqSBCVRNNJf3UHrRWJBlKMAdQZGnXzhyeo4Ywhjh2cIGxccV0N6s2Yt4XCeFJ9E2KZlHpmQwYOo7k7GHUxWQy8NfOirwF587gtSXON9kFd03gv2v289zCrX4roHvXZewTG8qDH60lM8E32JuWm8SHN41m7o97OC+vF9V1Dp/cIHesYDEZPZWPpw5IpMZqZ87iHXjHEmZj48Bi8X2nUt6gG5OU2NBG8825PJ9NB8p9HpTOdRoazW9QkJkQ7rMvfXuEs+juiTi0M7C6foIzcLxjSrZnnguH9eaefzqDVYNB+bTgdLt7arbn4fOd62EcEex7yzP7eYAbXQeib49wT8vskRlxrCksI6dnJPHhFi4entJouYbcR9ChYeqARF67fBijM+P4aU8ZI9JjPfN5v1w11M8rh9gdbGx58kyMhsa/z5i+8aTFhbmCOf8spvr9/d2Fg7niLz/6nc/drYt7P9+7fhSXzFnKS5edQlx4EKMy4hotE2zyvx+9s/MYufhlAG45tS/3TOsHBSWYjQbOe+k7ZgzuySs/Oas0xFrLiVEV9Fd7iFXlBGF11chzFvNZsHLRKT2JiowiokcqRPWCoHAwWdhVaeB6nAGuuwX/N/edSs+o+tzgq8amkxgZzE1/W9nkMfLHO9AzeF0oMwb15I8XD/HbDU7DRmdGr+WCzb7Hyt1CdOHdE4kONfu0vgUwGQyNSlj8pa3hC0Gf2FB6RFi4bnwGA3pGsqO4ki9crVq95wkJMjJjcE+g/kXeH4OfF47k6GA/c/rXKyaEL++Z5Gk85ubO+duue7kn8K1jkOf7NY9OxRxkBKMBtAbt4EB5HWOeXkgPSjlElGcdl+al8NsLBnPn775kd0mVp9Xx5Iwe3DQ+k3+uX0JfUzjE922UvtGZcYRbTD6NmJwUB4jzpA2cwfwhHeOZ4+KIPnQmCeZOAE/N28i/Vu7lu/tP5bP1RaTEhPDlpoOeYsnFWw6R/fD/uO20vtw1tZ9PoOZP7qOf8fCMnGbn8SclJsQnmIukkkpCcWAANE9Miua9r9dQixmA0YYNjDas54iO4NT9RfSyrIP9UGkMpgxnJW4bRqqHXMlDy6Cv2kc0laigEDbXxlFGGOnqAH3jgvh3eTYVtQ5mTx7E898dZGdVMIeJ4n+3j2emV127/kkRbDpQ351AKDWulmtWUtUBgpSNUGoIwsYtE1KYs3gH+3Q8N18+k6FpPbjg5e8pKK4gWRWTEG5hR4URM3YiQoI4VK1dS5ooeNwZBBiAKV7HyNhE/0TJ0SHcMSWLX5zal49WOVsqnj4g0XPj9W6UcumIFCb2S/Dbjcqw1FiGpToDhagQ53Ge3L8HeSnRDOhZv467Ts/mD59vpm+P+iDK+0ZtMjQOcuLDLcSHN11p+S9X5WMyGIgKNTPS62HvXq3ZaPB5oLm+9buujITGuZINTR2QyOcbijAoGJkZ7/NdkMngE6i5+5wKs/je8mptja8FP7vOvdP6ccXo1Gb3v9F6vKo5KKU8LbjHZcU3t5gPfw3Ugkytq1vmfYSDzfXLDkmJbjyzS8N+tEZlxPHTY1OJDDY3uYzBT4A5M9836HX//MNdXUUUPD2DT37ax6euYK6ESEp0JNt1L+LDg3h4xgBSYkOZs3g7n60v4uEZOfQen+F3+6k9ILXBNH8vGwN7RTW5D03xDp699zIpKrhRYBbIOhq+RLx+ZT6FJdUkR4f47Z/RaFD0jmm8LwAmP8fdbfF9p/p87hMXyoKNvsGc9zkBvueXv9ziQLhfFhtqLq1u0aFmMuLDfHIE3fcyV6JAGYkJDQKUp16dm/u6066oy/tlyR34+kvG2semEhFsZlRGnE8wd9fp2X67EWqosxuTSjDXzf1u/iZPFxMHy2ub7N8I4PlF2ygsrSYjPqzF9frrcLEpCgdxVDDd9hOTTUtJUQfprQ7Rx3CICh3CPh1HvCojbmkFVzR4FhbpaIKwERXeF0Y9DlmnM/jZHa4A0KngvBnsPriEf7j69OoRYuGgvb7OzfQeSSypOMwRh5Ub+ozkg8nxnje/9Ab72vB6c9fhqgSKdZTnrSs1LpTcM07lP1851xMaHEJ0aBB2rXFgoFD3wKoslOFMh02ZqaTljiebupcFGQ0opQgyKU+7Tn+5Y+C8wTbVH563abmJ3DY5i2vHpfveDIHZEzK4bny6z83ae2umJrbdnNP6+y9qca8pyGholKMUwL29RQalPH08ufvkarhai+uBldUj3KcIq3d0KOKnRJUAACAASURBVHtKqvndhYNJiwvj4teW+Ak4nQ/fph6mTRmTGceLX25jRFpsyzO7/O/28ew6fJQb33XmHLX348HilXtm8Xlo+14bvzyjf6NlmwvkmvLMhYN5yuvB6O/n9pfbA84Axt3vZK/o1h375vjLjW15Ga/rpInzoyXewa77OnBPSY4KYYzrhcRfca/JoLh5UiZVdXbOzUvmTK8X1LZcq253nZ7daH8CCbiaEhsWRMnROoLNBr/BnL8c5YbumJzFUwF0jdNUjrZ7G+7zOdQ7mPMc9/p0uIudI5o4v88ekhxgMNfiLB3qhAnmlFJnAM8BRuANrfXTnZykDvfDjsO8/NV2z+eH/72uxWX+tXIv5/vpmLehKteFGEINOWo3DgyEKGeF335qj6elZZCyka0KiVRVsBtKjOHs0Mls1Km8bz2VfoY9RFPJOpVNTPop/GtzLZGqikodzBadwiadAigKbqzveNdBQaP0xDWTI2L0ykppmKvS1hvTBUN9+5Ayux58Nq8Wq94Xb6AXcsMb57TcRD5bX9TEDfzYWveZjAbuOj27ye8bpsV7F9rywGuJ2aQa5d545wy2lVLO/txWPDyFwiPVng6WvY3rG88Llw5lWm4Sr3/j7Cy44OkZ3PSus45hZLCJtDhnwOAvh6ktxvSNZ93j0wi3BH6bzekZ6RM4+OuRv9W8fmeLVy5MUDO/cSAdjR9rWjyTmpj1WM//pjT1ktTQpH71xaTeAYL3LqgG3zfH+yXB3GDfzC3kthoNCpPRwP1nNg6yj+U4NcyVayktLe3l+9eP4sOVexncO4qb/RRlBxJ4mowGQoOMbR65x5Mz57p0QrxyTt0/gfdv+PoV+c2OghPiJ+fVXV/QW1PF4MfLCRHMKaWMwEvA6UAhsEwp9bHWekPzS3ZNpVV1RAabfR4qa/aUktkjnPJqK/9ds4/rxmc0aknaVPcbDX3UoI6cWzQVjDWsp79hNyO2GRll3soow0ZPU21vRTqa/TqWWoL4r300BTqRQePO5ravHfhc8q6Xs6gQM5cl9uGTjdsbrau1Gl4y3gFbw7f8hjdZjW5UAdifhkVZ7puvd/GTdzramsX+wqWneIbCauhY3riP1bG8nTe9zvpi1hFpsbx5VX6Tb8Ot4f7N48ItfjscBWfgevaQxl0H1N+AledcCeTBHKjWBHJu3oF0e48a5J0zZ2hQdOje1LTc1gVyGfFh7Cg+GtC8/o5sUyV5PkWbrj/b43np78WpoYaj2Hj/Jt7JbU0ppPf+uK9t9wuVuYVzrrnr8Viu1WGpMY2mNQw0AXpGBbO/rIbw4ObP56zECE/A+eqsYdzoelly85fr3ZDJoAg1GykNoKTDn4bXr7sYXFN//ng/J4LNxmaLyv1Va3hoxgD2ldb4dF/V2SN8nRDBHDAC2Ka13gGglHoPOBfodsHc/rJqRv92EQ9O7++p+F1dZ+fcl77zGQqmosbWqhuJP1FUMtSwjaGGbUww/MQQtR2D0ti14mhxKAcN0bxjn8JSxwBsGDwdga5xZDbqYgLgmfiBgP++rYwGdUzj2TW3q0Y/xRee5RrmPml459qRvPLVdr9D9rg1CuZMzvXYm3iatHXPgkyGJuthNVX81FG8t9ZSTkGr1utuXae8ck4VxxzIuY+593FqbSCWlxLDZ+uL6B0T4rmhn9a/xzGl61h5n3vt/bZvaeJ3VV7lrBec0rqRDebdPt5nCDe3c/wEz/5Paf+/malBsAn19aCORXM5kk2pP690m++7PnVSG7yotRRgNndet/Wl74Ez+3vq13rzl3P58IwBWO0ORqY3nr81p2ggdfBMRoNPcPW7nw0OfAM0Plbe++NOa2t+w4bXzKNnD3CtxHe+9jg3j8WJEsz1Arw74CkERnZSWo7JtoPOznsXbTrI9RMyWbixyNNJ7WKvnLcXv9zGhOzmh+jyZsROqipiiNpOX8NeBqoCxhjWY1Z27FqxRmfyvP18FtsHs1ZnYMXkM65jc168bCg9IoJJjGy6KNSglE8RZXPGZ8WzZPthbE0Efw2vQ9+cuYA20eIg8I2COWPjnDlvnV1for35eztvDx0RoHr/5u5nYqA/xw0TMpiS04Ms15jE39x3KomRgbfO6wjexWY/O6W3T1cubeF9xJsK5sC3wU1r+MvZeHXWKZwxsKeftPgpZj3OOXOBFrP6LOP1mzQVkAxLjWH6oMb77ObTAKLB9dUwIHtoeg7ZSRFc6Wpt3HCbKbEhhAU5H99trRIR2kKdM595LUZO7df0vvnT3KXub7xZN5NB+Zy0Q/s03VDHH/c9psZVVSjUK3fcfV9vzTXesJj16rHpQOPnkL9z+3g6UYI5f0ex0WWvlLoeuB6gT5/ObUbsrcZqp87uIMho4I73VgOwdEcJj3283jPupT+LWyxW1Yw2bOBy4xeMN6wlQjk7qLVpA7t0Im/ap/OlPY8NOpX/3H0mhp/2s9Kromdz9dS8jc6I88z74U2jufsfayho0HeXv6IAf/UOAN66egQOrcny6sCzuTc6o6G+j66WggX3SeGvK4Hm0uu3mNWnztyJEc1lxIezuaiiXYsaI12NLwIpYmkL73OjtcGiwaA8gRz4b/14vLkPfUyouV0G73Y/wO72U9ndPe6pon0CpfFZ8QzqFeU3kAP/D/imfjHv39L98A30ntQc97ntr75Yk8sYverMeU33Pp4f3jSmhe3W/+0u4o4JM1NZa2t0bcye4L/Frts3953mtd72va78NYryd10lucbwbk0Lb/cp9s61I9lzpIqpzy5uNI/JqNpc1GFQcOEwZ51wd527JK/ArV9SBM/8bJDP2OAtrrOJ4+tucf/CpUP5YedhLjil5broHelECeYKAe828L2BfQ1n0lrPAeaAc2zW45O0lnn3eO+tuUDO26e3jWPG8/X9ni28pg/FP35A8JaPGWLYQbGO5GP7GFbpviRkjeS1TUGkxIXz9b2ncmTeRm7NSiAjIZzbJmdx/YQMHv/vBub+uJsEr4s0OzHcM+TXBUN78dHqvZ6bv3cu1rDUWD66eSxDf+07huq5ecncPKkvf/muPnh7aMYAXv9mJ1kNKsEbDQpjK95yTM0Us4LzDbSqQetca4MctoKnZ/j0fdRwLZEhzkvFt0K6//pzx8rddcXPR/bhgxVtH1C9Ld69biRr9pS2uvuL5rw2axgfr9lLalwoB8rbfxQPb+39YOsM7npJLT3QA3X12DQOV9Zx7fj0Rt/9fGRqo1EFjuUIvnNt8wUi+X7qaKU10breO364emw6PSKDOauZnK9AKaX49XkDGZ3RuMjwjilZrPbTQbrZJ5ewbUfIX0D09+tG8dXmg426zGmNjqjf2pC/TVw1Jo2kyGCmDwo8MHILCTI22SLfZFBtql5w+oBEXr+iftjSMX3j+Gx9ET0inM8x9wv3zOGtz8hxj8Ty6qxhnmm3ndaXYakxTMxO8Fsf93g7UYK5ZUCWUiod2AtcAlzWuUmCd5fuQmvN5aPTmPbsYjYXVXDHlCyGpcaQnRhBydE6Xv9mR8srakFuchQRVDHT+CXTjMvI/PtWMtEcjcvBOvKPHEk5l4decGbXXxbVBwe7PS1aH5ju259csNnoyX7vFVN/sb119QhPs/Pk6BD+ePEQbn9vNR+v2edTqRogJiyIgqdncPmbP3gaGtw1NRuLycgNEzJ8Bq7/5NZxAeWGNF9nztDsG+TojDgWbnKOM+i+oKcP7MkrXzXdGKNhMOPulqHWK0fvxomZniKw0wck8p/Vjd4f2iQ5OsRTATurRzhntsMDLFAJEZZ2b8mYFBXsqf/ZnvydE8e7nmFHsJiMPhXwrxqT5rdOmj/+dj80yMQj7no+DRzP4PfmSZmM6du4j73sxAi+v/80xjy9yGd6H6/7gtGg/NbBa6vLRzXskc7Ju2Nqb0114dMa/s7NlNhQLh+d1sY1utLTxnO+NeGSv7QbDcrT0bA//lL1gFdr3DCLib9clc81b/mOdW00GHzSFhtA7vTX905qVHT67Mw81uwpo6rO/8hCrZEUGUxplZWU2PpnosloYGIrqjp1tBMimNNa25RStwCf4eya5C9a6/Wdmabthyo9XYUkRgazucjZUe2fF7TPgMHgHOZqqGEbLFrNYsvLxKhK1jnSYNIDkHcpYdHON5Asr2XCXIFacy8+I9NjefPbnUzql8DzC53pTXa9Rbn/V0rxx4uH8NCMnCZzcZqrI+LWUgeeTa37v7eM4+wXnbmRJqPyWxnezXuSe75BvaMa5ca53TM1m9MbBDTurHZ31v2SB06jZ1SIJ5j73YWD+eUZ/Rs9kI7VF3dNbNf1nejcv1P3D+nqPXZObsDztjZDw9PvloL279XOV2RI0w1ekv3k0vzh4iEdmZxW8emOxPvvVqzDfT8MJDhpixF+Gif4E0jdrvT4MM4ZkuwZOaW9XpIajhzi3TflGblJzF9/AEV9w58PbxodUNF6alzj3N3QIBOjM+NYuLH19UAbcgfMXbk2zQkRzAForecBTY0qfNwVHqkfQP36d1Y0M6evc4Ykc+fp2azZU8od769udt5f5tm4ZtOTsBjWkscztRdx5c/OZWB+00MNhXg1027K6QMSPf1j/eWqfJY0Mb6q2WhotiLpmMw4TwetDesdjPJTxNHQorsneh4ADQOyrMT6otkVXhVp2yOn4ZbT6sPfe6Zm+x1f1p1T9+vzBmJUCovJ6PeBJHx5boYddFPsqHp5Jyp38V5ucv1LVVtzetpbWzop7ijul8p+SRFtrujeXv0X+vPd/acRG9p+QeKX90xyrtcTzLXbqhu5blw6GrhmXDoWs4EhKdHEhVnYU1LtqZfXlHPzkgMuEWnLLednrpbdXeOKaN4JE8x1NeP8FCcE4rrx6Y1GLQCYMbinZ8gbcI7l2TfWAjsiofdwBulwXrfamwwoLjilF/9auZeJ/RJ4ftE2JjQzpJBSytM/1mn9E5vs2b8ls8dn8NQ8/91/TOrXcvcPzQ3p5B209YiwUODq58o9+dy8ZE8QNiYzngUbncWsrb2ibzktyye4c3MXRTcsrokKMTfbAWVX1F0bb/hLdSCNcAf2imTd3raNIdzVRYe2LgBKjg7hqfMHkZscyQuL2q/U4EQTGWxm7uxR5PaKZPfhpscubU5bXjQigk1UNBr0vbFARoRpi/pOdo89nJk7e5Tf6Q+fVV8F4LlLnEPVz7l8GJ9vKGpxv/50cR7PtNB1iXvYumvHNa4z2pI/dqHc4ZZIMNdBjAbFmkenMuTxzwNeJi8l2jM4ecML/88z85i/7oCnNWVGfLgzcsmeBkCM619Tfn/hEJ46fxDBZiM7fzv9uLx9+9tGW8MG7zWNTI/1qfT750vyGPNbZ/Gm++3XfVMAZwXw0ZlxnPncN5zVTB2PQFw1Jo23vi9o8vgtfWByk33RCa8c1nY8/bx/ikAemB/dPPaY+jzsyn55Rn9SYkL9DBTetMtGOqtjyGnbvNGZziJCn1OsFedxW3r7+fKeSRw5Wtf6BZsxPD2Wd5buIsdrrOaO0PAe2ZpHTo/IYGY1Ua/Rm9GgMBqaHxs3PtzSqBPo1uoimdXNkmCuA0WFmLGYDNQGOCzJ3VPrK9+mxIZw/5n9iQ+3sP1QJWajgS/vnsSPBSVcOKx1nXqC70nfmcUo7lygtqbg5kmZ3DY5C6UU3/7yVOLDLVhMRs/F5r/OnCKnZyRrH5vq6ZuprR47J7fZOkxNjRcojo9AitnNRgMBjo3e7YRZTMyekNGqYM6tj2s4s5hW5u6dbLyLWVtT5NqWemfx4ZZWdf0RiHOGJDMqI5YeEYH3tdaW3Ht3K1K3nlGd239je+jKLzwSzHWwIGNgwdyWJ8/0qeyvlOLGib4tAPvEhXpuuN1da+9rZw7qyb9X7+OCU3p5Oij1N/B5czkzLY048OjZA9r9Ldifd64d0aXqA51IOrJe0onu/jP7M65vPPlpgVWk7ygv//wUopppLNHZ2vou3JXOzNYEcm01JCWaf9wwmryUaA5W1Pi9X3cXl4zow6/+vc6nh4euRoK5DlZR23J9h+NV7NkZrp+Q0S59pU3LTWLHU9NbfFgfy8AF7p69O9r4rK7TnP14y3Z10Hv1mLQOWb80gGg7i8nI5Jz27ZamLdLjwzq8CPBYNNWy9UTW1ueTu4Vtdw7kwFk3uqnubLoKCeY6wfRBScxbe8Dz+UQN5AAenJ7Dgw36smurQHJdToR+xk5ksa4+CDuK/P7dX1f/Db2LVi8bEXgHtO77fFvGhu1s3bWR1MlEgrnjJMRsxGRQVNTaeOzsXLYWVbLVNQ7ryaSj7wmt7Zrk3WtHdvoAyV1FF3+GNuLvXOpu+yAa6+ol5fUtPFs3/FtMqJnbJ2dxTl7njxYQqM4eb1QErvu9InRT1Va7p8+0yBAz824f38kp6hxjXV2iDPMzrM+xaVtnseOy4k/qYk9v3fXl2/s3t5gMjMmM45VZp3RaesSx6er1Ht2pa+31opTiztOzyWymyyUh2kpy5o6jv103km+3FXsq8J+MTu3Xgw1PTCP0GFuVNqlrPwdEB1NK8fcm+rMS3UOXL2bt4unrCN30Pa/N3r5mBNUNxvPu6iSY62Deg7ynxYc1Oaj0yaTDAjk4+e467egkfEaJLqirN2Lp4skT7aArjbkaKClm7WAf3zK2s5NwUpAb7LHrrsWs3TTZooE8V0/9FnPXfix5+r0+ie45J9GudluSM9fBmitS/eHBydRaA+tQWDQvLyWaLzYUYTGdvEXYQnRnr8w6hfV7y5sd77krkGJW0RVJMNfBQpoJ5rr6Tas7ee6SPLYWVRIlvde32Un4jDphvXTZKYQHd6/be8+oEHq2MLB6V3BSXSYn1c52b93rau+G3Dlz7T0ki/AVGmTyDKgsTi7yvGlsxjGOQSyadjK99AxMjuLHnSXEhQV1dlJECySY62BhFhMPz8hhShfoWV2IE8n0QUks2FjkGVVCiOPB3dq2u9YxbY0HpvfnnLxksuQa6/IkmDsOrhuf0dlJEOKEc8EpvTlrcLLPmMZCiPZjNho8DVNE1yZ3QSFEtyWBnDjeTqZiVtF9yJ1QCCGECNDJ2JpVdH0SzAkhhBABklBOdEUSzAlxkjsZKnIL0V4kY050RRLMCSGEEAFSkjcnuiAJ5oQQQogAGSSWE12QBHNCCCFEoCSYE12QBHNCCCFEgKSYVXRFEswJIYQQAZIGEKIrkmBOCAFIjoMQgZCrRHRFEswJIQDQSB8lQrREOg0WXZEEc0IIIUSApDWr6IokmBNCAFLMKkQg5DoRXZEEc0IIQIpZhQiIxHKiC5JgTgghhAiQVJkTXZEEc0IIQIqPhAiEXCWiK5JgTgghhAiQtGYVXZEEc0Kc5HpGhwCQEGHp5JQI0fVJKCe6ok4J5pRSFyml1iulHEqp/AbfPaCU2qaU2qyUmuY1fZhSaq3ru+eV6/VIKWVRSr3vmv6DUirt+O6NEN3bVWPSeO3yYZybl9zZSRGiyzNIzpzogjorZ24dcAGw2HuiUmoAcAmQC5wBvKyUMrq+fgW4Hshy/TvDNf1a4IjWui/wLPBMh6deiBOI0aCYlpskxUdCBEAuE9EVdUowp7XeqLXe7Oerc4H3tNa1WuudwDZghFKqJxCptV6itdbAX4HzvJZ52/X3B8BkJU8lIYQQQpwkAg7mlFIhSql+HZkYoBewx+tzoWtaL9ffDaf7LKO1tgFlQJy/lSulrldKLVdKLT906FA7J10IIcSJTrIKRFcUUDCnlDobWA3Md33OU0p93MIyC5RS6/z8O7e5xfxM081Mb26ZxhO1nqO1ztda5yckJDSXfCGEEKIR6cJHdEWmAOd7DBgBfAWgtV7dUkMDrfWUNqSnEEjx+twb2Oea3tvPdO9lCpVSJiAKKGnDtoUQQohmSc6c6IoCLWa1aa3LOjQlTh8Dl7haqKbjbOjwo9Z6P1ChlBrlqg93BfAfr2WudP19IbDIVa9OCCFEF3GixEDSmlV0RYHmzK1TSl0GGJVSWcBtwPdt3ahS6nzgBSAB+FQptVprPU1rvV4p9Q9gA2ADfqG1trsWuwl4CwgB/uf6B/Am8I5SahvOHLlL2pouIYQQojkSyomuKNBg7lbgIaAW+DvwGfBkWzeqtf4I+KiJ734D/MbP9OXAQD/Ta4CL2poWIYQQIlCSMSe6ohaDOVc/bx+76sA91PFJEkIIIbomd89XYzL9dpogRKdoMZjTWtuVUlVKqajjVG9OCCGE6LIW3j2RnlHBnZ0MITwCLWatAdYqpb4Ajronaq1v65BUCSGEEF1UZkJ4ZydBCB+BBnOfuv4JIYQQQoguJKBgTmv9tlIqCMh2TdqstbZ2XLKEEEIIIUQgAgrmlFKTcI5/WoCzZXaKUupKrfXijkuaEEIIIYRoSaDFrH8EpmqtNwMopbKBucCwjkqYEEIIIYRoWaAjQJjdgRyA1noLYO6YJAkhhBBCiEAFmjO3XCn1JvCO6/PPgRUdkyQhhBBCCBGoQIO5m4Bf4BzGSwGLgZc7KlFCCCGEECIwgQZzJuA5rfWfwDMqhKXDUiWEEEIIIQISaJ25hTgHuHcLARa0f3KEEEIIIURrBBrMBWutK90fXH+HdkyShBBCCCFEoAIN5o4qpU5xf1BK5QPVHZMkIYQQQggRqEDrzN0B/FMptQ/QQDIws8NSJYQQQgghAtJszpxSarhSKklrvQzoD7wP2ID5wM7jkD4hhBBCCNGMlopZXwPqXH+PBh4EXgKOAHM6MF1CCCGEECIALRWzGrXWJa6/ZwJztNYfAh8qpVZ3bNKEEEIIIURLWsqZMyql3AHfZGCR13eB1rcTQgghhBAdpKWAbC7wtVKqGGfr1W8AlFJ9gbIOTpsQQgghhGhBs8Gc1vo3SqmFQE/gc621dn1lAG7t6MQJIYQQQojmtVhUqrVe6mfalo5JjhBCCCGEaI1AOw0WQgghhBBdkARzQgghhBDdmARzQgghhBDdmARzQgghhBDdmARzQgghhBDdmARzQgghhBDdmARzQgghhBDdmARzQgghhBDdmARzQgghhBDdmARzQgghhBDdmARzQgghhBDdWKcEc0qp3yulNimlflJKfaSUivb67gGl1Dal1Gal1DSv6cOUUmtd3z2vlFKu6Ral1Puu6T8opdKO/x4JIYQQQnSOzsqZ+wIYqLUeDGwBHgBQSg0ALgFygTOAl5VSRtcyrwDXA1muf2e4pl8LHNFa9wWeBZ45XjshhBBCCNHZOiWY01p/rrW2uT4uBXq7/j4XeE9rXau13glsA0YopXoCkVrrJVprDfwVOM9rmbddf38ATHbn2gkhhBBCnOi6Qp25a4D/uf7uBezx+q7QNa2X6++G032WcQWIZUCcvw0ppa5XSi1XSi0/dOhQu+2AEEIIIURnMXXUipVSC4AkP189pLX+j2uehwAb8Df3Yn7m181Mb26ZxhO1ngPMAcjPz/c7jxBCCCFEd9JhwZzWekpz3yulrgTOAia7ik7BmeOW4jVbb2Cfa3pvP9O9lylUSpmAKKDkmHdACCGEEKIb6KzWrGcAvwTO0VpXeX31MXCJq4VqOs6GDj9qrfcDFUqpUa76cFcA//Fa5krX3xcCi7yCQyGEEEKIE1qH5cy14EXAAnzhaquwVGt9o9Z6vVLqH8AGnMWvv9Ba213L3AS8BYTgrGPnrmf3JvCOUmobzhy5S47bXgghhBBCdLJOCeZc3Yg09d1vgN/4mb4cGOhneg1wUbsmUAghhBCim+isnLkuyWq1UlhYSE1NTWcnRRwnwcHB9O7dG7PZ3NlJEUIIIdpEgjkvhYWFREREkJaWhnRVd+LTWnP48GEKCwtJT0/v7OQIIYQQbdIV+pnrMmpqaoiLi5NA7iShlCIuLk5yYoUQQnRrEsw1IIHcyUV+byGEEN2dBHNdSEFBAQMH+rbxeOyxx/jDH/7Q7HLLly/ntttuA6C2tpYpU6aQl5fH+++/32FpBfjNb35Dbm4ugwcPJi8vjx9++KFDtzdp0iSWL18OwPTp0yktLe3Q7QkhhBDdgdSZOwHk5+eTn58PwKpVq7BaraxevTrg5e12O0ajsVXbXLJkCZ988gkrV67EYrFQXFxMXV1dq9ZxLObNm3fctiWEEEJ0ZZIz141MmjSJX/7yl4wYMYLs7Gy++eYbAL766ivOOussDh48yKxZs1i9ejV5eXls376dhQsXMnToUAYNGsQ111xDbW0tAGlpaTzxxBOMGzeOf/7zn6SlpfHggw8yevRo8vPzWblyJdOmTSMzM5NXX321UVr2799PfHw8FosFgPj4eJKTkwF44oknGD58OAMHDuT666/H3YfzpEmTuPPOO5kwYQI5OTksW7aMCy64gKysLB5++GHAmTvZv39/rrzySgYPHsyFF15IVVVVo+2npaVRXFxMQUEBOTk5zJ49m9zcXKZOnUp1dTUAy5YtY/DgwYwePZp77723Ua6nEEIIcSKQnLkmPP7f9WzYV96u6xyQHMmjZ+ce0zpsNhs//vgj8+bN4/HHH2fBggWe73r06MEbb7zBH/7wBz755BNqamqYNGkSCxcuJDs7myuuuIJXXnmFO+64A3B2y/Htt98CcP/995OSksKSJUu48847ueqqq/juu++oqakhNzeXG2+80ScdU6dO5YknniA7O5spU6Ywc+ZMJk6cCMAtt9zCI488AsDll1/OJ598wtlnnw1AUFAQixcv5rnnnuPcc89lxYoVxMbGkpmZyZ133gnA5s2befPNNxk7dizXXHMNL7/8Mvfcc0+Tx2Tr1q3MnTuX119/nYsvvpgPP/yQWbNmcfXVVzNnzhzGjBnD/ffff0zHXQghhOiqJGeuC2mqMr739AsuuACAYcOGUVBQ0Oz6Nm/eTHp6OtnZ2QBceeWVLF682PP9zJkzfeY/55xzABg0aBAjR44kIiKChIQEgoODG9VPCw8PZ8WKFcyZM4eEhARmzpzJW2+9BcCX9i+GcwAAIABJREFUX37JyJEjGTRoEIsWLWL9+vV+t5Gbm0vPnj2xWCxkZGSwZ88eAFJSUhg7diwAs2bN8gScTUlPTycvL8/nuJSWllJRUcGYMWMAuOyyy5pdhxBCCNFdSc5cE441B60t4uLiOHLkiM+0kpISnz7Q3MWaRqMRm83W7PpaGqI2LCzM57N73QaDwfO3+7O/bRmNRiZNmsSkSZMYNGgQb7/9Npdccgk333wzy5cvJyUlhccee8yn649AttEwqG2pxan3eoxGI9XV1S3uuxBCCHGikJy5LiQ8PJyePXuycOFCwBnIzZ8/n3HjxrVpff3796egoIBt27YB8M4773iKQo/V5s2b2bp1q+fz6tWrSU1N9QRu8fHxVFZW8sEHH7R63bt372bJkiUAzJ07t037HxMTQ0REBEuXLgXgvffea/U6hBBCiO5Acua6mL/+9a/84he/4O677wbg0UcfJTMzs03rCg4O5v/+7/+46KKLsNlsDB8+vFHdt7aqrKzk1ltvpbS0FJPJRN++fZkzZw7R0dHMnj2bQYMGkZaWxvDhw1u97pycHN5++21uuOEGsrKyuOmmm9qUxjfffJPZs2cTFhbGpEmTiIqKatN6hBBCiK5MnazFUfn5+drdZ5nbxo0bycnJ6aQUCXC2Zj3rrLNYt27dMa+rsrKS8PBwAJ5++mn279/Pc88912g++d2F6DhPzdvInMU7eODM/twwsW0vpkIIUEqt0Frn+/tOcubECevTTz/lt7/9LTabjdTUVE8DDSGEEOJEIsGc6FLS0tLaJVcOnK11G7bYFUIIIU400gBCCCGEEKIbk2BOCCGEEKIbk2BOCCGEEKIbk2BOCCGEEKIbk2CuizEajeTl5TFw4EDOPvvsRsNoNXTVVVd5OuadNGkS7u5Wpk+f3uKyrfHss88SHBxMWVlZu61TCCGEEMdOgrkuJiQkhNWrV7Nu3TpiY2N56aWX2rSeefPmER0d3W7pmjt3LsOHD+ejjz5ql/XZ7fZ2WY8QQghxspNgrgsbPXo0e/fuBZzDZY0aNYrBgwdz/vnnNxrDtaG0tDSKi4spKCggJyeH2bNnk5uby9SpU6murgZg2bJlDB48mNGjR3PvvfcycOBAv+vavn07lZWVPPnkk8ydOxeAV155hfvuu88zz1tvvcWtt94KwLvvvsuIESPIy8vjhhtu8ARu4eHhPPLII4wcOZIlS5bwxBNPMHz4cAYOHMj111/vGU+1qXTZ7Xbuvfdehg8fzuDBg3nttdfaemiFEEKIE4b0M9eU/90PB9a27zqTBsGZTwc0q91uZ+HChVx77bUAXHHFFbzwwgtMnDiRRx55hMcff5w///nPAa1r69atzJ07l9dff52LL76YDz/8kFmzZnH11VczZ84cxowZw/3339/k8nPnzuXSSy/9f/buO06q6m78+Ofc6ds7yxZ26WVZem8WFAiiKBaIJqKxl5gYnyS/aBLBaDTGGKNRn/jE2BVb7L1jAwUFpYOwwC5tC2wvU87vjynMzs7sDrDLsuz3/XrxYva2OXfmzrnfeypTp05l48aN7Nu3j3POOYeJEydy5513AvDss89y0003sX79ep599lk+//xzLBYLV199NU899RQXXnghtbW1DB06lFtuuQWAIUOG8Mc//hGAn/70p7z++uucfvrpEdP18MMPk5iYyNdff01jYyOTJ09mxowZ9O7dO6rPQQghhDgeScncMaa+vp4RI0aQmppKRUUFp556KpWVlRw4cIATTjgBgIULF7J06dKoj9m7d29GjBgBwOjRoykqKuLAgQNUV1czadIkAM4///yI+y9ZsoQFCxZgGAbz5s3j+eefJz09nT59+rBs2TLKy8vZuHEjkydP5oMPPmDlypWMHTuWESNG8MEHH7B161bA2x7w7LPPDhz3o48+Yvz48RQWFvLhhx+ydu3aVtP17rvv8vjjjzNixAjGjx9PeXk5mzdvjvpzEEIIIY5HUjIXSZQlaO3N32ausrKSOXPmcP/997Nw4cIjOqbNZgu8NplM1NfXE+2cvN999x2bN2/m1FNPBaCpqYk+ffpwzTXXMH/+fJ577jkGDRrEWWedhVIKrTULFy7k9ttvb3Esu92OyWQCoKGhgauvvpoVK1aQm5vLokWLaGhoaDVdWmvuu+8+Zs6ceSinL4QQQhzXpGTuGJWYmMi9997LXXfdRUxMDMnJyXz66acAPPHEE4FSusOVnJxMfHw8y5YtA7ylb+E888wzLFq0iKKiIoqKiti1axclJSVs376defPm8fLLL/PMM88Eps2aPn06L7zwAvv27QOgoqKC7du3tzhuQ0MDAGlpadTU1AR65LaWrpkzZ/Lggw/idDoB2LRpE7W1tUf0OQghhBBdnZTMHcNGjhzJ8OHDWbJkCY899hhXXnkldXV19OnTh0ceeeSIj//www9z2WWXERsby4knnkhiYmKLbZYsWcJbb73VbNlZZ53FkiVL+O1vf8uQIUNYt24d48aNA7zt4G699VZmzJiBx+PBYrFw//33k5eX1+wYSUlJXHbZZRQWFpKfn8/YsWPbTNell15KUVERo0aNQmtNeno6L7/88hF/DkIIIURXpqKtbjvejBkzRvvHZPNbv349gwcP7qQUHX01NTXExcUBcMcdd7B7927+8Y9/dHKqjn66utv3LsTR9Oc31/PQ0q387keDuOKEvp2dHCG6LKXUSq31mHDrpGSuG3vjjTe4/fbbcblc5OXl8eijj3Z2koBjN11CCCHEsUiCuW5s/vz5gbZux5JjNV1CCCHEsUg6QAghhBBCdGESzIXorm0Iuyv5voXoWAvG5pLosDBneFZnJ0WI45YEc0Hsdjvl5eVyg+8mtNaUl5djt9s7OylCHLf6pMex+uYZZCc5OjspQhy3OqXNnFLqT8BcwAPsAy7SWu/yrfsdcAngBq7TWr/jWz4aeBRwAG8Cv9Baa6WUDXgcGA2UA/O11kWHk66cnByKi4spLS09grMTXYndbicnJ6ezkyGEEEIcts7qAPFXrfUfAJRS1wF/BK5USg0BFgAFQBbwvlJqgNbaDTwIXA4swxvMzQLewhv47dda91NKLQD+AhxW63mLxSLzfAohhBCiS+mUalatdVXQn7GAv15zLrBEa92otd4GbAHGKaV6Agla6y+1tw70ceDMoH0e871+AZiulFIdfhJCCCGEEMeAThuaRCl1G3AhUAmc5Fucjbfkza/Yt8zpex263L/PTgCttUspVQmkAmVh3vNyvKV79OrVq71ORQghhBCi03RYyZxS6n2l1Jow/+YCaK1v0lrnAk8B1/p3C3Mo3cry1vZpuVDrh7TWY7TWY9LT0w/thIQQQgghjkEdVjKntT4lyk2fBt4AbsZb4pYbtC4H2OVbnhNmOUH7FCulzEAiUNHWm65cubJMKdVyBvj2lUaYEsJupDuff3c+d+je59+dzx269/l353OH7n3+R+Pc8yKt6KzerP211pt9f54BbPC9fhV4Wil1N94OEP2Br7TWbqVUtVJqArAcb/XsfUH7LAS+BM4BPtRRjC2ite7wojml1IpI86h1B935/LvzuUP3Pv/ufO7Qvc+/O587dO/z7+xz76w2c3copQbiHZpkO3AlgNZ6rVLqOWAd4AKu8fVkBbiKg0OTvOX7B/Aw8IRSagveErkFR+skhBBCCCE6W6cEc1rrs1tZdxtwW5jlK4ChYZY3AOe2awKFEEIIIboImQGiYz3U2QnoZN35/LvzuUP3Pv/ufO7Qvc+/O587dO/z79RzVzJ1lRBCCCFE1yUlc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZgEc0IIIYQQXZi5sxPQWdLS0nR+fn5nJ0MIIYQQok0rV64s01qnh1vXbYO5/Px8VqxY0dnJEEIIIYRok1Jqe6R1Us0qhBBCCNGFSTAnhBBCCNGFSTAnhBBCCNGFSTAnjlsriirYUV7X4e+jtW7xd12T65CP4/ZoKuuc7ZWsdlVR29TiPI8Wt0fj9nTOe4eqrHPi8Wi2l9cC3s/lWP3OhJfHo6ltjO73qLVGa02D043L7englEXvSH97dU0uGl3uVrc5UNdEeU3jEb3PsWxfdQMb91QD4HJ7+HRzaZv7fLNjP/uqGwDvZ1hR29ShaTwSpkWLFnV2GjrFQw89tOjyyy/v7GSIDjTpjg955IsiLpvah017q/lsSxkJDgu1jS6cbo1Ha17+toRBPRMwlMLj0Ryoc+KwmnB7NOt2VdHo8pDosADeDPX5lcVoDRkJ9sCyE+/6mL1VjUzpn4bbo3l19S5O/+fnnDQwg417q3G5NSmxVgC+3bGff32ylVibmddX72JMfgp7qxpwWEz84ZU1XPP0NwzpmcCnm0tRSmG3GLy6ahcF2Ylhz/Hhz7bxx1fWUFHbRE5yDPF2S9jttpfX0ujyYDUZmAwFeDMnt0djNnmf6ZpcHkqrG4mzH+wX9fK3Jcz6x6f8a+lW0uKsDM9NanZcp9vDe+v20Dc9DqVUi/ddUVSB2VDE2lr2tXK6PVz79DfkJseQmWjnyidW8tmWMk4Z0iOwTV2Ti1Pu/oQ/vb6ey6f1wRKUVgXsrmzgxpfWcNLAjMC61uworwMFW0tr0Vo3S9e/P93KbW+uZ/7YXGobXfy//37Hxj3V5KbEYCjFl1vLmXnPUl5eVcI9729mav80pv/tE/73kx/45SkDAKhpdHHBv5fx9PIdnDky23ueHg8mpWhwevhscxm5yQ4MQ1FZ5+SVVSUUZCUEPru6Jhf9b3qLOJuZUXnJge/ObjEFvqdInG4P1z3zLXmpMaTEWjEMxc6KOuwWg7LqJg7UN5HgsLC3qoH9tU2U1TSyrawWj9ac9cDnVNY7Gd8nFYCdFXW43B7Ka5pY/OpahmQlBH4HfmU1jTz79U4e/aKIoVmJOCymwLXldHv4+TPfkJXkoGeio0Vai8pqKd5fzz0fbGbxa+uorHcyLj8l8Dm8t24vz329k6n9w3bci+ijjfvomWinorYJt9Z8tGEfv395DTe+tIbJ/dLweHSL8wDYvLearWW1nHTXJ/xQWsPPn/mWez/cQpPLQ2ainXi7OXBu0XC6PWzaW01pdSONLjcOq4mPNuwjPzU27O8kkn1VDRTc/A4vfVvCE8u2E2M1ceWTK8lOctAvI47Keierdx7gq20V9MuIC6SxvslNTYPL+93XNDHilvf4dHMZswoycVhN1DS6+Pv7m0BDerwNi8lg2KJ3eTDoWgZocLrxaHht9S6a3B5eWVXCjoo6Pt9SxrwHv+Diyb2xW0xorVm5fT9Ws8Gdb2/koke+JivR0Szf0lqzv87Jxr3VJMdaMBve6/mTTaU0uTykxtlYU1LJut1VfLxxH6uLK1m+tRyLyWDDniry02IB2HWgntU7K8lMtLN0UynT//Yx28pqyUpykBxjpdHlZuY9S7ntzfVcMa0vt7y2losf/Zr/+3QbTy7bzsWTenPmA5/z70+3kZcaw7ayWk79+1JsZhNj8pJ5/Mvt2MwGZkNx8t8+4fXvdnPp1D6cdNfH/PWdjazbVUWszcRnW8op6JnAltIaahvdJMaEz3vb0+LFi3cvWrTooXDrVGc9bXe2MWPGaOnNevRs3FPN9yWVnDM6p8W6JpcHj9bYLabAsrKaRsbc+j5XTOvDv5Zu5c6zhzFzaCZ/f28TOckOLp3ahw/W7+WSx7zfYV5qDG9eNzVwc25yeRjw+7cOKY2nDO7B++v38twVE/l44z4e+PgHAFJjrVx5Ql+2V9Ty5LIdAFw+rQ+vr95F7/RYPt9SDsCQngms210VON6f5hbwh1fWAnDG8Cym9Evj/o+3sD1MaWFOsoPi/fUR09YjwUZKrI3b5xVSVt1IWryNEblJ5P+/N5ptt+322Xyz4wBf/lDGSYMyeOO73fx65kB6/+5NAAZlxmMzGwzJSuSZr3aQHm/j89+ezP0fbWHzvmre/H4Pb/9yKrWNbkbnJTPtzo/YUeFN76S+qTx4wWhsFgO7xUTx/jqm/OUjAOJsZi6Y0It/fbIVgF4pMTxwwSjm3PcZAFv/PBvDd6NZvrWcz7aU8cH6fazbXYXVZPDO9dM46a6PAfjhz7Px+G4OCx5a1uz87j5vOGU1jfz5zQ0ATB+UwQcb9gGQmWDnvvNHcufbG5gxJJOnlm/nhasmkRZnw+X28K+lW/nrOxtJsJupavCW1Dz2s3H895tihvRM4Pa3NgTeJ3ibSE4d0oP31u0NvC6tbmTVzgMttouxmphd2JMXVhYDMLJXEr1TY1m+rYKSA/W8/6sT+HjjPm59Yz25KQ52Vnivg6I7TuOLLWWc/+/l9M+I45IpvemdFsuQrATi7RZcbg+1jW6Wbi7lqeXbWba1otX0Arx7/TRm/H1pq9v8ff5wrn92dbNl8XYzH95wIq+u3sWfXl/HS1dP4qwHvmi2zaDMeMbkJ/P8imJOHJjOO2v3tjh2fmoMY/NTeHvtHqojfL6zCzN58/s9APx21iD+8vYG7vvxSJJjrEzqmxq4jt76fjef/1BGgt1CnN3MnW9vbHYch8VEvfNgaVR2koOSA/UsPqOAPVUNrCmp5NPNZQzoEcemvTWtfiYAA3rE8dSlE0iPt/HJplKeXr6dm08voKrByUNLt7J8awU3zh5MvN3MH15ZE/idW80GZ43I5tkVOwFYu3gm28pqUQo+2+x9wJw+KIPXvtvNCQPS6ZcRx4cb9vKzR1u/P93345Hc+NL3gc+xX0Ycfzl7GC9/W8JzK3bS6PLQNz2WH0prm+33l7ML+b6kMpCXAWz40ywG/eFtAK45qS9uD/xkQq/A7zuS7CQHt88rxGo2WvxWAV7/+RTm3PcZVpNBU1BJZ35qDDefUcCtr68LpG/1zTMYvvjdiO+V6jB4+arxTLv7c3QrlYpmQ+HyleS3la9G6+cn9+O+D7dEXJ9FGV/cdj6YOjagU0qt1FqPCbtOgrnuqbS6EY/W9PCVMIXavLea3mmxVDe4SHRYAhkoQFWDk6KyWoblJLG/tomaRhe5KTFhj/Pp5lLG5KUw+I/ejKLojtNabHPyXR+zrbyWbbefxiebStlb2UCCw8yVT37TbLux+cl8XbQfgO8WzWDYopY//HG9UzhvTC7/8/zqFusORbzNTHUbVTN2GlFo6rEBLZ+2Q28m4VhxYqcJG96qunR1ADNuzLhJUjXYcGKgMeEmW3mDxs89BTRgJYZG9ukkqomhDhtuTK291SELvqmGKsxO5PuSykM63pxhPdld2cDK7fvb3DbWaqK2qfXP7nh34cQ8Hv8y/EgEl0zpzcOfbTvKKYrMggsDDy5MWHARSwMO1YQVJxqFB4UbA482vP+jsCknOaoMgAZtpYoYdugMPBiYcaPQNGIh+LdlMxsM6pnAzoq6TqnyiuY3faTOGZ0TCPw7SqHaSqqqxIKbLF++4sagEQvrPHls0z2pw46Bh0RqiMFb/apRGEqTp/YQSwMeDGpw0KS9D9He786FXTmx4kRxML6w4yRO1RNPHcmqmlxVSr7agwV34NgWXFhxYlIezLgx4caCGwdN2JQ3j3RrxR5S8GgDk3JjwZtPl+kkinQPKnQC63UvbL58dbXuy16djBUXFt8/w5cuD8p3fRo0+UZqM+HBhpMY1YidRgzfdehB0VNV0EvtI4Vq8o09gXQPVz9QVHA1g89b3KHfmwRzYXT3YM5fohMuuNpeXssJf/2Yc0fn8PzKYq47uR+/mjGQ37ywmudWHMxktv55Nlc8uZL31u3lu0UzqKp38sxXO/ifGQNRSrHo1bU8+kURc4b15PXvdgPeUheTofho4z4q65w8v3JnoGSr6I7TAuka1zuFr7a1XdJw5DRWXMTQQE9VQQ9VQZaqIIYGynUCmWo/6eoAPVQFsTSSpGpIoYo4VU+K8j7NO7WJahxU6xg0CrtqYoOnFwYe3w3OjBMziarGm3FoAzeKVFXFYGNnu5yFU5toxIIJD24MvvH0p1in04CVIt2DIp1JjXZQTQw12kEF8TRibZf3jsSEm95qN73UPkx4iKGBGhzE0kA9NmpwMFDtZJKxlmpiMPmC2CYsNGkzjVhowoITsy8AMHBrgxRVRZqqpBELjdqKGwONwoQHD4omLNTgADSJ1JKqqqjTdjTgxIzJl4XHqXriqCfe97/35qQC7wVQRQwOmmjEQq22U4cdJyb6ql1UEuu9ueHxBdyewGsXBqU6GRcGTVio0PGU6iTcGDgxs1/HYaCxKBdm382sEQuxNGCnKXCTyVLlpKoqqnUM5cSzU2egfO9lwkMqVTgxsVunUqzTUWiSVA0KTZqqooeqYJMnlwasWHDhxqAOO9XaEVgWQyMxqpEE6hhk7CBblVGqE9ml07DhxK4asdNEvtpLb7Ubmy9Ac2HCiRkXBgNUMTZ16O1EQ3m0N1jwK9fxFOt0KnUsP+gsXJjYq5Mp1wm4MVFJLMU6jb06OXAdaFSLIBA0BhobTTRhaeXBR+OgkRgaSVB1uDGo11bKSCSFalJVFY1YcGoztdipJC6q81J46K9KqMNGsc447M8n1CmDM3h//b6w6yy4GGNsZLTaxJDYalR9Of1VCTac5Bpttxer0g7iaGj2fbSnXTqFIk8m1cQEgj4nJhqx4tYGTky4MZGRFMvWAx7qtPeBNUHVkqaqvCGYyUyty4RDNdFL7SWJWjJVBfHqyEvjInFrRRmJ7NYp1Gk7ZuVmj07BNusWZk4e12HvC60Hc9120ODuqsHpZl9V641c91R6G3w+73s6vPfDLdwbpoi5weUOVDMFl5LNHZHNgB7xPPpFEUAgkAOod7pxWExc/MjXLY63+LW1gdftEchtuvVHWM0G+f/vDWw00UPtJ50DDDJ2MkjtYJCxg6GqCIdq/Qm/Vtuox8Z+Hc9uncI2Mqn32NhrZNDohnhVRzz1JKha781PN5Giqr3BBwYW6rHiZL+ORwGG8mDBgyO5J3rw6ajEHH7/+mbMuCnVSQzKTcdktfPulnryeqaybnctbgwqdSx2mhhrbPSFDYoY1UA8dfRQB1BoPCjy1V6GqO0MNnbgoJE41dDinBq1mbU6nx06gxrtoIpYDuhYKomjSZuZZvqOUp3Eak9f0tUBXJh86ffeLhNUHWONDSRSi0M1EUODNxBR/lulmwSjEYtuu/Rkt07BjYENJ5U6FitOrIYLG96newsub/Diu6k0agulJGLgwez7JMxK47DZcDVUY8YdCCxqtY19Ogm74S0BVWicvmyv2hfc9snOZFlxI1XEYPiCJP//8dRRRiIOGnGoJpKpwUYTtdgDgbkLUyCQcPu+cztNZKkyb1pwkmJUk6AOvTOOWytKdBoxRiNJ1GBWHdso343BPp1EGpVY1MESqAZt4QBx1Gkb3+p+AJh9pSZWnKzx9GaH7oEZF068gU4DVs4d35cnl+1gZE48ZwzLRGk3d769HhMeNIqdOh0XJuw0kaYq6at20YSZBm3Fqlzkqz2kU0kvtY+RxhbsNDVLVyQerXzlLwpvUKd9wbN33wM6FicWmnzXtQsTGUYVqaoKm46+E8ABHeu7Og126nR26nTqtJ29OpnRxiZyVClOzPRQ+0lS3urE5Z5BlOkE9uhUmjCzVyf7Am0TNpy84RnPT04ezX0fbEKhyVX7GJXqZOFQO99//hopeBvxKzSTdQqrYypocrmwmsDp1tRrGxOMddjVwY45HlMqxcpgq86iklj+45zFt57+GHhojMnk5rNGkxZr5uKHPmK4+oE+xm6SqeYA8Ywb0pdv9zjZVl7n+/3Dfh1HiU5H4SFe1fsetzSPLBzNip3VZKencNWSNXhQXDA+j8wEKycNzQNbPBUuO6P/+iUag0cuHssVvnvBKYN7cM1Jfdmwp5o3v9/NojMKiLGayEyws2L7flZu388dQU0hbj59CAsn5tPnxjebfSdWnCRSw9NXncR5Dy5lguG93ob2SsNjWPmiqAo3RiAv8z+IXTgui2e+2oEbgyunF7BydyOvrt2PRmHFiQ0nVcRQWDiSWo+Ft9bs4Z1fTqO8ppEz+qVFfc10FAnmjmNPL9/BO2v38NjPvE8L1Q1OCsNUTQZ7+dsSfvnsqqiOX3IYbRHqmlxs3lsddt0jnxcd8vFaY22sgG9e4jHL44w31jfL3OpUDGvcuXwQM4t1VQ6yMtLY44olNbs/8T3y+Ns7G4lV9bx20wJe+G4/N796sO3bq6t3AXDWyGxe+rYkcMz5Y3KJsZl45PMicpId3Hx6AW6P5sonVwLw4lUTaXJp/u+zrVw6tQ/5vsbmAHU7VvFf37FSs/K4+fQCzq1tIj3extJNpVz4n68C25Z4wjcKP398L1xuD7evCK6i0WRTRg+1/2AJlKqnn9rFKGMzE6zbsLhqSKAuqptkMK1MrHbn0YiVUp3EDmzUe6w4MTF9aDaW5CRqEvrSmDKIrRWN/PbVLcRRTx02bDhJUHX0zUjgyT05+EtRLhjfi+dW7MTp1i3au8we2oOp/VIpzElizj+97bX+78IxZCc56JsRy7c7DgTa7dhpJCs1mdeum8pJN7/TLN1rFs9k895qznrgCwZlxvP2FdMYVdNIvdPdZhuhSMyGondaLJv3NW979dYvpnLiPz4NpOnPZxbwl5e/wqacaODWeaO48cVVNGHmpln9yMtM58ePrsZA0ytesbnaTIzNRnWjCzuNpKnKQFWlRlFBPGbc9FMl9FL76J/Xi/eLmrhpzhAmDh1Awe3LGKCPiTSEAAAgAElEQVSKcWHgwhy4KSWras4b1ZNThuaCNQYssWCNpTqmFxNv+wQDDwnUsupPc8n/wwcEl3J9ddN0xt32QbPzvPXMoTz48ppmywqyEhh3xlTGnXFwWV2TiyVvHPw+7j5vOL96rmWTiDvPGcZvXvguzCetyaSCmQOT+XqT92ElT+0lRVVz1ohMCnrG88q3xRTtLcfiq6pVQL/0WLaWVlOPDbc2SFcHcJg0Dk81dpw4aCRr2HSITeNvS/dgVh5KdSJuDBKoI1HVcta00dz2cRkWXJiVm2xVRjLVmPCQqGopUEWkm6roqcuwKDcNyQOwZ02kvKqGuOR0frMylkK1jRHGFkZYqsj2fBXm/OC3LIHP4fLgFjA1wDLoYzaz1p2DCzMeFCZ3IyOy40AZGIbBmq07GWzsoCR9Kn0LJ/Hrd/fxoXskKxedT9O+aianxmIoxfjqRppcHlLirMT52hnvqWxgu85ku84E3zODv+bmn/9exuf7ynn04rGMzkvmDy+vYV5OEn96fR1o+NWpA7h4cj6G3cK4gd59U76JY+2uKn585inNzi8F+OYPMzGUIjHGwprFM3lv3R7OGJ6NyVCM7JXMj8f1arbP2PwUxuanUJidyAX/Xg7ATybkYRiKBWNzWfL1wRqOJiyUkkz/vGz2k8BbnvH866ejmVmQSXWDk/hVu/jJ+F4opSitbuSVVSXc+sZ6fj5yEhcMdvLksu2MOWkMw9weppXWUJCVyMY91dz7wWb+fEp/+veID/nGQv/uHBLMHcdufOn7Zn9Han+zblcVr6wu4ZnlOyK2fQvn1AgNqSvrnS0a5vsdqHO2aDjdnky4Odn4lgWmj+Bv34PHRS/Vg1fck1mr8xhaMIzzZs9EOXrSs7aJr1aV8MC7m7h6YF9+M2sQ4O3xuPudMtDgiEukR4K3RGVcfgp/nz8iEMzdPq8w0HsV4MJJeazd5e0AoZS3UTzAf6+exLDsxEBvxIl9Uwl15znDGJgZz+1vbSA11obJUKTH2wCYNiCdl6+ZjMWkOO3ez1rsmxprpby2iZxkB1ef2K9ZVTgoSkinRKdDmNqSH4/I5ZmvdgKaWBpIpJZ4VUfB4CEsW1dEgqqjTsVg0k7uObeArKQYXlhZwmXT+mBOyibfbUMp5W3L5GtADfDOidPokRlPHBAH1JbXsVWHlExpOHNyIalvb6Tc1/7ptrMKedaXMd9//iiuenIlu3wlxb84ZSADM5tnnBP7pgZuRsG9WRuw8c8LRhNrM/OTCb2aNfa2mgxG9krm7vOGB76j1Dhbs+NeNCk/ULIc+CQVRGqVsuXPs/njK2uaBXOJDgsx1oPVeQ3YMNnj2EtK4LvwJGRRQgkOi4nTp43HZCi+/EMuJpOiwenmhudWM7BHPP/+bBsNviq64OYIS399EtP++hFrdB/W6D58dt5J/DLJcbB3LHZW+UrSbpo9mNveXB/47C8dPRF6pzQ7D39fZQ8GB4gHi4PQ9qAZ8XbeuG4KL6wsDjyATR+cwe9fbv6Z5KW2zEtirAdvOa9eO5nc5JbbDM1O4LwxuRGCOcUeUql0ZLFOe7/vldobPfz+XG/gMXcKzPz7UjYGPTRe1DufR3cX0S8jji2+7yjeZKbaebBquGied//7Pgqfd501/mTO71vL22v2RMxL547I4s1VO5g+KI3/vWgyAP5f+4n9d3P1U952wOOyUlhd5G13NnVABh9tKkMB8aqemcbXnD0qixe/2YWBh5t+fCok5kBcBkZcD7aureAXS7wP3EWXnNasK8Ac33Wx6cofgdngvLwKrvFd2/0yDv52MhNbtpWOt0cOB3qlxPI55dgtJuLtFu5ZMBLAG8wB103v32KfRy4aG7HnbnLswSYecTYzZ41s2TEunElBeaf/937p1D7NgrlwzL423/F2Cz+dkBdYnh5v49KpfThndA5JMd40nTjQWw1uN0wUZHl74w7MjOf+C0ZFlcbOIsFcN+ByewLDUoQz+95PA6/9PRePxK1vrI+47kA7jsnVk3JON31BjiojXR1ggComT+3FrDzU2zNg9DUwbD5nPFhMtW/ct+vS+0NSLg4gN2hYiuB7tBHyMfkzDY1uNjyB3WKiZ9LBTNFsGIFMQwXdAEf1Sm7zXMwmg0un9iHGZmb+mNwW60fkJlHdEP6zi3akg1evncwZ//y82bKMeH/6FbU4qMXBn+YOZfnWcnaRxi4NeSkxbCuvw5kykPT8FK7qMzywf/OBSg6ymJonymYJf+3ZzCZOHdKDJV/v5PZ5hc3WWc1Gs+8l9Jihy0LXm31/33pmIR+u3xcICv3bzRsV+Qay6IyCFsHc27+YxrVPf9Oi9M3PGWZcMpu5edus0Jub/zc5JCshcG35b3QJdgtPXDKeBz6O3IsuJa55u0dDqaiHvgj3eUarICuRbWW1PPJ5ET8amtlsiJctt/2IDzfsY0KYhxY42LnIYjLCXhfm0B9g2LS3vs1zV06kqt7J1Dubl7RGM3xNJGbDYFLfNJb9UN7qNk7MuFTL9qj+hzO/Rqzef5YkqvBem5U6jofds5kweAwPf+1t033T0ObtmueOyA4Ec5H4r6Wx+Smtbhcs+MEj1B/mDGZkbhLjQ4L/V66ZHHHIlkMZgiVa4Y4Z6b4WrK1hZfyBXFcmgwZ3A79YsoqCm99pNn6YX2jpXXv8/FaHGZ7BL1JAMrlf+IzfS2OnkXT2c5bxKbeYH2Gp9Rd8af85N1qe4ULzexQa29iis/m3+zQub7qe9Qu+gFNvgR4FfHjDiZw40Fs1aTO3fcmH3kwCwVyYUhkjKHMxm1Qg0zicfMxkKH46IQ9rhDRGypAijaf71U3TA68n90sNOwad/wl9cM8EwJsx/nRCXuC9JvVNDZzjoYyzFXrTtAcFNa9dOyXwOty5+k8n9Bjh3t8S9F2Fbm9u55uM3WK0+r1OCxkTTWvd5vV2uMHFPfNH8OuZA1ucY2vfUWjajySwAeiT5m38P6V/WrPzNJsMZhRkkhBhzMPg9w93I470vTXbJmS/a07q2+zvRIclbC1D8PV2qM36owl+rebI2wTnFTro3U1hjmuP8PATrUP4qQa09ruIsZo5b2xui22G5yYxNMIYmEeLpZXP3C+aB4SuTkrmuoE3vt8dcd3Ty3c0+7utsbUOlwUX6RygriZ8e7kBjlqK1R4MND1VOekcwKGamGZ8xzTju2aN+N1a8bFnBM85T+QVzyR26h4tjned5eCTVnq8jfzUWKA04s01ODuIXDLXUnCmaTYOBnNGBzyVRjqmxxdlhgabB0vdvPumhHn6TIuz8eENJ+D26GbV5qagAM7/roeSIYYGacElMIU5iZw4MJ2PN5ZiNRmBdIem32Y+uK5Hgo2spJaDzwYPmdOiZC4ove1RSmANSk84PyrsybxR2fz3m5Jm+wQLHT3gcJPlH5A4dGaMQzlepIeGaA3JSmD5jdPJCClxapMvjVaTEXYg5Gh+O9aQ79rpji40C97vUD/6tgZthtYD5EgBVrjgNbRE91B1RKnYsSqafMl8BKXQXcVxE8wppWYB/wBMwL+11nd0cpLajcej+XRLGdP6px3Rj9TfS/VIJVPFRGMdZtz0NXYzWm0k0ddTK5YG9hNPrbbjwkSm2k8cdaSpKmJUI7wBk21x1GoHNdix4sKKi9zNpdwc5p5QqhN4xT2ZnTodJya+8/TlO92nzWE1Qm9U/qlsbJa2M0lTyGfsDxLCDeMTfOMxGSqomrX9HUmA6G9sHDp2WazVRJ/0OHZXNu/M4g+STIYKnMyRlMyFBtH+I7V2TG81q/czX3L5xDZLklqU5LVzBh48e0YkwVVVmrZLgo80haHpOZRr5EhL5oCI41RGI2IwGcUphAZWTa7oevkeSQDr/23npca2sk3k40e6dsItP9JAuzuJppo1mtLeru64COaUUibgfuBUoBj4Win1qtZ6XeemrH08/mURi15bxwMXjGJ2Yc+o9gmXuf3jg82HnQY7jZxr+oRpxndMN63C8HV3cmvFep1HhW/YihLSiaeOeFVPArVUEstGPYD9nng262zO7G9l45YtxKp6EqhH4UFjsDH9R7xVEoNSmr062Ru8aTN7ScZ1GJdp6E2t0elNrz2KTDI0c/UfKtyz/9mjcgIjg5sNA5MvM++IB+NDCaYi7XvL3KFU1DYFhovxt3WyBrULDNbo9AQ+y0N5ug0tJYv0EBL6fnAwaA4utYsmMw4NTiztnIGbDMWDPxnNfz7bxhPLDgbE548/2PMuNN5vqzTHv3l7jffZWjAX+h2EPrQcLf53PZI2e6HXYrj2iuEcSQDr33feqGzMJhW23Zo58ODXcv9I3024a7s7BB/tJZp86Ujyzq7iuAjmgHHAFq31VgCl1BJgLnDMBnP1TW427a0OzHVZ0+jinAe/4K/nDKcwp3kbhB2+6X12HfD+X9XgZPGr67jptMGBOT9DhZtW6FBYcTLW2EC+2stkYw0nGatwqCYO6Fi+yTqf27b1p4oYSnUiVVEOnAmwy5POJ66WA1bO75HLizujH0D3wQtGEWMzs/A/4bv3h5rQN5X/flsS6J3k16wEKmSZn/8m6M+gL5yYF6hayk+LxWJSON0asymoZK5DqlnDL48mDmjWXido+1RfA3p/SYB/3evfeXvofrm1nP4Z3u/3UM6orZtm8Odzw8wB1DS6mDsiq/kxggLvaG7CoZn64WTgpxX2JDm2eVuvO88exv66JpJirCTFWLllbkEgmJvUN5U/n3Ww48aFE/N5yt90wfdZLv31SYC3feJbaw42eQjer71EOuW7zxvOCQPS+e83xc16XIfzwAWjAr0uAf7fjwZhNhQvflPCpVN6H3Ea/Zef//t55OKxYcedbE1oaUxbwdyUfmk8+kUR0/p7q/cBJvdL4+213lH8s8L07gxlCvptT4kwrlhrwfShlMy1R6lpdxFNMCdt5rqObCA4EigGxndSWpr5aMM+7BZTs+Eo3l+3l0sf9/ZU+m7RDOKsZr7aVs6GPdXc9e5GHvvZODwezVPLt9Pk1gdLhny54EOfbOXFb4rplxHHVSf2DX1LVhRVcN6/vjzElGqyKCdXlXK66QvmmJYFBrncpVN40T2V19yTWK4Hc9OgwXy7NXKP1dZ8sin8yOPRxD6/mN4/ULo4oyCzjZt18wjn3NE5TB+U0WIIioUT8ynZX8/l0/oEloWWWPj/8pee3DJ3aNh3DG4z1xHPgc0CoFMH8MrqXWzZV9Nqqc5/LhrDzx5dwaIzhgSWeYK2z/JNgh5areMKaoPUWslkJJFuRlP7e2+CBz9Tb9u+cN3+g2/Y0VwflpAM+3Ay8OB0/PDn2TjdnmZzBnvTEtTuKiRdAzPjA/NcDsnydirpFTREh7+jyT3zR3DmyGz2+4ZkuWRKHyIJfQBpTaSHiNOG9cRmNvHGdVOZfMeHlByIPEZkaOn/lSd485hLp0ZOI3gD2zFR9J780dBMnltRHPhcTxqYwWmFPQNte5MPY8LyAS3G/mrulCE9WLN4JhaT4hbfcBpnj845GMyFaY/ZmuBr67Vrp7DgoS+pbXLjv2TD/VaC86vgn2y4ALA7lCQdiYFB37fNbOLSKb35dyvT23WHz/N4CebCfVMtfk9KqcuBywF69erVYoeOcPGj3ifO4GmzfvPiwfGTrnvm28CTIhxsY/Ofz7cFhvj42WTv0/Djy4q4bFqfQAmdf1yg+iY3DquJ9buruP7ZVWzYE76TQahsSplq+p5haivjjA30M7ylMfXaytuesbzhnsAWnUWRziT4Iw4ehqC9hLsHPXDBKL4rruR/P/FOeH/9qQMCwVykH+ft8wpZu6sy0NPu4PFVi0AOvOdyW0gJSctqVl/JXBvn0KzNXAfnHT+f3p/dVQ1s2VdDgsNCVYMr7DhRJw/q0WLKttOHZ/HWmj08eMGoQCmk1WQwOi+Zy3w3bLPp4GTV/i4Q0dYEPnDBqLDfz8ZbZwUC5exk780z3LV08+kF3PrGOiwmFXFIkzNHZDWbWQRafubBT+yXTOnNLa+v439/Mjq6k8D7fZqM1ttY/mR8XotldouJZy+fwCBf4BZsQI941iyeGRgbLznWGnZKvWAnDEjn/V9N45S7w4/rGCzSPSs40P3becP5+3ub6BlFadShePqyCVFtd9tZhfzPjIHNguT7LxjFG74x0v5y9rCw+/3+tMEthj2aVZDJtSf3Y0iYz9rPX4oeF3StJdjNnDI4IzAGYaQajv+ZMYC73t3UYnlwe8zCnESSY63UNtW3WjJ3KNWsR1IF3V5G9oo08FDn2vCnWS3ylytP7NtqMCcdILqOYiB4cK4cYFfoRlrrh4CHwDs369FJWuC9cXs0tY3uZj/q4EAOCGRwP5TWBpb953PvRbqzop4Ne6qo8U0A//76vWTE27j8iZX8Yc4QPt64r9VA7oNfTWPDt5+x89OnONn4hgGGt9ddlXaw2tOXp5zT2aqzyBg0iefX1UY8TrghTo5UaIlCSqyV2YU9mV3YMxDMgbdk59PNZWGP0dZNMVqhmW6Sw1tSMDgz/A3jYLuugw3kO6I3ayh/8HbNSf2ob3Jz/rjoHlBmF/Zk2+2zQ0qYFC9eNSnwt7fkoXnVVbj2bcFumVvAvR9sjtiuM7iH3o2zBzM6L7nFuFUACyfls3BSPgCPXjyOV74tadFj8p4FIwMDl/rF2czkpjjY6WuWEJzh/2xKb37WDlWEoX4U4VzH94k81E7cYTwM5bfS6D5YpIec4KYDE/qk8uwVE1s9zsf/c2KLEsn2YjEZZLTScSJ0zK8rpvXhX0u3NhtqRGtYu3gmNnP4HrF+3y+a0aKE9rtFMwDvNX/JlD48uWxHsxkHFk7M4zFfJ6E5w7KwmIwWE9+3NexNuLWRAu3QZh0QXc/ZjvZMlMH50RbuumyrjaGUzHUdXwP9lVK9gRJgAXB+5ybpYBs3gN6/e5MJfVJYtrX1OUdfXb2LW88ayjNf7Qi7ftY9n9I33Zuxf7yxNBAM+kfiDmXCzRMn1dNn9xtkPnwVfRsrcVvMfOEaRO9TruITzwgufbMSULxx3RSG9ExAKcXzEWZwAO+gn8FevGoSZz/Y+qwO6fE2Sqsjz3kY/FN77/ppEZ+U/+/CMVTVt9/Aw+GEZgz5abE8f+VECtsYT8lkKNy+yK6tMbbawy+nDyDRYeHc0TmHnPm31aYvOPM7cWA6G/dWhx3aJNiFE/O5cGJ+VO9vt5iYOyK7ze36psfxqxkDozqm2WTw6W9ODsyOcDw1Im/tZrT65hkMX+ydpq+9HiLy06ILHo+G384axNT+6Uzul0pBVkKgzV80NQThxlYM/m32Tott8RC4eO5QFp1RQFW9i8QYC1ec0JcrTmjenCVi+7dWSvEj7XOsdoDoqGC+IwQHxKcMzqCitvmc0MfC59nRjotgTmvtUkpdC7yDd2iS/2it17axW4e7PmSO07YCOb9/BZVEhRNcahcqFe9k1efkVDJNryC98ntMX1aDLRH6nwL5U2HAbIZZUrA4LJwCnF7yLa+t3kVanC2qhvvBVV82s8GgzJbtVXKSHcRYTZTXNPH+r07AYTUFpnv679WT2LSnmoKsRE7/p3d6quC3DZ377uVrJrPOl4HbLaawmcz6W2a1me5ohXtSbm0k9b+dN5x/vL8Zi0lRVe8tNU08jHY/h8phNXH1if065NjB1Ty/mTWIn07Ma7U05ViSYDdT1eA6rp7GW/tdJjosQds1XxdjNQWqy491r1wzudlcvH6GoZjia2s5qyAzEMx1JOUbyieSSD2BW3umaj5ocPjlfsmdPCPBBeOPTjOk9hIcrP174dgW64+nvCCS4yKYA9Bavwm82dnpCBb6dBCt+z9qPZgDOGd0Dsu/WcmVptfprXZjVS7y1F7SVaV3g1IguTcUzoP+M6D/qWD2VlWZgOAypjvPHsZFk/KajRm1/MbpjP/zwcm0fzzOP4dn88bt714/Lew0MJ/99uQWy7783cmYDYP0eFtgiqvFZxRw86trm01/FWpEbhIjcltvv+FoZSqajjZ3RHaglMlfojg2v+0pvI5lwVVTJkORE2YOzWPVK9dOYfnW8uNu4NSrT+wbmDcyktDAYMXvT2n1t3UsGZ6bFOjd35a2qvw7mv+BLyGkycnBnu8t0xcpoDhrVDb/Wrq12TJ/h6Sko/BQGOyDG06gvsnd6bM6HKq2gjXpzSqOSEe2m7r1zKHc7y5hzvov2aKzadQWPvYMZ4PuxWadzeO/+Qkk5kbVEt9hNTE6r3nJU48EO389Zxi/9k12ffu8YRRmJ3HjS98zMDOeZb+bjkbTM7FlL7B+GeGHKgm3rX8Ylsn90pqN3dVVjeudwivXTGZYTtfKDEN15SfZ3mmx9D6Gqgnby29mDWpzm9A8J3hi++NBjK9q1XEMVAHeM39EoJOA/2M/1KFJHr14LINC2uKe6Rui59PfnHRY7SuPRN/06IeZOpZEKintlRLDjoq6bjEI8/H1Sz/GhKuuO1xKNe9NaLeYmDRmHMNX/R/+Fmc/HpfLxL5pWHdXQdKRF5OfOyY3EMyBd3DUH4/zzs8X2ibsuSsmkpvioLrBRb9DyBBG9Upm9c0zmlUVHYq3fzm1xZRGnS3a0gXRdd1//ijy04690souHIO3cMe8whZTEf50Qh51ja42h0k5GvxTqgXzf/7hSoXbGoJkTF4yL1w1KVCqF25uWRGe/3McFdID98GfjGJtSVXENtjHEwnmOkhdk4sNew6/bYfdYtDgPNibMNywEBP7pfHkJRO46JGvcHk0V53Qj16pMZwxPKvlxu0kUtXVOF+vxJ6HUSB1uIEc0OKpVrSP9pqR4Hh12rDoZmI52o6nquUF43qxIKSHttVs8PPp/TspRW3zBxVtVbP6Z+jxj6W47HfTA/ng8fQdHi1KKV7/+ZRmYzqCd4zGQxmnsSs7/sseO8knG0ujHpcrnDEh1Z6zCzPDbjelfxprFs/k1Wsnt7iQhThc/sLOJZcfm8MTCHEs8ldrhw6vAs1LTf2dUvr6mqRkJto7td3v8WBoduJRGUXgWCUlcx3kcHr+9UmPpdHpobSmkTvO9g5kazObAk95b37vHa383h83H1/LbjExLEeq9kT78Q+xcjy2PROio0zqm8rNpw/h7NE5LdYFN7t54IJRfL6ljLQwA5kfiZW/P4XGMPNyi+OfBHMdxD/IqdlQ3HTaYBa/1vY0sR/ecGLEdf6Bgq0mo0OrUUP95ezCds9wugKlYMHY3LY3PE5N6ZfGS9+WhO2pLIQITym4eHL4wamDh8840k46b143NeysBuFmuRHdgwRzHSQrycG8kdksnJRP7/TYsMHcddP7c69veqq2+Kf5amuIjvY2f+zRG2/omcsmHPZwLu1t2+3tM5tEV3XH2YVcN71/2EFXhRDhtTYMTHv2wPXP+yuEnwRzHcRkKO6ePwKABqc77Da/mN4fm9ngr+9sbPN4FpPBS1dPCrSxOB5N7Bt5CiRxdNnMJqliFeIQtTb+nVKKW+YWdOt2XaLjSAeIo8A/yO7Fk/ObLTcZimtOin4E/5G9kiUjEIE5WYUQx4ZoB2a+cGJ+2CFNhDhSclc4CkyGYuOts7AYBvuqGluMnSREtL66aXqzCeuFEJ2vs2ekEEJK5o4Sm9mEYSjuv2AUQNj5TIVoS0a8/YjG5RNCCHH8kZK5TvDmdVPJTjo4tdV/LhpDSZgJpoUQQhz7BmTEs7OiHvsxMM2Y6J4kmOsEoT2RTh7Uo5NSIoQQ4kjds2AEq3YeoMdhjC8qRHuQalYhhBDiCMTbLUztn97ZyRDdmARzQgjRxaXHy2CxQnRnUs0qhBBd3HvXT6Oq3tXZyRBCdBIJ5oQ4zpw8KIOymsbOToY4ipJirGEndxdCdA8SzAlxnPnPRWM7OwlCCCGOImkzJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhUkwJ4QQQgjRhXVKMKeUOlcptVYp5VFKjQlZ9zul1Bal1Eal1Myg5aOVUt/71t2rlFK+5Tal1LO+5cuVUvlH92yEEEIIITpPZ5XMrQHmAUuDFyqlhgALgAJgFvCAUsrkW/0gcDnQ3/dvlm/5JcB+rXU/4O/AXzo89UIIIYQQx4hOCea01uu11hvDrJoLLNFaN2qttwFbgHFKqZ5Agtb6S621Bh4Hzgza5zHf6xeA6f5SOyGEEEKI492x1mYuG9gZ9Hexb1m273Xo8mb7aK1dQCWQ2uEpFUIIIYQ4Bpij3VAp5QB6RShRC7f9+0BmmFU3aa1fibRbmGW6leWt7RMuTZfjraqlV69eEZIghBBCCNF1RBXMKaVOB+4CrEBvpdQI4Bat9RmR9tFan3IY6SkGcoP+zgF2+ZbnhFkevE+xUsoMJAIVEdL0EPAQwJgxY8IGfEIIIYQQXUm01ayLgHHAAQCt9SogvwPS8yqwwNdDtTfejg5faa13A9VKqQm+9nAXAq8E7bPQ9/oc4ENfuzohhBBCiONetNWsLq11ZXv1K1BKnQXcB6QDbyilVmmtZ2qt1yqlngPWAS7gGq2127fbVcCjgAN4y/cP4GHgCaXUFrwlcgvaJZFCCCGEEF1AtMHcGqXU+YBJKdUfuA744nDfVGv9EvBShHW3AbeFWb4CGBpmeQNw7uGmRQghhBCiK4u2mvXneMd+awSexttj9JcdlSghhBBCCBGdNkvmfIP2vurr0HBTxydJCCGEEEJEq82SOV+btTqlVOJRSI8QQgghhDgE0baZawC+V0q9B9T6F2qtr+uQVAkhhBBCiKhEG8y94fsnhBBCCCGOIVEFc1rrx5RSVmCAb9FGrbWz45IlhBBCCCGiEe0MECfincy+CO/0WblKqYVa66Udl19qiZEAABsUSURBVDQhhBBCCNGWaKtZ/wbM8M/LqpQaADwDjO6ohAkhhBBCiLZFO86cxR/IAWitNwGWjkmSEEIIIYSIVrQlcyuUUg8DT/j+vgBY2TFJEkIIIYQQ0Yo2mLsKuAbvNF4KWAo80FGJEkIIIYQQ0Yk2mDMD/9Ba3w2BWSFsHZYqIYQQQggRlWjbzH0AOIL+dgDvt39yhBBCCCHEoYg2mLNrrWv8f/hex3RMkoQQQgghRLSiDeZqlVKj/H8opcYA9R2TJCGEEEIIEa1o28z9EnheKbUL0EAWML/DUiWEEEIIIaLSasmcUmqsUipTa/01MAh4FnABbwPbjkL6hBBCCCFEK9qqZv0X0OR7PRG4Ebgf2A881IHpEkIIIYQQUWirmtWkta7wvZ4PPKS1fhF4USm1qmOTJoQQQggh2tJWyZxJKeUP+KYDHwati7a9nRBCCCGE6CBtBWTPAJ8opcrw9l79FEAp1Q+o7OC0CSGEEEKINrQazGmtb1NKfQD0BN7VWmvfKgP4eUcnTgghhBBCtK7NqlKt9bIwyzZ1THKEEEIIIcShiHbQYCGEEEIIcQySYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQoguTYE4IIYQQogvrlGBOKfVXpdQGpdR3SqmXlFJJQet+p5TaopTaqJSaGbR8tFLqe9+6e5VSyrfcppR61rd8uVIq/+ifkRBCCCFE5+iskrn3gKFa62HAJuB3AEqpIcACoACYBTyglDL59nkQuBzo7/s3y7f8EmC/1rof8HfgL0frJIQQQgghOlunBHNa63e11i7fn8uAHN/rucASrXWj1nobsAUYp5TqCSRorb/UWmvgceDMoH0e871+AZjuL7UTQgghhDjeHQtt5n4GvOV7nQ3sDFpX7FuW7XsdurzZPr4AsRJIDfdGSqnLlVIrlFIrSktL2+0EhBBCCCE6i7mjDqyUeh/IDLPqJq31K75tbgJcwFP+3cJsr1tZ3to+LRdq/RDwEMCYMWPCbiOEEEII0ZV0WDCntT6ltfVKqYXAHGC6r+oUvCVuuUGb5QC7fMtzwiwP3qdYKWUGEoGKIz4BIYQQQoguoLN6s84CfgucobWuC1r1KrDA10O1N96ODl9prXcD1UqpCb72cBcCrwTts9D3+hzgw6DgUAghhBDiuNZhJXNt+CdgA97z9VVYprW+Umu9Vin1HLAOb/XrNVprt2+fq4BHAQfeNnb+dnYPA08opbbgLZFbcNTOQgghhBCik6nuWog1ZswYvWLFis5OhhBCCCFEm5RSK7XWY8Kt66ySuWOS0+mkuLiYhoaGzk6KOEbY7XZycnKwWCydnRQhhBAiLAnmghQXFxMfH09+fj4yVJ3QWlNeXk5xcTG9e/fu7OQIIYQQYR0L48wdMxoaGkhNTZVATgCglCI1NVVKaoUQQhzTJJgLIYGcCCbXgxBCiGOdBHPHGJPJxIgRIygoKGD48OHcfffdeDyewz5eXFxc2OUXXXQRL7zwQtTHWbRoEdnZ2YwYMYL+/fszb9481q1bF1jf1NTEL3/5S/r27Uu/fv2YM2cOO3bsCKxXSnHDDTcE/r7rrrtYtGhRi/fZu3cvc+bMYfjw4QwZMoTZs2dHnUYhhBCiO5Jg7hjjcDhYtWoVa9eu5b333uPNN99k8eLFnZ0sAK6//npWrVrF5s2bmT9/PieffDL+adFuvPFGqqur2bRpE1u2bOHss89m7ty5gUDUZrPx3//+l7Kyslbf449//COnnnoqq1evZt26ddxxxx1HnG6Xy9X2RkIIIUQXJcHcMSwjI4OHHnqIf/7zn2itaWho4OKLL6awsJCRI0fy0UcfAfDoo49y7bXXBvabM2cOH3/8ceDvG264gVGjRjF9+nTCzUm7cuVKTjjhBEaPHs3MmTPZvXt3m2mbP38+M2bM4Omnn6auro5HHvn/7d15dBRVvsDx748ECAQIRONThzXPEYEkhpCwiBo8SlCeOIBoADExT2HyWBzGGZSRGQTEAZlxUJbDqIcdTHAZ0SfiYwBH1APKYkYIOxjcUDCQyKoh+b0/uhI60J0FAp3u/n3O6ZPKrarb91dVnb65t+re+UyfPp2QkBAAMjIyaNSoEatXrwYgNDSUYcOGMX369ArzPXjwIM2bn53sIy4urmx52rRpxMbGcuONNzJ27FgAcnJy6Nq1K3FxcfTr14+jR48C0KNHD5588kmSk5N54YUXvMY4Y8YM2rdvT1xcHAMH2hCFxhhj/I89zerFxP/NZfu3P9Zonu2vbcJTfTpUa5/o6GhKSko4dOgQS5YsAWDr1q3s3LmTlJQUdu/eXeH+J06cICEhgeeee45JkyYxceJEZs2aVba+qKiIUaNG8dZbbxEVFcWyZcsYN24c8+bNq7RsCQkJ7Ny5k71799KyZUuaNGlSbn1iYiLbt28nJSUFgBEjRhAXF8fjjz/uNc8RI0aQmprKrFmzuOOOO8jIyODaa69l5cqVLF++nE8++YSGDRty5Ihrxra0tDRmzpxJcnIy48ePZ+LEiTz//PMAFBQU8MEHH1BUVERycrLHGKdOncoXX3xB/fr1KSgoqDRmY4wxpraxypwfKB3Y+aOPPmLUqFEA3HDDDbRq1arSylydOnVITU0FYMiQIfTv37/c+l27drFt2zZ69uwJQHFxMddcc021yqWqHh8UOHdA6iZNmpCWlsaMGTNo0KCBxzx79erF/v37ee+991i5ciUdO3Zk27ZtrF69moyMDBo2bAhAZGQkhYWFFBQUkJycDEB6ejr33XdfWV6lcVcUY1xcHA888AB9+/alb9++VYrbGGOMqU2sMudFdVvQLpX9+/cTEhLCVVdddV7lqFRoaGi5hyQqGkrj3EqXqtKhQwfWr19f7bJ99tlnJCYmct1113HgwAGOHTtG48aNy9Zv2bKFAQMGlNtn9OjRJCQkkJGR4TXfyMhIBg8ezODBg7n77rtZt26d1wpjRcLDw4GKY1yxYgXr1q3j7bff5umnnyY3N5fQUPtYGGOM8R92z1wtdvjwYTIzMxk5ciQiwq233srSpUsB2L17N19++SVt27aldevW5OTkUFJSwldffcWnn35alkdJSUnZU6uvvPIKN998c7n3aNu2LYcPHy6r6BQVFZGbm1tp2d544w1WrVrFoEGDCA8PJz09nccee4ziYtdUuosWLSIsLIzu3buX2y8yMpL777+fuXPnesx37dq1nDx5EoBjx46xb98+WrZsSUpKCvPmzStbd+TIESIiImjWrBkffvghAIsXLy5rpatKjKXH67bbbmPatGkUFBRw/PjxSmM3xhhjahNrgqhlTp06RXx8PEVFRYSGhvLggw/y2GOPATB8+HAyMzOJjY0lNDSUBQsWUL9+fbp3706bNm2IjY0lJiaGhISEsvzCw8PJzc2lU6dOREREsGzZsnLvV69ePV5//XUeffRRCgsLOXPmDKNHj6ZDh/NbJqdPn86SJUs4ceIEMTExrF27lqioKACmTJnCmDFjaNu2LadOnSIqKor169d7bE373e9+V+6+PXebN29m5MiRZa2NjzzyCElJSYDrYYfExETq1atH7969+fOf/8zChQvJzMzk5MmTREdHM3/+/PPy9Bbj9ddfz5AhQygsLERV+e1vf0vTpk2reKaMMcaY2kG8dd0FusTERN20aVO5tB07dtCuXTsflShwfPfdd9x5550MHz6cYcOG+bo4F82uC2OMMb4mIptVNdHTOmuZMzXu6quvJicnx9fFMMYYY4KC3TNnjDHGGOPHrDJnjDHGGOPHrDJnjDHGGOPHrDJnjDHGGOPHrDJnjDHGGOPHrDJXy4SEhBAfH09MTAx9+vSpdL7Qhx56qGxQ4B49elA63Erv3r1rdK7R6dOnExYWRmFhYY3laYwxxpiLZ5W5WqZBgwbk5OSwbds2IiMjmT179gXl8+6779boALhZWVkkJSXx5ptv1kh+pTNFGGOMMebiWGWuFuvWrRvffPMN4Jr9oGvXrsTFxdGvXz+OHj1a4b6tW7fmhx9+IC8vj3bt2jF06FA6dOhASkoKp06dAmDjxo3ExcXRrVs3xowZQ0xMjMe89u3bx/Hjx5k8eTJZWVkAzJkzh8cff7xsmwULFjBq1CgAlixZQufOnYmPj+fXv/51WcWtUaNGjB8/ni5durB+/XomTZpEUlISMTExDBs2rGzuWW/lKi4uZsyYMSQlJREXF8eLL754oYfWGGOMCRg2aLA3K8fCd1trNs+rY+GuqVXatLi4mDVr1vDwww8DkJaWxsyZM0lOTmb8+PFMnDiR559/vkp57dmzh6ysLF5++WXuv/9+3njjDYYMGUJGRgYvvfQSN910E2PHjvW6f1ZWFoMGDeKWW25h165dHDp0iAEDBtCtWzemTZsGwLJlyxg3bhw7duxg2bJlfPzxx9StW5fhw4ezdOlS0tLSyqYBmzRpEgDt27dn/PjxADz44IO888479OnTx2u55s6dS0REBBs3buSnn36ie/fupKSk0KZNmyodB2OMMSYQWctcLVM6N+sVV1zBkSNH6NmzJ4WFhRQUFJRNIp+ens66deuqnGebNm2Ij48HoFOnTuTl5VFQUMCxY8e46aabABg8eLDX/bOzsxk4cCB16tShf//+vPbaa0RFRREdHc2GDRvIz89n165ddO/enTVr1rB582aSkpKIj49nzZo17N+/H3DdD3jvvfeW5fv+++/TpUsXYmNjWbt2Lbm5uRWWa9WqVSxatIj4+Hi6dOlCfn4+e/bsqfJxMMYYYwKRtcx5U8UWtJpWes9cYWEhd999N7NnzyY9Pf2i8qxfv37ZckhICKdOnaKqc/J+/vnn7Nmzh549ewLw888/Ex0dzYgRI0hNTeXVV1/lhhtuoF+/fogIqkp6ejpTpkw5L6+wsDBCQkIAOH36NMOHD2fTpk20aNGCCRMmcPr06QrLparMnDmTXr16VSd8Y4wxJqBZy1wtFRERwYwZM/jrX/9Kw4YNadasGR9++CEAixcvLmulu1DNmjWjcePGbNiwAXC1vnmSlZXFhAkTyMvLIy8vj2+//ZZvvvmGAwcO0L9/f5YvX05WVhapqakA3H777bz++uscOnQIgCNHjnDgwIHz8j19+jQAV155JcePHy97IreicvXq1Ys5c+ZQVFQEwO7duzlx4sRFHQdjjDHG31nLXC3WsWNHbrzxRrKzs1m4cCGZmZmcPHmS6Oho5s+ff9H5z507l6FDhxIeHk6PHj2IiIg4b5vs7GxWrlxZLq1fv35kZ2fzxBNP0L59e7Zv307nzp0B131wkydPJiUlhZKSEurWrcvs2bNp1apVuTyaNm3K0KFDiY2NpXXr1iQlJVVarkceeYS8vDwSEhJQVaKioli+fPlFHwdjjDHGn0lVu9sCTWJiopaOyVZqx44dtGvXzkcluvyOHz9Oo0aNAJg6dSoHDx7khRde8HGpal+5gu26MMYYU/uIyGZVTfS0zlrmgtiKFSuYMmUKZ86coVWrVixYsMDXRQJqb7mMMcaY2sha5txYC4zxxK4LY4wxvlZRy5w9AGGMMcYY48esMneOYG2pNJ7Z9WCMMaa2s8qcm7CwMPLz8+0L3ACuilx+fj5hYWG+LooxxhjjlU8egBCRp4FfASXAIeAhVf3WWfcH4GGgGHhUVf/PSe8ELAAaAO8Cv1FVFZH6wCKgE5APpKpq3oWUq3nz5nz99dccPnz4IqIzgSQsLIzmzZv7uhjGGGOMV756mvUvqvonABF5FBgPZIpIe2Ag0AG4FlgtIterajEwBxgGbMBVmbsTWImr4ndUVa8TkYHAs0DqhRSqbt26Ns+nMcYYY/yKT7pZVfVHt1/DgdJ+zV8B2ar6k6p+AewFOovINUATVV2vrj7QRUBft30WOsuvA7eLiFzyIIwxxhhjagGfjTMnIs8AaUAhcJuT/AtcLW+lvnbSipzlc9NL9/kKQFXPiEghcAXwg4f3HIardY+WLVvWVCjGGGOMMT5zyVrmRGS1iGzz8PoVgKqOU9UWwFJgZOluHrLSCtIr2uf8RNWXVDVRVROjoqKqF5AxxhhjTC10yVrmVPWOKm76CrACeApXi1sLt3XNgW+d9OYe0nHb52sRCQUigCOVvenmzZt/EJHzZ4CvWVfioYUwiARz/MEcOwR3/MEcOwR3/MEcOwR3/Jcj9lbeVvjqadZfquoe59d7gJ3O8tvAKyLyN1wPQPwS+FRVi0XkmIh0BT7B1T07022fdGA9MABYq1UYW0RVL3nTnIhs8jZaczAI5viDOXYI7viDOXYI7viDOXYI7vh9Hbuv7pmbKiJtcQ1NcgDIBFDVXBF5FdgOnAFGOE+yAvwPZ4cmWem8AOYCi0VkL64WuYGXKwhjjDHGGF/zSWVOVe+tYN0zwDMe0jcBMR7STwP31WgBjTHGGGP8hM0AcWm95OsC+Fgwxx/MsUNwxx/MsUNwxx/MsUNwx+/T2MWmrjLGGGOM8V/WMmeMMcYY48esMneJiMidIrJLRPaKyFhfl6emiUgLEXlfRHaISK6I/MZJnyAi34hIjvPq7bbPH5zjsUtEevmu9DVDRPJEZKsT5yYnLVJE/ikie5yfzdy2D4j4RaSt2/nNEZEfRWR0IJ97EZknIodEZJtbWrXPtYh0cq6ZvSIywx9mq/ES+19EZKeIfC4ib4pIUye9tYiccrsG/u62T6DEXu3r3B9jB6/xL3OLPU9Ecpz0QDv33r7jaufnXlXtVcMvIATYB0QD9YB/A+19Xa4ajvEaIMFZbgzsBtoDE4Dfe9i+vXMc6gNtnOMT4us4LvIY5AFXnpM2DRjrLI8Fng3U+J24QoDvcI1/FLDnHrgVSAC2Xcy5Bj4FuuEa7HwlcJevY7vA2FOAUGf5WbfYW7tvd04+gRJ7ta9zf4zdW/znrH8OGB+g597bd1yt/Nxby9yl0RnYq6r7VfVnIBvXHLIBQ1UPquoWZ/kYsIOzU6x54nHe3Utf0svOfa7ghZSfQzgQ478d2KeqFQ3A7fexq+o6zh+MvFrnWiqeY7rW8hS7qq5S1TPOrxsoP6j7eQIp9goE1HmHiuN3WpfuB7IqysNf46/gO65Wfu6tMndplM0X63CfSzbgiEhroCOuAZ0BRjrdL/PcmqAD8ZgosEpENotr3l+A/1DVg+D6YwBc5aQHYvzgGtfR/Y95sJx7qP65/gXe55j2Z//N2XE/AdqIyGci8oGI3OKkBVrs1bnOAy32UrcA3+vZCQAgQM/9Od9xtfJzb5W5S6PK88X6OxFpBLwBjFbVH4E5wH8C8cBBXM3wEJjHpLuqJgB3ASNE5NYKtg24+EWkHq4ZXF5zkoLp3FfkQuaY9ksiMg7XAO9LnaSDQEtV7Qg8hmtGnyYEVuzVvc4DKXZ3gyj/j1xAnnsP33FeN/WQdtnOv1XmLg1vc8wGFBGpi+siX6qq/wBQ1e9VtVhVS4CXOdudFnDHRFW/dX4eAt7EFev3TrN6affCIWfzgIsfVyV2i6p+D8F17h3VPdcVzTHtd0QkHbgbeMDpPsLpYsp3ljfjum/oegIo9gu4zgMm9lLimge9P7CsNC0Qz72n7zhq6efeKnOXxkbglyLSxmm9GIhrDtmA4dwvMRfYoap/c0u/xm2zfkDpU1BvAwNFpL6ItMGZd/dylbemiUi4iDQuXcZ1Q/g2zs4VjPPzLWc5oOJ3lPvPPFjOvZtqnWunS+aYiHR1Pj9pbvv4FRG5E3gCuEdVT7qlR4lIiLMcjSv2/QEWe7Wu80CK3c0dwE5VLes+DLRz7+07jtr6ua/pJyrsVfYkTG9cT7/sA8b5ujyXIL6bcTUVfw7kOK/ewGJgq5P+NnCN2z7jnOOxCz94mqmS+KNxPbn0byC39BwDVwBrgD3Oz8gAjb8hkA9EuKUF7LnHVWk9CBTh+k/74Qs510Airi//fcAsnIHba/PLS+x7cd0fVPrZ/7uz7b3O5+HfwBagTwDGXu3r3B9j9xa/k74AyDxn20A7996+42rl595mgDDGGGOM8WPWzWqMMcYY48esMmeMMcYY48esMmeMMcYY48esMmeMMcYY48esMmeMMcYY48esMmeM8SsiUiwiOW6vsZVsnykiaTXwvnkicmU1tv+XiGxy+z1RRP51seVw8npIRGbVRF7GGP8X6usCGGNMNZ1S1fiqbqyqf7+UhanEVSJyl6qurHzTy0dEQlS12NflMMbUDGuZM8YEBKfl7FkR+dR5XeekTxCR3zvLj4rIdmeS9GwnLVJEljtpG0Qkzkm/QkRWOROHv4jbHIsiMsR5jxwRebF05HsP/gL80UNZy7Wsicg7ItLDWT7uxLFZRFaLSGenlW+/iNzjlk0LEXlPRHaJyFOVlc3Jd5KIfAJ0u5BjbIypnawyZ4zxNw3O6WZNdVv3o6p2xjXK+vMe9h0LdFTVOCDTSZsIfOakPQksctKfAj5S18ThbwMtAUSkHZAKdHdaCIuBB7yUdT3wk4jcVo34woF/qWon4BgwGeiJa+qoSW7bdXbeNx64z+nGrahs4cA2Ve2iqh9VozzGmFrOulmNMf6mom7WLLef0z2s/xxYKiLLgeVO2s24piJCVdc6LXIRwK24JhNHVVeIyFFn+9uBTsBG11SLNODsZNueTMbVOvdEFWID+Bl4z1neCvykqkUishVo7bbdP9WZ2FxE/uHEcaaCshXjmjTcGBNgrDJnjAkk6mW51H/hqqTdA/xJRDrg1n3qYV9PeQiwUFX/UKUCuSqITwNd3ZLPUL5nJMxtuUjPzrNYAvzk5FMiIu5/s88tm1ZSttN2n5wxgcm6WY0xgSTV7ed69xUiUgdooarvA48DTYFGwDqcrkjnvrUfVPXHc9LvApo5Wa0BBojIVc66SBFpVUm5nnHes1QeEC8idUSkBa4u0+rq6bx3A6Av8PEFls0Y4+esZc4Y428aiEiO2+/vqWrp8CT1nRv86wCDztkvBFjidKEKMF1VC0RkAjBfRD4HTgLpzvYTgSwR2QJ8AHwJoKrbReSPwCqnglgEjAAOeCuwqr4rIofdkj4GvsDVjboN2FKtI+DyEbAYuA54RVU3AVS3bMYY/ydnW/ONMcZ/iUgekKiqP/i6LMYYczlZN6sxxhhjjB+zljljjDHGGD9mLXPGGGOMMX7MKnPGGGOMMX7MKnPGGGOMMX7MKnPGGGOMMX7MKnPGGGOMMX7MKnPGGGOMMX7s/wHjXhz3/Et7aQAAAABJRU5ErkJggg==
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Kernel-density-plot-of-the-scores">Kernel density plot of the scores<a class="anchor-link" href="#Kernel-density-plot-of-the-scores"> </a></h4><p>In general, the kernel density plot will be bimodal with one mode less than -100 and a second mode 
greater than 200. The negative mode corresponds to those training episodes where the agent crash 
landed and thus scored at most -100; the positive mode corresponds to those training episodes 
where the agent "solved" the task. The kernel density or scores typically exhibits negative 
skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go 
horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to 
a gentle landing (and a high score).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">uniform_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Uniform Sampling&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">prioritized_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Priority Sampling&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;symlog&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Where-to-go-from-here?">Where to go from here?<a class="anchor-link" href="#Where-to-go-from-here?"> </a></h2><p>Up next in this series will be <a href="https://arxiv.org/abs/1511.06581"><em>Dueling Network Architectures for Deep Reinforcement Learning</em></a>.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="davidrpugh/stochastic-expatriate-descent"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/stochastic-expatriate-descent/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/stochastic-expatriate-descent/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An expat scientist&#39;s musings about machine learning, deep learning, and living abroad.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/davidrpugh" title="davidrpugh"><svg class="svg-icon grey"><use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/TheSandyCoder" title="TheSandyCoder"><svg class="svg-icon grey"><use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
