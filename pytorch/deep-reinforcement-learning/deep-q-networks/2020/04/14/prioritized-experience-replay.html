<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Improving the Double DQN algorithm using prioritized experience replay | Stochastic Expatriate Descent</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Improving the Double DQN algorithm using prioritized experience replay" />
<meta name="author" content="David R. Pugh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on improving the Double DQN algorithm using prioritized experience replay." />
<meta property="og:description" content="Notes on improving the Double DQN algorithm using prioritized experience replay." />
<link rel="canonical" href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html" />
<meta property="og:url" content="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html" />
<meta property="og:site_name" content="Stochastic Expatriate Descent" />
<meta property="og:image" content="https://davidrpugh.github.io/stochastic-expatriate-descent/images/prioritized-experience-replay.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-14T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"David R. Pugh"},"datePublished":"2020-04-14T00:00:00-05:00","headline":"Improving the Double DQN algorithm using prioritized experience replay","image":"https://davidrpugh.github.io/stochastic-expatriate-descent/images/prioritized-experience-replay.png","description":"Notes on improving the Double DQN algorithm using prioritized experience replay.","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html"},"@type":"BlogPosting","url":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html","dateModified":"2020-04-14T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/stochastic-expatriate-descent/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml" title="Stochastic Expatriate Descent" /><link rel="shortcut icon" type="image/x-icon" href="/stochastic-expatriate-descent/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Improving the Double DQN algorithm using prioritized experience replay | Stochastic Expatriate Descent</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Improving the Double DQN algorithm using prioritized experience replay" />
<meta name="author" content="David R. Pugh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on improving the Double DQN algorithm using prioritized experience replay." />
<meta property="og:description" content="Notes on improving the Double DQN algorithm using prioritized experience replay." />
<link rel="canonical" href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html" />
<meta property="og:url" content="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html" />
<meta property="og:site_name" content="Stochastic Expatriate Descent" />
<meta property="og:image" content="https://davidrpugh.github.io/stochastic-expatriate-descent/images/prioritized-experience-replay.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-14T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"David R. Pugh"},"datePublished":"2020-04-14T00:00:00-05:00","headline":"Improving the Double DQN algorithm using prioritized experience replay","image":"https://davidrpugh.github.io/stochastic-expatriate-descent/images/prioritized-experience-replay.png","description":"Notes on improving the Double DQN algorithm using prioritized experience replay.","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html"},"@type":"BlogPosting","url":"https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html","dateModified":"2020-04-14T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://davidrpugh.github.io/stochastic-expatriate-descent/feed.xml" title="Stochastic Expatriate Descent" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/stochastic-expatriate-descent/">Stochastic Expatriate Descent</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/stochastic-expatriate-descent/about/">About Me</a><a class="page-link" href="/stochastic-expatriate-descent/search/">Search</a><a class="page-link" href="/stochastic-expatriate-descent/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Improving the Double DQN algorithm using prioritized experience replay</h1><p class="page-description">Notes on improving the Double DQN algorithm using prioritized experience replay.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-14T00:00:00-05:00" itemprop="datePublished">
        Apr 14, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">David R. Pugh</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      21 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#deep-reinforcement-learning">deep-reinforcement-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/stochastic-expatriate-descent/categories/#deep-q-networks">deep-q-networks</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/davidrpugh/stochastic-expatriate-descent/tree/2020-04-14-prioritized-experience-replay/_notebooks/2020-04-14-prioritized-experience-replay.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/davidrpugh/stochastic-expatriate-descent/2020-04-14-prioritized-experience-replay?filepath=_notebooks%2F2020-04-14-prioritized-experience-replay.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/davidrpugh/stochastic-expatriate-descent/blob/2020-04-14-prioritized-experience-replay/_notebooks/2020-04-14-prioritized-experience-replay.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/stochastic-expatriate-descent/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-14-prioritized-experience-replay.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I am continuing to work my way through the <a href="https://www.udacity.com/">Udacity</a> <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893"><em>Deep Reinforcement Learning Nanodegree</em></a>. In this blog post I discuss and implement an enhancement of the <code>ExperienceReplayBuffer</code> from <a href="https://arxiv.org/abs/1511.05952"><em>Prioritized Experience Replay</em></a> (Schaul et al 2016).</p>
<blockquote><p>Experience replay liberates online learning agents from processing transitions in the exact order
they are experienced. Prioritized replay further liberates agents from considering transitions with
the same frequency that they are experienced.</p>
<p>In particular, we propose to more frequently replay transitions with high expected learning progress,
as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead
to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which
we correct with importance sampling.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Prioritized-Experience-Replay">Prioritized Experience Replay<a class="anchor-link" href="#Prioritized-Experience-Replay"> </a></h2><p>Using an experience replay buffer naturally leads to two issues that need to be addressed.</p>
<ol>
<li>Which experiences should the agent store in the replay buffer?</li>
<li>Which experiences to replay from the buffer in order to learn efficiently?</li>
</ol>
<p>Schaul et al 2016 take the contents of the replay buffer more or less as given and focus solely on 
answering the second question. That is, the paper focuses on developing a procedure for making the 
most effective use of the experience replay buffer for learning.</p>
<p>Before discussing the procedure to sample from prioritized experiences, I need to discuss how what 
information a reinforcement learning (RL) agent has available to prioritize its experiences for 
replay.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prioritization-using-the-temporal-difference-(TD)-error-term">Prioritization using the temporal-difference (TD) error term<a class="anchor-link" href="#Prioritization-using-the-temporal-difference-(TD)-error-term"> </a></h3><p>You can't prioritize experiences for learning unless you can measure the importance of each 
experience in the learning process. The ideal criterion would be the amount that RL agent can 
learn from experience given the current state (i.e., the expected learning value of the 
experience).</p>
<p>Unfortunately such an ideal criterion is not directly measurable. However, a reasonable proxy is 
the magnitude of an experience’s temporal-difference (TD) error $\delta_i$. The TD-error 
indicates how "surprising" or "unexpected" the experience is given the current state of the RL 
agent. Using the TD-error term to prioritize experiences for replay is particularly suitable for 
incremental, online RL algorithms, such as SARSA or Q-learning, as these algorithms already 
compute the TD-error and update the parameters proportionally.</p>
<p>Using the notation develop in my previous post on <a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/11/double-dqn.html"><em>Improving the DQN algorihtm using Double Q-learning</em></a> 
the TD-error term can be written as follows.</p>
<p>
$$ \delta_{i,t} = R_{t+1} + \gamma Q\big(S_{t+1}, \underset{a}{\mathrm{argmax}}\ Q(S_{t+1}, a; \theta_t); \theta^{-}_t\big) - Q(S_t, a_t; \theta_t\big)$$
</p>
<p>In the cell below I define a function for computing the TD-error (as well as 
several addition functions that will be used by the RL agent later in the post).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">def</span> <span class="nf">synchronize_q_networks</span><span class="p">(</span><span class="n">q_network_1</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">q_network_2</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;In place, synchronization of q_network_1 and q_network_2.&quot;&quot;&quot;</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">q_network_1</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">q_network_2</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">select_greedy_actions</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">q_network</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Select the greedy action for the current state given some Q-network.&quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">actions</span> <span class="o">=</span> <span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">actions</span>


<span class="k">def</span> <span class="nf">evaluate_selected_actions</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">q_network</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Compute the Q-values by evaluating the actions given the current states and Q-network.&quot;&quot;&quot;</span>
    <span class="n">next_q_values</span> <span class="o">=</span> <span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="p">)</span>        
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q_values</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">q_values</span>


<span class="k">def</span> <span class="nf">double_q_learning_update</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                             <span class="n">q_network_1</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                             <span class="n">q_network_2</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Double Q-Learning uses Q-network 1 to select actions and Q-network 2 to evaluate the selected actions.&quot;&quot;&quot;</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">select_greedy_actions</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">q_network_1</span><span class="p">)</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">evaluate_selected_actions</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">q_network_2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_values</span>


<span class="k">def</span> <span class="nf">double_q_learning_error</span><span class="p">(</span><span class="n">states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">next_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                            <span class="n">q_network_1</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                            <span class="n">q_network_2</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">expected_q_values</span> <span class="o">=</span> <span class="n">double_q_learning_update</span><span class="p">(</span><span class="n">next_states</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">q_network_1</span><span class="p">,</span> <span class="n">q_network_2</span><span class="p">)</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_network_1</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">expected_q_values</span> <span class="o">-</span> <span class="n">q_values</span>
    <span class="k">return</span> <span class="n">delta</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that I have defined a measurable criterion by which an RL agent can prioritize its 
experiences, I can move on to discussing the major contribution of the Schaul et al 2016 paper 
which was an efficient procedure for randomly sampling and replaying prioritized experiences.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Stochastic-prioritization">Stochastic prioritization<a class="anchor-link" href="#Stochastic-prioritization"> </a></h3><p>Schaul et al 2016 introduce a stochastic sampling method that interpolates between pure greedy 
experience prioritization (i.e., always sampling the highest priority experiences) and uniform 
random sampling of experience. The probability of sampling experience $i$ is defined as follows.</p>
<p>
$$ P(i) = \frac{p_i^{\alpha}}{\sum_{j=0}^{N} p_j^{\alpha}} $$
</p>
<p>where $p_i &gt; 0$ is the priority of transition $i$. The exponent $\alpha$ determines how much 
prioritization is used, with $\alpha = 0$ corresponding to the uniform random sampling case. Note 
that the probability of being sampled is monotonic in an experience’s priority while guaranteeing 
a non-zero probability for the lowest-priority experience.</p>
<h3 id="Correcting-for-sampling-bias">Correcting for sampling bias<a class="anchor-link" href="#Correcting-for-sampling-bias"> </a></h3><p>Estimation of the expected value from stochastic updates relies on those updates being drawn from 
the same underlying distribution whose expectation you wish to estimate. Prioritized experience 
replay introduces a form of sampling bias that changes the underlying distribution (whose 
expectation needs to be estimated) in an uncontrolled fashion. When the underlying distribution 
changes, the solution to which the algorithm will converge also changes (even if the policy and 
state distribution are fixed). In order for the algorithm to converge properly, the bias 
introduced by the prioritized experience replay procedure needs to be corrected.</p>
<p>Schaul et al 2016 correct for this bias using an importance sampling scheme that computes a weight 
for each sampled experience that can be used when computing the loss for that sample.</p>
<p>
$$ w_i = \left(\frac{1}{N}\frac{1}{P(i)}\right)^\beta $$
</p>
<p>The hyperparameter $\beta \ge 0$ controls how strongly to correct for the bias: $\beta=0$ implies 
no correction; $\beta=1$ fully compensates for the bias. For stability reasons, since these 
importance sampling weights are included in the loss, they are be normalized by $\max_i\ w_i$.</p>
<h3 id="Implementation">Implementation<a class="anchor-link" href="#Implementation"> </a></h3><p>The <code>PrioritizedExperienceReplayBuffer</code> defined below is a substantial re-write of the <code>ExperienceReplayBuffer</code> 
that I used in my 
<a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html">previous</a> 
<a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/11/double-dqn.html">posts</a>.</p>
<p>The most important implementation detail is that instead of using a 
<a href="https://docs.python.org/3.8/library/collections.html#collections.deque">fixed-length, double-ended queue</a> 
as the underlying data structure for storing experiences, I am now using a NumPy 
<a href="https://docs.scipy.org/doc/numpy/user/basics.rec.html">structured array</a> to store 
priority-experience tuples. In addition to cleaning up a lot of the internal implementation details, 
using a structured array as an internal buffer led  which lead to significant performance improvements.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="n">_field_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;action&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reward&quot;</span><span class="p">,</span>
    <span class="s2">&quot;next_state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;done&quot;</span>
<span class="p">]</span>
<span class="n">Experience</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;Experience&quot;</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="n">_field_names</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PrioritizedExperienceReplayBuffer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Fixed-size buffer to store priority, Experience tuples.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize an ExperienceReplayBuffer object.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        buffer_size (int): maximum size of buffer</span>
<span class="sd">        batch_size (int): size of each training batch</span>
<span class="sd">        alpha (float): Strength of prioritized sampling. Default to 0.0 (i.e., uniform sampling).</span>
<span class="sd">        random_state (np.random.RandomState): random number generator.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span> <span class="o">=</span> <span class="n">buffer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># current number of prioritized experience tuples in buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;priority&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;experience&quot;</span><span class="p">,</span> <span class="n">Experience</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span> <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">random_state</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Current number of prioritized experience tuple stored in buffer.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">alpha</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Strength of prioritized sampling.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Number of experience samples per training batch.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">buffer_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Maximum number of prioritized experience tuples stored in buffer.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">:</span> <span class="n">Experience</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot;</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_empty</span><span class="p">()</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_full</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">priority</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">():</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">argmin</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">experience</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">pass</span> <span class="c1"># low priority experiences should not be included in buffer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">experience</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">is_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;True if the buffer is empty; False otherwise.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">==</span> <span class="mi">0</span>
    
    <span class="k">def</span> <span class="nf">is_full</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;True if the buffer is full; False otherwise.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_size</span>
    
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Sample a batch of experiences from memory.&quot;&quot;&quot;</span>
        <span class="c1"># use sampling scheme to determine which experiences to use for learning</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span><span class="p">][</span><span class="s2">&quot;priority&quot;</span><span class="p">]</span>
        <span class="n">sampling_probs</span> <span class="o">=</span> <span class="n">ps</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ps</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">ps</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                                         <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">,</span>
                                         <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">p</span><span class="o">=</span><span class="n">sampling_probs</span><span class="p">)</span>
        
        <span class="c1"># select the experiences and compute sampling weights</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;experience&quot;</span><span class="p">][</span><span class="n">idxs</span><span class="p">]</span>        
        <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_length</span> <span class="o">*</span> <span class="n">sampling_probs</span><span class="p">[</span><span class="n">idxs</span><span class="p">])</span><span class="o">**-</span><span class="n">beta</span>
        <span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">weights</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">idxs</span><span class="p">,</span> <span class="n">experiences</span><span class="p">,</span> <span class="n">normalized_weights</span>

    <span class="k">def</span> <span class="nf">update_priorities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">priorities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update the priorities associated with particular experiences.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer</span><span class="p">[</span><span class="s2">&quot;priority&quot;</span><span class="p">][</span><span class="n">idxs</span><span class="p">]</span> <span class="o">=</span> <span class="n">priorities</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Refactoring-the-DeepQAgent-class">Refactoring the <code>DeepQAgent</code> class<a class="anchor-link" href="#Refactoring-the-DeepQAgent-class"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>


<span class="n">A</span> <span class="o">=</span> <span class="n">typing</span><span class="o">.</span><span class="n">TypeVar</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="s1">&#39;Agent&#39;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">A</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load an Agent from a saved checkpoint.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
    
    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiences</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">Experience</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update the agent&#39;s state based on a collection of recent experiences.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
             <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
             <span class="n">next_state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
             <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update agent&#39;s state after observing the effect of its action on the environment.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="n">NotImplmentedError</span>


<span class="k">class</span> <span class="nc">DeepQAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">state_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">action_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">number_hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">optimizer_fn</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="n">typing</span><span class="o">.</span><span class="n">Iterable</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]],</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">beta_annealing_schedule</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
                 <span class="n">epsilon_decay_schedule</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
                 <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">update_frequency</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize a DeepQAgent.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        state_size (int): the size of the state space.</span>
<span class="sd">        action_size (int): the size of the action space.</span>
<span class="sd">        number_hidden_units (int): number of units in the hidden layers.</span>
<span class="sd">        optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer.</span>
<span class="sd">        batch_size (int): number of experience tuples in each mini-batch.</span>
<span class="sd">        buffer_size (int): maximum number of experience tuples stored in the replay buffer.</span>
<span class="sd">        alpha (float): Strength of prioritized sampling; alpha &gt;= 0.0.</span>
<span class="sd">        beta_annealing_schedule (callable): function that takes episode number and returns beta &gt;= 0.</span>
<span class="sd">        epsilon_decay_schdule (callable): function that takes episode number and returns 0 &lt;= epsilon &lt; 1.</span>
<span class="sd">        alpha (float): rate at which the target q-network parameters are updated.</span>
<span class="sd">        gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1).</span>
<span class="sd">        update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated.</span>
<span class="sd">        seed (int): random seed</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_action_size</span> <span class="o">=</span> <span class="n">action_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        
        <span class="c1"># set seeds for reproducibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span> <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># initialize agent hyperparameters</span>
        <span class="n">_replay_buffer_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha</span><span class="p">,</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="n">buffer_size</span><span class="p">,</span>
            <span class="s2">&quot;random_state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span> <span class="o">=</span> <span class="n">PrioritizedExperienceReplayBuffer</span><span class="p">(</span><span class="o">**</span><span class="n">_replay_buffer_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta_annealing_schedule</span> <span class="o">=</span> <span class="n">beta_annealing_schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_decay_schedule</span> <span class="o">=</span> <span class="n">epsilon_decay_schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        
        <span class="c1"># initialize Q-Networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span> <span class="o">=</span> <span class="n">update_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_q_network</span><span class="p">(</span><span class="n">number_hidden_units</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_q_network</span><span class="p">(</span><span class="n">number_hidden_units</span><span class="p">)</span>
        <span class="n">synchronize_q_networks</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">)</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        
        <span class="c1"># initialize the optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

        <span class="c1"># initialize some counters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span> <span class="nf">_initialize_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">number_hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Create a neural network for approximating the action-value function.&quot;&quot;&quot;</span>
        <span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">number_hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_action_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">q_network</span>
           
    <span class="k">def</span> <span class="nf">_uniform_random_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_action_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_greedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Choose an action that maximizes the action_values given the current state.&quot;&quot;&quot;</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">select_greedy_actions</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="p">(</span><span class="n">actions</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>  <span class="c1"># actions might reside on the GPU!</span>
                         <span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="k">def</span> <span class="nf">_epsilon_greedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_state</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uniform_random_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the action for given state as per current policy.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        state (np.array): current state of the environment.</span>
<span class="sd">        </span>
<span class="sd">        Return:</span>
<span class="sd">        --------</span>
<span class="sd">        action (int): an integer representing the chosen action.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># need to reshape state array and convert to tensor</span>
        <span class="n">state_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                             <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                             <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
            
        <span class="c1"># choose uniform at random if agent has insufficient experience</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_sufficient_experience</span><span class="p">():</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uniform_random_policy</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_decay_schedule</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon_greedy_policy</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">experiences</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">sampling_weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Update the agent&#39;s state based on a collection of recent experiences.&quot;&quot;&quot;</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">vs</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span> <span class="k">for</span> <span class="n">vs</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">experiences</span><span class="p">))</span>
        
        <span class="c1"># need to add second dimension to some tensors</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">(</span><span class="n">actions</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
                          <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">dones</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">deltas</span> <span class="o">=</span> <span class="n">double_q_learning_error</span><span class="p">(</span><span class="n">states</span><span class="p">,</span>
                                         <span class="n">actions</span><span class="p">,</span>
                                         <span class="n">rewards</span><span class="p">,</span>
                                         <span class="n">next_states</span><span class="p">,</span>
                                         <span class="n">dones</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="p">)</span>
        
        <span class="c1"># update experience priorities</span>
        <span class="n">priorities</span> <span class="o">=</span> <span class="p">(</span><span class="n">deltas</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                            <span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">update_priorities</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">priorities</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="c1"># priorities must be positive!</span>
        
        <span class="c1"># compute the mean squared loss</span>
        <span class="n">_sampling_weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sampling_weights</span><span class="p">)</span>
                                  <span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">deltas</span> <span class="o">*</span> <span class="n">_sampling_weights</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># updates the parameters of the online network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">synchronize_q_networks</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_target_q_network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">has_sufficient_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;True if agent has enough experience to train on a batch of samples; False otherwise.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">batch_size</span>
    
    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves the state of the DeepQAgent.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        filepath (str): filepath where the serialized state should be saved.</span>
<span class="sd">        </span>
<span class="sd">        Notes:</span>
<span class="sd">        ------</span>
<span class="sd">        The method uses `torch.save` to serialize the state of the q-network, </span>
<span class="sd">        the optimizer, as well as the dictionary of agent hyperparameters.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;q-network-state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_online_q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;optimizer-state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;agent-hyperparameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
                <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">buffer_size</span><span class="p">,</span>
                <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">,</span>
                <span class="s2">&quot;update_frequency&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
             <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
             <span class="n">next_state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
             <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the agent&#39;s state based on feedback received from the environment.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        state (np.array): the previous state of the environment.</span>
<span class="sd">        action (int): the action taken by the agent in the previous state.</span>
<span class="sd">        reward (float): the reward received from the environment.</span>
<span class="sd">        next_state (np.array): the resulting state of the environment following the action.</span>
<span class="sd">        done (bool): True is the training episode is finised; false otherwise.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">experience</span> <span class="o">=</span> <span class="n">Experience</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span> 
            
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># every so often the agent should learn from experiences</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_timesteps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_frequency</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_sufficient_experience</span><span class="p">():</span>
                <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_annealing_schedule</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_episodes</span><span class="p">)</span>
                <span class="n">idxs</span><span class="p">,</span> <span class="n">experiences</span><span class="p">,</span> <span class="n">sampling_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">experiences</span><span class="p">,</span> <span class="n">sampling_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Training-Loop">The Training Loop<a class="anchor-link" href="#The-Training-Loop"> </a></h2><p>The code for the training loop remains unchanged from previous posts.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">import</span> <span class="nn">gym</span>


<span class="k">def</span> <span class="nf">_train_for_at_most</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">max_timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_timesteps</span><span class="p">):</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">score</span>

                
<span class="k">def</span> <span class="nf">_train_until_done</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="k">return</span> <span class="n">score</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">,</span>
          <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span>
          <span class="n">checkpoint_filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
          <span class="n">target_score</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">number_episodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">maximum_timesteps</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reinforcement learning training loop.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    agent (Agent): an agent to train.</span>
<span class="sd">    env (gym.Env): an environment in which to train the agent.</span>
<span class="sd">    checkpoint_filepath (str): filepath used to save the state of the trained agent.</span>
<span class="sd">    number_episodes (int): maximum number of training episodes.</span>
<span class="sd">    maximum_timesteps (int): maximum number of timesteps per episode.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    scores (list): collection of episode scores from training.</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">most_recent_scores</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_episodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">maximum_timesteps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">_train_until_done</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">_train_for_at_most</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">maximum_timesteps</span><span class="p">)</span>         
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">most_recent_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
        <span class="n">average_score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">most_recent_scores</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">most_recent_scores</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">average_score</span> <span class="o">&gt;=</span> <span class="n">target_score</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Environment solved in </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">d</span><span class="si">}</span><span class="s2"> episodes!</span><span class="se">\t</span><span class="s2">Average Score: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint_filepath</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">Episode </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="se">\t</span><span class="s2">Average Score: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scores</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Solving-the-LunarLander-v2-environment">Solving the <code>LunarLander-v2</code> environment<a class="anchor-link" href="#Solving-the-LunarLander-v2-environment"> </a></h2><p>In the rest of this blog post I will use the Double DQN algorithm with prioritized experience replay to train an agent to solve the <a href="https://gym.openai.com/envs/LunarLander-v2/">LunarLander-v2</a> environment from <a href="https://openai.com/">OpenAI</a>.</p>
<p>In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land "gracefully" (and not crash in the landing area!).</p>
<p>A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered "solved" if the lander is able to achieve 200 points (I will actually be more stringent and define "solved" as achieving over 200 points on average in the most recent 100 training episodes).</p>
<h3 id="Action-Space">Action Space<a class="anchor-link" href="#Action-Space"> </a></h3><p>There are four discrete actions available:</p>
<ol>
<li>Do nothing.</li>
<li>Fire the left orientation engine.</li>
<li>Fire main engine.</li>
<li>Fire the right orientation engine.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Colab-specific-environment-setup">Colab specific environment setup<a class="anchor-link" href="#Colab-specific-environment-setup"> </a></h3><p>If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install gym<span class="o">[</span>box2d<span class="o">]==</span><span class="m">0</span>.17.*
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: <span class="ansi-yellow-fg">WARN: Box bound precision lowered by casting to float32</span>
  warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;))
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Creating-a-DeepQAgent">Creating a <code>DeepQAgent</code><a class="anchor-link" href="#Creating-a-DeepQAgent"> </a></h3><p>Before creating an instance of the <code>DeepQAgent</code> with prioritized experience replay I need to define a $\beta$-annealing schedule, an $\epsilon$-decay schedule and choose an optimizer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="$\beta$-annealing-schedule">$\beta$-annealing schedule<a class="anchor-link" href="#$\beta$-annealing-schedule"> </a></h4><p>Due to the inherent non-stationarity of the RL training process, Schaul et al 2016 hypothesize 
that a small sampling bias can be ignored during early training episodes. Instead of fixing 
$\beta=1$ (and fully correcting for the bias throughout training) they increase the amount of 
importance sampling correction as the number of training episodes increase by defining a schedule 
for $\beta$ that reaches 1 (i.e., full bias correction) only near the end of training.</p>
<p>Note that the choice of $\beta$ interacts with choice of prioritization exponent $\alpha$: 
increasing both simultaneously prioritizes sampling more aggressively while at the same time as 
correcting for it more strongly.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">constant_annealing_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">constant</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">constant</span>


<span class="k">def</span> <span class="nf">exponential_annealing_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rate</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
<span class="n">rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">exponential_annealing_schedule</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">rate</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s2">&quot;horizontal&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of Episodes&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="$\epsilon$-decay-schedule">$\epsilon$-decay schedule<a class="anchor-link" href="#$\epsilon$-decay-schedule"> </a></h4><p>As was the case with the DQN and Double DQN algorithms, the agent chooses its action using an 
$\epsilon$-greedy policy. When using an $\epsilon$-greedy policy, with probability $\epsilon$, the 
agent explores the state space by choosing an action uniformly at random from the set of feasible 
actions; with probability $1-\epsilon$, the agent exploits its current knowledge by choosing the 
optimal action given that current state.</p>
<p>As the agent learns and acquires additional knowledge about it environment it makes sense to 
<em>decrease</em> exploration and <em>increase</em> exploitation by decreasing $\epsilon$. In practice, it isn't 
a good idea to decrease $\epsilon$ to zero; instead one typically decreases $\epsilon$ over time 
according to some schedule until it reaches some minimum value.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">power_decay_schedule</span><span class="p">(</span><span class="n">episode_number</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">decay_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                         <span class="n">minimum_epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">decay_factor</span><span class="o">**</span><span class="n">episode_number</span><span class="p">,</span> <span class="n">minimum_epsilon</span><span class="p">)</span>

<span class="n">_epsilon_decay_schedule_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;decay_factor&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;minimum_epsilon&quot;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">epsilon_decay_schedule</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">power_decay_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="o">**</span><span class="n">_epsilon_decay_schedule_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Choosing-an-optimizer">Choosing an optimizer<a class="anchor-link" href="#Choosing-an-optimizer"> </a></h4><p>Given the good results I achieved in my previous post from using the <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam</a> optimizer I decided to continue to use that optimizer here.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>


<span class="n">_optimizer_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="s2">&quot;betas&quot;</span><span class="p">:(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="mf">1e-08</span><span class="p">,</span>
    <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;amsgrad&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">optimizer_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">_optimizer_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-the-DeepQAgent">Training the <code>DeepQAgent</code><a class="anchor-link" href="#Training-the-DeepQAgent"> </a></h3><p>Now I am finally ready to train the <code>deep_q_agent</code>. The target score for the <code>LunarLander-v2</code> environment is 200 points on average for at least 100 consecutive episodes. If the <code>deep_q_agent</code> is able to "solve" the environment, then training will terminate early.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Uniform-random-sampling-baseline">Uniform random sampling baseline<a class="anchor-link" href="#Uniform-random-sampling-baseline"> </a></h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_agent_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;state_size&quot;</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;action_size&quot;</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> 
    <span class="s2">&quot;number_hidden_units&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;optimizer_fn&quot;</span><span class="p">:</span> <span class="n">optimizer_fn</span><span class="p">,</span>
    <span class="s2">&quot;epsilon_decay_schedule&quot;</span><span class="p">:</span> <span class="n">epsilon_decay_schedule</span><span class="p">,</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>
    <span class="s2">&quot;beta_annealing_schedule&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">constant_annealing_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
    <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;update_frequency&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">double_dqn_agent</span> <span class="o">=</span> <span class="n">DeepQAgent</span><span class="p">(</span><span class="o">**</span><span class="n">_agent_kwargs</span><span class="p">)</span>

<span class="n">uniform_sampling_scores</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">double_dqn_agent</span><span class="p">,</span>
                                <span class="n">env</span><span class="p">,</span>
                                <span class="s2">&quot;uniform-sampling-checkpoint.pth&quot;</span><span class="p">,</span>
                                <span class="n">number_episodes</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                                <span class="n">target_score</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Episode 100	Average Score: -113.36
Episode 200	Average Score: 51.15
Episode 300	Average Score: 119.37
Episode 400	Average Score: 158.45
Episode 500	Average Score: 154.08
Episode 600	Average Score: 216.22
Episode 700	Average Score: 220.47
Episode 800	Average Score: 229.12
Episode 900	Average Score: 223.13
Episode 1000	Average Score: 230.66
Episode 1100	Average Score: 229.96
Episode 1200	Average Score: 209.60
Episode 1300	Average Score: 190.09
Episode 1400	Average Score: 212.46
Episode 1500	Average Score: 219.84
Episode 1600	Average Score: 226.77
Episode 1700	Average Score: 231.39
Episode 1800	Average Score: 208.05
Episode 1900	Average Score: 183.24
Episode 2000	Average Score: 194.43
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Prioritized-sampling">Prioritized sampling<a class="anchor-link" href="#Prioritized-sampling"> </a></h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_agent_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;state_size&quot;</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;action_size&quot;</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> 
    <span class="s2">&quot;number_hidden_units&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;optimizer_fn&quot;</span><span class="p">:</span> <span class="n">optimizer_fn</span><span class="p">,</span>
    <span class="s2">&quot;epsilon_decay_schedule&quot;</span><span class="p">:</span> <span class="n">epsilon_decay_schedule</span><span class="p">,</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>
    <span class="s2">&quot;beta_annealing_schedule&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">exponential_annealing_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">),</span>
    <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;update_frequency&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">double_dqn_agent</span> <span class="o">=</span> <span class="n">DeepQAgent</span><span class="p">(</span><span class="o">**</span><span class="n">_agent_kwargs</span><span class="p">)</span>

<span class="n">prioritized_sampling_scores</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">double_dqn_agent</span><span class="p">,</span>
                                    <span class="n">env</span><span class="p">,</span>
                                    <span class="s2">&quot;prioritized-sampling-checkpoint.pth&quot;</span><span class="p">,</span>
                                    <span class="n">number_episodes</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                                    <span class="n">target_score</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Episode 100	Average Score: -130.92
Episode 200	Average Score: 15.15
Episode 300	Average Score: 71.90
Episode 400	Average Score: 191.28
Episode 500	Average Score: 226.89
Episode 600	Average Score: 238.60
Episode 700	Average Score: 194.70
Episode 800	Average Score: 227.00
Episode 900	Average Score: 223.78
Episode 1000	Average Score: 228.35
Episode 1100	Average Score: 220.97
Episode 1200	Average Score: 220.92
Episode 1300	Average Score: 241.30
Episode 1400	Average Score: 209.41
Episode 1500	Average Score: 200.94
Episode 1600	Average Score: 202.66
Episode 1700	Average Score: 235.63
Episode 1800	Average Score: 249.41
Episode 1900	Average Score: 237.83
Episode 2000	Average Score: 259.26
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Plotting-the-time-series-of-scores">Plotting the time series of scores<a class="anchor-link" href="#Plotting-the-time-series-of-scores"> </a></h4><p>I can use <a href="https://pandas.pydata.org/">Pandas</a> to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">uniform_sampling_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">uniform_sampling_scores</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scores&quot;</span><span class="p">)</span>
<span class="n">prioritized_sampling_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">prioritized_sampling_scores</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scores&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">uniform_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Uniform Sampling&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="p">(</span><span class="n">uniform_sampling_scores</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
               <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
               <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;Rolling Average&quot;</span><span class="p">)</span>
               <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">prioritized_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Double DQN Scores&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="p">(</span><span class="n">prioritized_sampling_scores</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
                      <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                      <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;Rolling Average&quot;</span><span class="p">)</span>
                      <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Episode Number&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Kernel-density-plot-of-the-scores">Kernel density plot of the scores<a class="anchor-link" href="#Kernel-density-plot-of-the-scores"> </a></h4><p>In general, the kernel density plot will be bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent "solved" the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score).</p>
<p>Depending, you may see that the distribution of scores for Double DQN has a significantly higher positive mode and lower negative mode when compared to the distribution for DQN which indicates that the agent trained with Double DQN solved the task <em>more</em> frequently and crashed and burned <em>less</em> frequently than the agent trained with DQN.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">uniform_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Uniform Sampling&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">prioritized_sampling_scores</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Priority Sampling&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Where-to-go-from-here?">Where to go from here?<a class="anchor-link" href="#Where-to-go-from-here?"> </a></h2><p>Up next in this series will be <a href="https://arxiv.org/abs/1511.06581"><em>Dueling Network Architectures for Deep Reinforcement Learning</em></a>.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="davidrpugh/stochastic-expatriate-descent"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/stochastic-expatriate-descent/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/stochastic-expatriate-descent/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An expat scientist&#39;s musings about machine learning, deep learning, and living abroad.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/davidrpugh" title="davidrpugh"><svg class="svg-icon grey"><use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/TheSandyCoder" title="TheSandyCoder"><svg class="svg-icon grey"><use xlink:href="/stochastic-expatriate-descent/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
