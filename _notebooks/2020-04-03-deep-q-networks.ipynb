{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "> Notes on the famous Deep Q-Networks paper from Deepmind.\n",
    "\n",
    "- toc: true\n",
    "- branch: 2020-04-03-deep-q-networks\n",
    "- badges: true\n",
    "- image: images/q-network-architecture.jpg\n",
    "- comments: true\n",
    "- author: David R. Pugh\n",
    "- categories: [pytorch, deep-reinforcement-learning, deep-q-networks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/pughdr/Research/stochastic-expatriate-descent/env\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch=1.4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.11.28         |           py37_1         156 KB\n",
      "    intel-openmp-2020.0        |              166         896 KB\n",
      "    mkl-2020.0                 |              166        93.5 MB\n",
      "    ninja-1.9.0                |   py37h04f5b5a_0          90 KB\n",
      "    openssl-1.1.1f             |       h1de35cc_0         2.2 MB\n",
      "    pytorch-1.4.0              |          py3.7_0        34.5 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       131.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  intel-openmp       pkgs/main/osx-64::intel-openmp-2020.0-166\n",
      "  mkl                pkgs/main/osx-64::mkl-2020.0-166\n",
      "  ninja              pkgs/main/osx-64::ninja-1.9.0-py37h04f5b5a_0\n",
      "  pytorch            pytorch/osx-64::pytorch-1.4.0-py3.7_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2019.11.~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
      "  openssl            conda-forge::openssl-1.1.1e-h0b31af3_0 --> pkgs/main::openssl-1.1.1f-h1de35cc_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge::certifi-2019.11.28-py37h~ --> pkgs/main::certifi-2019.11.28-py37_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2019.11.28   | 156 KB    | ##################################### | 100% \n",
      "openssl-1.1.1f       | 2.2 MB    | ##################################### | 100% \n",
      "intel-openmp-2020.0  | 896 KB    | ##################################### | 100% \n",
      "pytorch-1.4.0        | 34.5 MB   | ##################################### | 100% \n",
      "mkl-2020.0           | 93.5 MB   | ##################################### | 100% \n",
      "ninja-1.9.0          | 90 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --channel pytorch pytorch=1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use deep convolutional neural network to approximate that optimal action-value function\n",
    "\n",
    "$$ Q^*(s, a) = \\max_{\\pi} \\mathbb{E}\\Bigg[\\sum_{s=0}^{\\infty} \\gamma^s r_{t+s} | s_t=s, a_t=a, \\pi \\Bigg] $$\n",
    "\n",
    "which is the maximum sum of rewards $r_t$ discounted by $\\gamma$ at each time-step $t$ achievable by a behaviour policy $\\pi = P(a|s)$, after making an observation of the state $s$ and taking an action $a$. \n",
    "\n",
    "\n",
    "Reinforcement learning is known to be unstable or even to diverge when a non-linear function approximator such as a neural network is used to represent the $Q$ function. Why?\n",
    "\n",
    "1. Correlations present in the sequence of observations of the state $s$. In reinforcement learning applications the sequence state observations is a time-series which will almost surely be auto-correlated. But surely this would also be true of any application of deep neural networks to model time series data. \n",
    "2. Small updates to $Q$ may significantly change the policy, $\\pi$ and therefore change the data distribution.\n",
    "3. Correlations between the action-values, $Q$, and the target values $r + \\gamma \\max_{a'} Q(s', a')$\n",
    "\n",
    "In the paper they address these issue by...\n",
    "\n",
    "* A biologically inspired mechanism they refer to as *experience replay* that randomizes over the data which removes correlations in the sequence of observations of the state $s$ and smoothes over changes in the data distribution (issues 1 and 2 above).\n",
    "* Using an iterative update rule that adjusts the action-values, $Q$, towards target values that are only periodically updated, thereby reducing correlations with the target (issue 3 above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the state-action function, $Q(s,a)$\n",
    "\n",
    "They parameterize an approximate value function $Q(s,a; \\theta_i)$ using the deep convolutional neural network shown in the figure below. Note that $\\theta_i$ are the parameters or weights of the $Q$-network at iteration $i$.\n",
    "\n",
    "![](images/q-network-architecture.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_q_network(state_size: int, action_size: int) -> nn.Module:\n",
    "    deep_q_network = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=state_size, out_channels=32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64, out_features=512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=512, out_features=action_size)\n",
    "    )\n",
    "    return deep_q_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "To perform *experience replay* the authors store the agent's experiences $e_t$ as represented by the tuple\n",
    "\n",
    "$$ e_t = (s_t, a_t, r_t, s_{t+1}) $$\n",
    "\n",
    "consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences.\n",
    "\n",
    "$$ D_t = \\{e1, e2, ..., e_t \\} $$\n",
    "\n",
    "Depending on the task it may note be feasible for the agent to store the entire history of past experiences.\n",
    "\n",
    "During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s')$, drawn uniformly at random from the pool of stored samples $D_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "The $Q$-learning update at iteration $i$ uses the following loss function\n",
    "\n",
    "$$ \\mathcal{L_i}(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Bigg[\\bigg(r + \\gamma \\max_{a'} Q\\big(s', a'; \\theta_i^{-}\\big) - Q\\big(s, a; \\theta_i\\big)\\bigg)^2\\Bigg] $$\n",
    "\n",
    "where $\\gamma$ is the discount factor determining the agentâ€™s horizon, $\\theta_i$ are the parameters of the $Q$-network at iteration $i$ and $\\theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $\\theta_i^{-}$ are only updated with the Q$-network parameters $\\theta_i$ every $C$ steps and are held fixed between individual updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f483aa612782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtarget_q_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_deep_q_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_q_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2201\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \"\"\"\n\u001b[0;32m-> 2203\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m         warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n\u001b[1;32m   2205\u001b[0m                       \u001b[0;34m\"This will likely lead to incorrect results due to broadcasting. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "expected_q_network = create_deep_q_network(state_size=4, action_size=4)\n",
    "target_q_network = create_deep_q_network(state_size=4, action_size=4)\n",
    "\n",
    "expected_q_values = ???\n",
    "target_q_values = ???\n",
    "\n",
    "loss_fn = F.mse_loss(expected_q_values, target_q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
