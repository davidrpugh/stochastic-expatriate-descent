{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black box optimization for RL\n",
    "> Notes on various approaches to black box optimization used in RL\n",
    "\n",
    "- branch: 2020-05-12-black-box-optimization\n",
    "- badges: true\n",
    "- image: images/???\n",
    "- comments: true\n",
    "- author: David R. Pugh\n",
    "- categories: [pytorch, deep-reinforcement-learning, optimization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Preamble\n",
    "\n",
    "If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# install required system dependencies\n",
    "apt-get install -y xvfb x11-utils\n",
    "\n",
    "# install required python dependencies (might need to install additional gym extras depending)\n",
    "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set `visible=False`.\n",
    "\n",
    "**This code only needs to be run once per session to start the display.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvirtualdisplay\n",
    "\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                    size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binder Preamble\n",
    "\n",
    "If you are running this code on Binder, then there isn't really much to do as all the software is pre-installed. However you do still need to run the code in the cell below to creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set `visible=False`.\n",
    "\n",
    "*This code only needs to be run once per session to start the display.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvirtualdisplay\n",
    "\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                    size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-box optimization\n",
    "\n",
    "Black-box optimization is great for proto-typing, or for situations when the objective function is not differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base RL Agent\n",
    "\n",
    "The code in the cell below defines a base `Agent` class designed to interact with [OpenAI Gym](https://gym.openai.com/) environments. When creating an `Agent` you must specify a `PolicyFn`. A `PolicyFn` is a no-argument function that returns a `Policy`. A `Policy` is a function that takes the environment state is its input and then returns the probabilities of choosing feasible actions. Here I use neural networks implemented in [PyTorch](https://pytorch.org/) as [`torch.nn.Module`](https://pytorch.org/docs/master/generated/torch.nn.Module.html) to represent a `Policy`. \n",
    "\n",
    "can also specify whether the agent should use a deterministic or stochastic policy for choosing actions. With a stochastic policy (which is the default), the agent samples from the probability distribution returned by its current approximate policy function when choosing its action given the state of the environment. With a deterministic policy, meanwhile, the agent always chooses the action with the largest probability mass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions, nn\n",
    "\n",
    "\n",
    "GymState = np.ndarray\n",
    "Policy = nn.Module\n",
    "PolicyFn = typing.Callable[[], Policy]\n",
    "PreprocessingFn = typing.Callable[[GymState], torch.Tensor] \n",
    "Action = int\n",
    "\n",
    "\n",
    "class GymAgent:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 policy_fn: PolicyFn,\n",
    "                 gamma: float = 1.0,\n",
    "                 is_deterministic: bool = False,\n",
    "                 preprocessing_fn: typing.Optional[PreprocessingFn] = None,\n",
    "                 seed: typing.Optional[int] = None):\n",
    "        if torch.cuda.is_available():\n",
    "            self._device = torch.device(\"cuda\") \n",
    "        else:\n",
    "            self._device = torch.device(\"cpu\")\n",
    "        self._gamma = gamma\n",
    "        self._is_deterministic = is_deterministic\n",
    "        self._policy = policy_fn()\n",
    "        _ = self._policy.to(self._device)\n",
    "        \n",
    "        # converts np.ndarray with shape (n_states,) to torch.Tensor with shape (1, n_states)\n",
    "        if preprocessing_fn is None:\n",
    "            self._preprocessing_fn = lambda state: torch.Tensor(state).unsqueeze(dim=0) \n",
    "        else:\n",
    "            self._preprocessing_fn = preprocessing_fn\n",
    "            \n",
    "        if seed is None:\n",
    "            self._random_state = np.random.RandomState()\n",
    "        else:\n",
    "            self._random_state = np.random.RandomState(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            \n",
    "            # if using CUDA need to set cuDNN flags\n",
    "            #if torch.cuda.is_available():\n",
    "            #    torch.backends.cudnn.deterministic = True\n",
    "            #    torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "    def __call__(self, state: GymState) -> Action:\n",
    "        \"\"\"Apply the current policy function to the state and returns the action.\"\"\"\n",
    "        state_tensor = (self._preprocessing_fn(state)\n",
    "                            .to(self._device))\n",
    "        sampling_probs = (self._policy(state_tensor)\n",
    "                              .cpu())\n",
    "        if self._is_deterministic:\n",
    "            action = (sampling_probs.argmax()\n",
    "                                    .item())\n",
    "        else:\n",
    "            sampling_dist = distributions.Categorical(sampling_probs)\n",
    "            action = (sampling_dist.sample()\n",
    "                                   .item())\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, rewards) -> None:\n",
    "        \"\"\"Evaluate rewards generated from following the current policy while interacting with an environment.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def load_state_dict(self, state_dict: typing.Dict[str, torch.Tensor]) -> None:\n",
    "        \"\"\"Update the parameters of the policy function using the state_dict.\"\"\"\n",
    "        copied_state_dict = copy.deepcopy(state_dict) # avoid sharing mutable state_dict!\n",
    "        _ = self._policy.load_state_dict(copied_state_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "Score = int\n",
    "\n",
    "\n",
    "def _train_for_at_most(agent: GymAgent, env: gym.Env, max_timesteps: int) -> Score:\n",
    "    \"\"\"Train the Agent for a maximum number of timesteps.\"\"\"\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    for t in range(max_timesteps):\n",
    "        action = agent(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return sum(rewards), rewards\n",
    "\n",
    "                \n",
    "def _train_until_done(agent: GymAgent, env: gym.Env) -> Score:\n",
    "    \"\"\"Train the Agent until the current episode is complete.\"\"\"\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        rewards.append(reward)\n",
    "    return sum(rewards), rewards\n",
    "\n",
    "\n",
    "def train(agent: GymAgent,\n",
    "          env: gym.Env,\n",
    "          target_score: Score,\n",
    "          maximum_episodes: int,\n",
    "          maximum_timesteps: typing.Optional[int] = None,\n",
    "          verbose: bool = False) -> typing.List[Score]:\n",
    "    \"\"\"Train and Agent to solve a given environment.\"\"\"\n",
    "    scores = []\n",
    "    most_recent_scores = collections.deque(maxlen=100)\n",
    "    for i in range(maximum_episodes):\n",
    "        if maximum_timesteps is None:\n",
    "            score, rewards = _train_until_done(agent, env)\n",
    "        else:\n",
    "            score, rewards = _train_for_at_most(agent, env, maximum_timesteps)         \n",
    "        scores.append(score)\n",
    "        most_recent_scores.append(score)\n",
    "        agent.evaluate(rewards)\n",
    "        \n",
    "        average_score = sum(most_recent_scores) / len(most_recent_scores)\n",
    "        if len(most_recent_scores) >=100 and average_score >= target_score:\n",
    "            if verbose:\n",
    "                print(f\"\\nEnvironment solved in {i:d} episodes!\\tAverage Score: {average_score:.2f}\")\n",
    "            break\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(f\"\\rEpisode {i + 1}\\tAverage Score: {average_score:.2f}\")\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "def simulate(agent: GymAgent, env: gym.Env, ax: plt.Axes) -> None:\n",
    "    state = env.reset()\n",
    "    img = ax.imshow(env.render(mode='rgb_array'))\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)       \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole environment\n",
    "\n",
    "The [CartPole](https://gym.openai.com/envs/CartPole-v0/) is a classic RL problem. A pole is attached by an un-actuated joint to a cart which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pole starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "In the cell below I define a function that takes the number of actions and number of states as inputs and returns a `PolicyFn`. In this case the defined policy amounts to performing logistic regression. The policy takes a state vector as its input and predicts the probabilities of moving left or right. Note that I do not include a bias term: if the state vector is the zero vector, then our policy should be to neither move left nor right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "## Hill-climbing\n",
    "\n",
    "A [Hill-climbing algorithm](https://en.wikipedia.org/wiki/Hill_climbing) is a mathematical optimization technique. It is an iterative algorithm that starts with an arbitrary (typically random) solution to a problem, then attempts to find a better solution by making an incremental change to the existing solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found.\n",
    "\n",
    "The code in the cell below defines a `HillClimber` agent that uses a neural network to parameterize an approximate policy function which return the probability of choosing each of a discrete number of actions given the current state of its environment. The agent will keep track of the \"best\" policy function parameters as measured by the total (discounted) rewards achieved from following that policy while intereacting with its environment. During an iteration of the training algorithm the agent randomly updates the current \"best\" parameters and then uses the resulting policy function to interact with its environment and generate more rewards. If the performance of the agent improves, then the agent updates its \"best\" parameters accordingly; if the agent's performance degrades, then the agent resets the current parameters to their previous \"best\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class HillClimber(GymAgent):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 policy_fn: PolicyFn,\n",
    "                 gamma: float = 1.0,\n",
    "                 is_deterministic: bool = False,\n",
    "                 preprocessing_fn: typing.Optional[PreprocessingFn] = None,\n",
    "                 seed: typing.Optional[int] = None,\n",
    "                 sigma: float = 1.0):\n",
    "        super().__init__(policy_fn, gamma, is_deterministic, preprocessing_fn, seed)\n",
    "        self._max_discounted_reward = -float(\"inf\")\n",
    "        copied_state_dict = copy.deepcopy(self._policy.state_dict()) # avoid storing mutable state!\n",
    "        self._best_policy_state_dict = copied_state_dict \n",
    "        self._sigma = sigma\n",
    "        \n",
    "    def __ge__(self, other):\n",
    "        return not self.__lt__(other)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self._max_discounted_reward < other._max_discounted_reward\n",
    "\n",
    "    @property\n",
    "    def best_policy_state_dict(self):\n",
    "        \"\"\"The state_dict parameterizing the best policy function found thus far.\"\"\"\n",
    "        return self._best_policy_state_dict\n",
    "    \n",
    "    @property\n",
    "    def score(self):\n",
    "        \"\"\"The maximum discounted reward achieved thus far.\"\"\"\n",
    "        return self._max_discounted_reward\n",
    "    \n",
    "    def evaluate(self, rewards):\n",
    "        \"\"\"Evaluate rewards generated from following the current policy when interacting with an environment.\"\"\"\n",
    "        discounted_reward = sum(self._gamma**i * reward for i, reward in enumerate(rewards))\n",
    "        if discounted_reward >= self._max_discounted_reward:\n",
    "            self._max_discounted_reward = discounted_reward\n",
    "            copied_state_dict = copy.deepcopy(self._policy.state_dict()) # avoid storing mutable state!\n",
    "            self._best_policy_state_dict = copied_state_dict           \n",
    "        else:\n",
    "            _ = self.load_state_dict(self._best_policy_state_dict)\n",
    "        with torch.no_grad():\n",
    "            for parameter in self._policy.parameters():\n",
    "                parameter.add_(torch.randn_like(parameter), alpha=self._sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of an untrained `HillClimber`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "simulate(hillclimber, env, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of a trained `Hillclimber`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_policy_fn(number_actions, number_states) -> PolicyFn:\n",
    "    \n",
    "    def policy_fn() -> Policy:\n",
    "        policy = nn.Sequential(\n",
    "            nn.Linear(number_states, number_actions, bias=False),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return policy\n",
    "    \n",
    "    return policy_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_ACTIONS = env.action_space.n\n",
    "NUMBER_STATES, = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 151.17\n",
      "Episode 200\tAverage Score: 145.24\n",
      "Episode 300\tAverage Score: 165.94\n",
      "Episode 400\tAverage Score: 169.29\n",
      "Episode 500\tAverage Score: 180.09\n",
      "\n",
      "Environment solved in 526 episodes!\tAverage Score: 195.39\n",
      "CPU times: user 10.1 s, sys: 30 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_seed = 42\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(_seed)\n",
    "\n",
    "_policy_fn = make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES)\n",
    "hillclimber = HillClimber(policy_fn=_policy_fn, seed=_seed)\n",
    "scores = train(hillclimber, env, target_score=195.0, maximum_episodes=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Hillclimber` requires almost 700 episodes of training to solve the environment. Seems slow but perhaps this training run is not representative of the typical performance of the algorithm. The following code simulates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 68.9 ms, total: 1.23 s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "hillclimber_kwargs = {\n",
    "    \"policy_fn\": make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES),\n",
    "    \"gamma\": 1.0,\n",
    "    \"is_deterministic\": False,\n",
    "    \"preprocessing_fn\": None,\n",
    "    \"seed\": None,\n",
    "    \"sigma\": 1.0\n",
    "}\n",
    "\n",
    "\n",
    "def simulate_hillclimber():\n",
    "    hillclimber = HillClimber(**hillclimber_kwargs)\n",
    "    scores = train(hillclimber, env, target_score=195.0, maximum_episodes=1000)\n",
    "    return scores\n",
    "\n",
    "\n",
    "n_jobs = multiprocessing.cpu_count()\n",
    "n_hillclimbers = 100\n",
    "hillclimber_results = joblib.Parallel(n_jobs)(joblib.delayed(simulate_hillclimber)() for _ in range(n_hillclimbers))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "hillclimber_required_episodes = [len(scores) for scores in hillclimber_results]\n",
    "_ = ax.hist(hillclimber_required_episodes)\n",
    "_ = ax.set(xlabel=\"Number of episodes\",\n",
    "           ylabel=\"Number of training runs\",\n",
    "           title=\"Number of episodes required to solve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving hill-climing using adaptive noise scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above histogram, a basic hill-climbing algorithm is not terribly robust: basic hill-climbing often takes a long time to solve the environment and sometimes outright fails to find a solution even after 1000 episodes.\n",
    "\n",
    "Many improvements to the basic hill-climbing algorithm have been developed over the years. One simple improvement called adaptive noise scaling rescales the noise in the parameter updates depending on the performance of the `Hillclimber` agent. If the `HillClimber` agent's previous random parameter update improved its performance, then the amount of noise used in the next parameter update is decreased (leading to less local exploration of the objective function); if the `HillClimber` agent's performance degraded following the previous random update, then the amount of noise used in the next parameter update is increased (leading to more local exporation of the objective function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class AdaptiveHillClimber(HillClimber):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 policy_fn: PolicyFn,\n",
    "                 gamma: float = 1.0,\n",
    "                 is_deterministic: bool = False,\n",
    "                 preprocessing_fn: typing.Optional[PreprocessingFn] = None,\n",
    "                 seed: typing.Optional[int] = None,\n",
    "                 sigma: float = 1.0):\n",
    "        super().__init__(policy_fn, gamma, is_deterministic, preprocessing_fn, seed, sigma)\n",
    "        self._best_policy_state_dict = self._policy.state_dict()\n",
    "    \n",
    "    def evaluate(self, rewards):\n",
    "        \"\"\"Evaluate rewards generated from following the current policy when interacting with an environment.\"\"\"\n",
    "        discounted_reward = sum(self._gamma**i * reward for i, reward in enumerate(rewards))\n",
    "        if discounted_reward >= self._max_discounted_reward:\n",
    "            self._max_discounted_reward = discounted_reward\n",
    "            copied_state_dict = copy.deepcopy(self._policy.state_dict()) # avoid storing mutable state!\n",
    "            self._best_policy_state_dict = copied_state_dict\n",
    "            self._sigma = max(2**-10, self._sigma / 2)\n",
    "        else:\n",
    "            self._sigma = min(2, self._sigma * 2)\n",
    "            _ = self.load_state_dict(self._best_policy_state_dict)\n",
    "        with torch.no_grad():\n",
    "            for parameter in self._policy.parameters():\n",
    "                parameter.add_(torch.randn_like(parameter), alpha=self._sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 53.39\n",
      "Episode 200\tAverage Score: 161.00\n",
      "\n",
      "Environment solved in 233 episodes!\tAverage Score: 195.31\n",
      "CPU times: user 3.32 s, sys: 10.7 ms, total: 3.33 s\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_seed = None\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(_seed)\n",
    "\n",
    "hillclimber = AdaptiveHillClimber(policy_fn=_policy_fn, seed=_seed)\n",
    "scores = train(hillclimber, env, target_score=195.0, maximum_episodes=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the impact of adaptive noise scaling, we can re-run the simulation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.25 s, sys: 15.9 ms, total: 1.27 s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "hillclimber_kwargs = {\n",
    "    \"policy_fn\": make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES),\n",
    "    \"gamma\": 1.0,\n",
    "    \"is_deterministic\": False,\n",
    "    \"preprocessing_fn\": None,\n",
    "    \"seed\": None,\n",
    "    \"sigma\": 1.0\n",
    "}\n",
    "\n",
    "\n",
    "def simulate_hillclimber():\n",
    "    hillclimber = AdaptiveHillClimber(**hillclimber_kwargs)\n",
    "    scores = train(hillclimber, env, target_score=195.0, maximum_episodes=1000)\n",
    "    return scores\n",
    "\n",
    "\n",
    "n_jobs = multiprocessing.cpu_count()\n",
    "n_hillclimbers = 100\n",
    "hillclimber_results = joblib.Parallel(n_jobs)(joblib.delayed(simulate_hillclimber)() for _ in range(n_hillclimbers))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZgdVZ3/8fcnCTsJEGliWAOIKIsgBgRhEIggCBLkEQRZAgaC4wi4DBLUcUMfwiCM+NNRIlsi+yDboAgYSBRFICACssgWAhKTZk2AEQW+vz/OaVPp3L5d3em6ne76vJ7nPvfWqVunvnVu9/fWPVV1ShGBmZnVx5D+DsDMzFrLid/MrGac+M3MasaJ38ysZpz4zcxqxonfzKxmnPhrRtKFkr7dT+uWpAskvSjpzj6u+zBJN/VxnWMkhaRhfVlvK0m6QdKEPqwvJL2jr+pbVpJ2k/RMf8cx0Djx9zNJcyTNl7RaoewYSTP7Mayq7ALsCawfETv0ZcURcXFE7NWXdQ4GEbFPRExrxbokzZR0TCvWZcvGiX/5MAw4sb+D6ClJQ3u4yEbAnIh4tYp4BqL+/DUxkH/J2LJx4l8+nAH8u6Q1O89o1N1Q3LOSdJSk30r6L0kvSXpC0gdy+dOSFjT4qb+2pJslLZI0S9JGhbrflee9IOkRSQcX5l0o6UeSfiHpVWD3BvGuK+m6vPxjko7N5ROBc4GdJL0i6ZuNGkLSpyQ9lLuDbuwUW0g6IW/jc5LOkDSk0A635dfK7bFA0suS7pO0VZ63hqTpktolPSXpq4U6hkr6bq77CWDfTrGtIek8SfMk/UXStzu+/CS9I7fly3n5y7vYvo7Pc6KkucAtJbZ7T0kP57p/kNfT8fl/Q9JFDeof1s3fygvANyStlLd5bv7l+WNJqxTqOylv77OSPtVom/L7vgP8C/CD/Pn+IJd/QNJdOfa7JH2gSR0n53ZdlP/2xuXylSR9L8fwbH69UoPlJ0u6slPZ2ZK+393nVzsR4Uc/PoA5wIeAq4Bv57JjgJn59RgggGGFZWYCx+TXRwFvAEcDQ4FvA3OBHwIrAXsBi4DV8/svzNO75vlnA7fleasBT+e6hgHbAc8BWxaWfRnYmbTTsHKD7ZkF/DewMrAt0A6MK8R6W5O2OAB4DHh3Xv9Xgd8V5gdwKzAS2BD4c6d26NiODwN3A2sCyvWNzvOmA9cCw3Pb/hmYmOd9GngY2CCv49Zi2wPXAOfkdloHuBM4Ls+7FPhKR7sAu3SxjR2f5/RczyrNthtYG1gIfBxYAfh8/rw7tvsbwEUN6u+IeSZL/60cn9ezCvA94Lq8vcOB/wVOy+/fG5gPbJVjvSTX/Y4utu2f68rTI4EXgSPy+g7N029rsOzmpL+9dQvbsWl+/S3g97nN24DfAafmebsBz+TXGwGvASPy9FBgHrBjd59f3R79HkDdHyxO/FuRkmobPU/8jxbmbZ3fP6pQ9jywbX59IXBZYd7qwJukZPcJ4Ded4jsH+Hph2elNtmWDXNfwQtlpwIWFWJsl/hvISThPD8n/yBvl6QD2Lsz/DDCjc93AHqSEviMwpPD+ocDrwBaFsuMKbX0L8OnCvL062h4YlZddpTD/UODW/Ho6MJV0/KLZ593xeW5SZruBI4HfF+YJeIbeJ/65nep6lZxgc9lOwJP59fnAlMK8d9KzxH8EcGen99wOHNVg2XcAC0j/Cyt0mvc48JHC9IdJXYZQSPx5+jbgyPx6T+Dx/Lrp51e3h7t6lhMR8QBwPTC5F4vPL7z+v1xf57LVC9NPF9b7CvACsC4p0bxfqcvoJUkvAYcBb2+0bAPrAi9ExKJC2VPAeiW3YyPg7MK6XyAlp+LyxfU/lde5hIi4BfgB6VfPfElTJY0g7T2vmJdrFN+6DeovxrYCMK8Q3zmkPUeAL+VY75T0p2bdIg22o9l2LxFTpIzV7DPoTnHZNmBV4O7Cun+Zy+m8bpZsjzLWbbBMw7+HiHgM+Bzpi2yBpMskdXy2netp+Llnl5ASOsAn8zR0//nVihP/8uXrwLEs+Y/RcSB01UJZMRH3xgYdLyStTvpJ/izpn3xWRKxZeKweEf9aWLbZcK7PAiMlDS+UbQj8pWRcT5N+ehfXv0pE/K5R7LnuZxtVFBHfj4j3AVuS9lRPInVb/YOUBBrFN69B/cXYXgfWLsQ2IiK2zOv7a0QcGxHrkn5F/Lean/ZYbMdm271ETJLUKcZX6dnfRnG9z5F2CrYsrHeNiOjYSWjWHt3VDemz2ahTWZd/DxFxSUTskpcJ4PQu6unycwf+B9hN0vrAx1ic+Jt+fnXjxL8cyXs9lwMnFMraSf8oh+eDj58CNl3GVX1E0i6SVgROBe6IiKdJvzjeKekISSvkx/aS3l0y/qdJ/a+nSVpZ0nuAicDFJeP6MXCKpC3hnwfjDur0npMkrSVpA9KZUEsdRM0xv1/SCqTE+DfgzYh4E7gC+I6k4fkA6heAjoOjVwAnSFpf0loUfn1FxDzgJuBMSSMkDZG0qaQP5nUelJMNpH7sIHV7Let2/xzYUtKB+YDtCSyZ3O8FdpW0oaQ1gFNKrpOIeAv4CfBfktbJ615P0ocL7XGUpC0krUraMWlmPrBJYfoXpL+nT0oaJukTwBakv7MlSNpc0h75oO3fSF9IHe13KfBVSW2S1ga+xuLPrPM2tZO6nC4gdVk9lMubfn5148S//PkW6eBT0bGkPdbnSXuwv+u8UA9dQvonfgF4H6k7h9xFsxdwCGmP6q+kva6lzqBo4lBSP/OzwNWk4wM3l1kwIq7O67tM0kLgAWCfTm+7lnTg9l5SUjyvQVUjSAntRVK3wPPAd/O840lfBk+Q+oMvIfVlk5e5EfgjcA/pgHvRkaSuogdz3VcCo/O87YE7JL1COlh6YkQ8uazbHRHPAQcBU/J2bAb8trDszaQvv/tyuyyVVLtxMunA8u/zun9FOtBKRNxAOvh7S37PLd3UdTbwcaUzk74fEc8D+wFfzLF/Cdgvb1NnK+VtfI70d7cO8OU879vA7LyN95M+m2YXIV5COlZwSafyZp9frSgf5DBb7kkKYLP8y6i2lC7uuygizu3vWGxg8h6/mVnNOPGbmdWMu3rMzGrGe/xmZjUzIAZpWnvttWPMmDH9HYaZ2YBy9913PxcRbZ3LB0TiHzNmDLNnz+7vMMzMBhRJDa+2dlePmVnNOPGbmdWME7+ZWc048ZuZ1YwTv5lZzTjxm5nVTGWJPw+zem/hsVDS5ySNVLqn66P5ea2qYjAzs6VVlvgj4pGI2DYitiUN/fsaaZjeyaTb5W0GzKB3d5wyM7NealVXzzjSvS+fAsYD03L5NNKNps3MrEVadeXuIaS76EC6Cfg8SHfF6bjzT2eSJgGTADbcsLs7vnVtzOSf93rZZTVnyr79tm4zs65Uvsefb++3P+lemKVFxNSIGBsRY9valhpqwszMeqkVXT37APdExPw8PV/SaID8vKAFMZiZWdaKxH8oi7t5IN2PdEJ+PYF0D1UzM2uRShO/pFWBPVnyptVTgD0lPZrnTakyBjMzW1KlB3cj4jXgbZ3Knied5WNmZv3AV+6amdWME7+ZWc048ZuZ1YwTv5lZzTjxm5nVjBO/mVnNOPGbmdWME7+ZWc048ZuZ1YwTv5lZzTjxm5nVjBO/mVnNOPGbmdWME7+ZWc048ZuZ1YwTv5lZzTjxm5nVjBO/mVnNOPGbmdWME7+ZWc1UmvglrSnpSkkPS3pI0k6SRkq6WdKj+XmtKmMwM7MlVb3Hfzbwy4h4F7AN8BAwGZgREZsBM/K0mZm1SGWJX9IIYFfgPICI+HtEvASMB6blt00DDqgqBjMzW1qVe/ybAO3ABZL+IOlcSasBoyJiHkB+XqfRwpImSZotaXZ7e3uFYZqZ1UuViX8YsB3wo4h4L/AqPejWiYipETE2Isa2tbVVFaOZWe1UmfifAZ6JiDvy9JWkL4L5kkYD5OcFFcZgZmadVJb4I+KvwNOSNs9F44AHgeuACblsAnBtVTGYmdnShlVc//HAxZJWBJ4AjiZ92VwhaSIwFzio4hjMzKyg0sQfEfcCYxvMGlfles3MrGu+ctfMrGac+M3MasaJ38ysZpz4zcxqxonfzKxmuk38knbOQy0g6XBJZ0naqPrQzMysCmX2+H8EvCZpG+BLwFPA9EqjMjOzypRJ/G9ERJBG1Tw7Is4GhlcblpmZVaXMBVyLJJ0CHA7sKmkosEK1YZmZWVXK7PF/AngdmJjH31kPOKPSqMzMrDLd7vHnZH9WYXou7uM3MxuwypzVc2C+P+7LkhZKWiRpYSuCMzOzvlemj/8/gY9GxENVB2NmZtUr08c/30nfzGzwKLPHP1vS5cA1pIO8AETEVZVFZWZmlSmT+EcArwF7FcoCcOI3MxuAypzVc3QrAjEzs9boNvFLuoC0h7+EiPhUJRGZmVmlynT1XF94vTLwMeDZasIxM7Oqlenq+VlxWtKlwK8qi8jMzCrVm/H4NwM27OtAzMysNcr08S9iyT7+vwInl6lc0hxgEfAmaZTPsZJGApcDY4A5wMER8WKPojYzs15ruscvScCWETGi8Hhn5+6fbuweEdtGxNg8PRmYERGbATPytJmZtUjTxJ/H4b+6j9c5HpiWX08DDujj+s3MrIkyffy/l7R9L+sP4CZJd0ualMtGRcQ8gPy8TqMFJU2SNFvS7Pb29l6u3szMOitzOufuwHGSngJeBUT6MfCeEsvuHBHPSloHuFnSw2UDi4ipwFSAsWPHLnUdgZmZ9U6ZxL9PbyuPiGfz8wJJVwM7APMljY6IeZJGAwt6W7+ZmfVct109EfFUo0d3y0laTdLwjteksX4eAK4DJuS3TQCu7X34ZmbWU2X2+HtrFHB1OjGIYcAlEfFLSXcBV0iaCMwFDqowBjMz66SyxB8RTwDbNCh/HhhX1XrNzKy53ly5a2ZmA1hvrtwFeBmYDXwx79mbmdkAUaar5yzSaJyXkE7lPAR4O/AIcD6wW1XBmZlZ3yvT1bN3RJwTEYsiYmE+v/4jEXE5sFbF8ZmZWR8rk/jfknSwpCH5cXBhni+sMjMbYMok/sOAI0gXWs3Prw+XtArw2QpjMzOzCpS5EcsTwEe7mH1b34ZjZmZVK3NWTxtwLGn8/H++3/fcNTMbmMqc1XMt8BvS7RbfrDYcMzOrWpnEv2pElLrjlpmZLf/KHNy9XtJHKo/EzMxaokziP5GU/P9P0kJJiyQtrDowMzOrRpmzeoa3IhAzM2uNLhO/pHdFxMOStms0PyLuqS4sMzOrSrM9/i8Ak4AzG8wLYI9KIjIzs0p1mfgjYlJ+3r114ZiZWdVK3YhF0gdY+gKu6RXFZGZmFSpz5e5PgU2Be1l8AVcATvxmZgNQmT3+scAWEeGROM3MBoEy5/E/QLrxipmZDQJl9vjXBh6UdCfwekdhROxfZgWShpJu0/iXiNhP0kjgctIxgznAwRHxYg/jNjOzXiqT+L+xjOs4EXgIGJGnJwMzImKKpMl52mMBmZm1SJkrd2f1tnJJ6wP7At8hXRcAMJ7F9+mdBszEid/MrGW67OOXdFt+XpTH6FnYi7F6vgd8CXirUDYqIuYB5Od1ulj/JEmzJc1ub28vuTozM+tOl4k/InbJz8MjYkThMTwiRnS1XAdJ+wELIuLu3gQWEVMjYmxEjG1ra+tNFWZm1kCpC7gAJK0DrNwxHRFzu1lkZ2D/PKTzysAISRcB8yWNjoh5kkaT7uVrZmYt0u3pnJL2l/Qo8CQwi3Qmzg3dLRcRp0TE+hExBjgEuCUiDgeuAybkt00g3eHLzMxapMx5/KcCOwJ/joiNgXHAb5dhnVOAPfOXyZ552szMWqRMV88/IuJ5SUMkDYmIWyWd3pOVRMRM0tk7RMTzpC8PMzPrB2US/0uSVgd+DVwsaQHwRrVhmZlZVcp09YwHXgM+D/wSeBz4aJVBmZlZdZru8efhFq6NiA+RzsWf1pKozMysMk33+CPiTeA1SWu0KB4zM6tYmT7+vwH3S7oZeLWjMCJOqCwqMzOrTJnE//P8KPLY/GZmA1SZxL9mRJxdLJB0YkXxmJlZxcqc1TOhQdlRfRyHmZm1SJd7/JIOBT4JbCzpusKs4cDzVQdmZmbVaNbV8ztgHukOXGcWyhcB91UZlJmZVafLxB8RTwFPATu1LhwzM6tamT5+MzMbRJz4zcxqptmtF2fk5x6NxGlmZsu3Zgd3R0v6IOkuWpcBKs6MiHsqjczMzCrRLPF/DZgMrA+c1WleAHtUFZSZmVWn2Vk9VwJXSvqPiDi1hTGZmVmFuh2yISJOlbQ/sGsumhkR11cblpmZVaXMzdZPA04EHsyPE3OZmZkNQGUGadsX2DYi3gKQNA34A3BKlYGZmVk1yp7Hv2bhtW/KYmY2gJXZ4z8N+IOkW0mndO5Kib19SSuTbtC+Ul7PlRHxdUkjgcuBMcAc4OCIeLFX0ZuZWY91u8cfEZcCOwJX5cdOEXFZibpfB/aIiG2AbYG9Je1IOkV0RkRsBszI02Zm1iJl9viJiHnAdd2+ccllAnglT66QHwGMB3bL5dOAmcDJPanbzMx6r9KxeiQNlXQvsAC4OSLuAEblL5KOL5R1ulh2kqTZkma3t7dXGaaZWa1Umvgj4s2I2JZ09e8OkrbqwbJTI2JsRIxta2urLkgzs5ppmvglDZH0wLKuJCJeInXp7A3MlzQ61z+a9GvAzMxapGniz+fu/1HShj2tWFKbpDXz61WADwEPk44VdNzHdwJwbU/rNjOz3itzcHc08CdJdwKvdhRGxP4llpsmaSjpC+aKiLhe0u3AFZImAnOBg3oXupmZ9UaZxP/N3lQcEfcB721Q/jwwrjd1mpnZsiszSNssSRsBm0XEryStCgytPjQzM6tCmUHajgWuBM7JResB11QZlJmZVafM6Zz/BuwMLASIiEfp4tx7MzNb/pVJ/K9HxN87JiQNI12Ba2ZmA1CZxD9L0peBVSTtCfwP8L/VhmVmZlUpk/gnA+3A/cBxwC+Ar1YZlJmZVafMWT1v5Zuv3EHq4nkkD8BmZmYDULeJX9K+wI+Bx0nj8W8s6biIuKHq4MzMrO+VuYDrTGD3iHgMQNKmwM8BJ34zswGoTB//go6knz2BB1YzMxuwutzjl3RgfvknSb8AriD18R8E3NWC2MzMrALNuno+Wng9H/hgft0OrFVZRGZmVqkuE39EHN3KQMzMrDXKnNWzMXA8MKb4/hLDMpuZ2XKozFk91wDnka7WfavacMzMrGplEv/fIuL7lUdiZmYtUSbxny3p68BNwOsdhRFxT2VRmZlZZcok/q2BI4A9WNzVE3nazMwGmDKJ/2PAJsWhmc3MbOAqc+XuH4E1qw7EzMxao8we/yjgYUl3sWQff9PTOSVtAEwH3k7qIpoaEWdLGglcTjo9dA5wcES82Kvozcysx8ok/q/3su43gC9GxD2ShgN3S7oZOAqYERFTJE0mjfd/ci/XYWZmPVRmPP5Zvak4IuYB8/LrRZIeIt2ofTywW37bNGAmTvxmZi1T5srdRSy+x+6KwArAqxExouxKJI0B3ku6mcuo/KVARMyT5Bu3m5m1UJk9/uHFaUkHADuUXYGk1YGfAZ+LiIWSyi43CZgEsOGGG5ZdnZmZdaPMWT1LiIhrKHkOv6QVSEn/4oi4KhfPlzQ6zx9NF2P7R8TUiBgbEWPb2tp6GqaZmXWhTFfPgYXJIcBYFnf9NFtOpDF+HoqIswqzrgMmAFPy87U9CdjMzJZNmbN6iuPyv0E6BXN8ieV2Jl3xe7+ke3PZl0kJ/wpJE4G5pBu7mJlZi5Tp4+/VuPwRcRvp5uyNjOtNnWZmtuya3Xrxa02Wi4g4tYJ4zMysYs32+F9tULYaMBF4G+DEb2Y2ADW79eKZHa/zlbcnAkcDlwFndrWcmZkt35r28edxdb4AHEa6ynY7j6tjZjawNevjPwM4EJgKbB0Rr7QsKjMzq0yzC7i+CKwLfBV4VtLC/FgkaWFrwjMzs77WrI+/x1f1mpnZ8s/J3cysZpz4zcxqxonfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZpz4zcxqxonfzKxmnPjNzGrGid/MrGac+M3MasaJ38ysZpz4zcxqprLEL+l8SQskPVAoGynpZkmP5ue1qlq/mZk1VuUe/4XA3p3KJgMzImIzYEaeNjOzFqos8UfEr4EXOhWPJ927l/x8QFXrNzOzxlrdxz8qIuYB5Od1unqjpEmSZkua3d7e3rIAzcwGu+X24G5ETI2IsRExtq2trb/DMTMbNFqd+OdLGg2Qnxe0eP1mZrXX6sR/HTAhv54AXNvi9ZuZ1V6Vp3NeCtwObC7pGUkTgSnAnpIeBfbM02Zm1kLDqqo4Ig7tYta4qtZpZmbdqyzxG4yZ/PN+We+cKfv2y3rNbGBYbs/qMTOzajjxm5nVjLt6zMy60V/dtlBN1633+M3MasaJ38ysZtzVMwj5bCIza8Z7/GZmNePEb2ZWM078ZmY148RvZlYzTvxmZjXjs3qszwy2i1zMBivv8ZuZ1YwTv5lZzTjxm5nVjBO/mVnNOPGbmdWMz+qxQaGO4xPVcZutb3iP38ysZpz4zcxqpl+6eiTtDZwNDAXOjYgp/RGHmQ0s/XmR4GDS8j1+SUOBHwL7AFsAh0raotVxmJnVVX909ewAPBYRT0TE34HLgPH9EIeZWS31R1fPesDThelngPd3fpOkScCkPPmKpEdaEFtfWxt4rr+DWM4MqjbR6ctcxYBrjz7Y5mYGXHtUTacvU5ts1KiwPxK/GpTFUgURU4Gp1YdTHUmzI2Jsf8exPHGbLMntsSS3x9KqaJP+6Op5BtigML0+8Gw/xGFmVkv9kfjvAjaTtLGkFYFDgOv6IQ4zs1pqeVdPRLwh6bPAjaTTOc+PiD+1Oo4WGdBdVRVxmyzJ7bEkt8fS+rxNFLFU97qZmQ1ivnLXzKxmnPjNzGrGib+XJG0g6VZJD0n6k6QTc/lISTdLejQ/r1VY5hRJj0l6RNKH+y/66kgaKukPkq7P03VvjzUlXSnp4fy3slOd20TS5/P/ywOSLpW0ct3aQ9L5khZIeqBQ1uM2kPQ+Sffned+X1OhU+cYiwo9ePIDRwHb59XDgz6QhKP4TmJzLJwOn59dbAH8EVgI2Bh4Hhvb3dlTQLl8ALgGuz9N1b49pwDH59YrAmnVtE9LFm08Cq+TpK4Cj6tYewK7AdsADhbIetwFwJ7AT6dqoG4B9ysbgPf5eioh5EXFPfr0IeIj0hz2e9M9Ofj4gvx4PXBYRr0fEk8BjpOErBg1J6wP7AucWiuvcHiNI/+TnAUTE3yPiJWrcJqQzCVeRNAxYlXQNT63aIyJ+DbzQqbhHbSBpNDAiIm6P9C0wvbBMt5z4+4CkMcB7gTuAURExD9KXA7BOflujoSrWa12ULfE94EvAW4WyOrfHJkA7cEHu/jpX0mrUtE0i4i/Ad4G5wDzg5Yi4iZq2Ryc9bYP18uvO5aU48S8jSasDPwM+FxELm721QdmgOZdW0n7Agoi4u+wiDcoGTXtkw0g/6X8UEe8FXiX9jO/KoG6T3G89ntRlsS6wmqTDmy3SoGzQtEdJXbXBMrWNE/8ykLQCKelfHBFX5eL5+WcY+XlBLh/sQ1XsDOwvaQ5pxNU9JF1EfdsD0jY+ExF35OkrSV8EdW2TDwFPRkR7RPwDuAr4APVtj6KetsEz+XXn8lKc+HspH0E/D3goIs4qzLoOmJBfTwCuLZQfImklSRsDm5EOzgwKEXFKRKwfEWNIw3DcEhGHU9P2AIiIvwJPS9o8F40DHqS+bTIX2FHSqvn/Zxzp2Fhd26OoR22Qu4MWSdoxt+WRhWW6199HuAfqA9iF9NPqPuDe/PgI8DZgBvBofh5ZWOYrpKPyj9CDI/AD7QHsxuKzemrdHsC2wOz8d3INsFad2wT4JvAw8ADwU9LZKrVqD+BS0jGOf5D23Cf2pg2AsbkdHwd+QB6JoczDQzaYmdWMu3rMzGrGid/MrGac+M3MasaJ38ysZpz4zcxqxonf+o2kkHRmYfrfJX2jj+q+UNLH+6KubtZzUB5189Y+qOtcSVssYx1jiqM+mjXixG/96XXgQElr93cgRZKG9uDtE4HPRMTuy7reiDgmIh5c1nrMuuPEb/3pDdL9RD/feUbnPXZJr+Tn3STNknSFpD9LmiLpMEl35rHJNy1U8yFJv8nv2y8vP1TSGZLuknSfpOMK9d4q6RLg/gbxHJrrf0DS6bnsa6QL+X4s6YwGy5xUWM83c9kYpbH5p+XyKyWtmufNlDQ2x3hhXtf9kj6f528r6fd5uas7xmzP47L/UdLtwL8V1t/Vto6W9GtJ9+Z1/EsPPjMbBJz4rb/9EDhM0ho9WGYb4ERga+AI4J0RsQNpOOjjC+8bA3yQNFT0jyWtTNpDfzkitge2B47Nl8JDGvL3KxGxRHeLpHWB04E9SFfibi/pgIj4Fumq3MMi4qROy+xFurx+h7zM+yTtmmdvDkyNiPcAC4HPdNq+bYH1ImKriNgauCCXTwdOzsvdD3w9l18AnBARO3Wqp6tt/SRwY0Rsm9vyXqxWnPitX0Ua0XQ6cEIPFrsr0v0QXiddrn5TLr+flOw7XBERb0XEo8ATwLuAvYAjJd1LGkb7baQEDWkMlCcbrG97YGakwcXeAC4mjbPfzF758QfgnrzujvU8HRG/za8vIv1qKHoC2ETS/5O0N7AwfzGuGRGz8numAbs2KP9ppxgabetdwNH5eMrWke4nYTUyrL8DMCON438Pi/dsIXUDDYF/Doi3YmHe64XXbxWm32LJv+nO45F0DGd7fETcWJwhaTfSsMmNlL+l3ZLLnBYR53Raz5gu4lo8EfGipG2AD5O6bg6mQXdYYT1djbvScFtzHLuSfgn9VNIZETG96dbYoOI9fut3EfEC6TZ8EwvFc4D35dfjgRV6UfVBkobkfv9NSINc3Qj8q9KQ2kh6p9LNUZq5A/igpLXzgd9DgVndLHMj8Cml+zUgaT1JHTfX2FBSR7fMocBtxQXzwe4hEfEz4D9It/h8GXix0B9/BDAr0h29XpbU8avhsE4xLLWtkjYi3TvhJ6QRZrfrZltskPEevy0vzgQ+Wzs3SfYAAADBSURBVJj+CXCtpDtJoxV2tTfezCOkBD0K+HRE/E3SuaTuoHvyL4l2urllXUTMk3QKcCtpL/oXEdF0CNyIuEnSu4Hb02p4BTgceJM0FPEESeeQRmP8UafF1yPdtatjx+yU/DyBdKxiVVJ30NG5/GjgfEmvkZJ9h662dTfgJEn/yHEd2WxbbPDx6JxmLZS7eq6PiK36ORSrMXf1mJnVjPf4zcxqxnv8ZmY148RvZlYzTvxmZjXjxG9mVjNO/GZmNfP/AdexBoQWqArvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "adaptive_hillclimber_required_episodes = [len(scores) for scores in hillclimber_results]\n",
    "_ = ax.hist(adaptive_hillclimber_required_episodes)\n",
    "_ = ax.set(xlabel=\"Number of episodes\",\n",
    "           ylabel=\"Number of training runs\",\n",
    "           title=\"Number of episodes required to solve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Ascent Hill-climbing\n",
    "\n",
    "Steepest ascent hill-climbing is an extension of the basic hill-climbing algorithm that is more robust. Rather than using a single `HillClimber`, the steepest ascent algorithm starts with a population of `HillClimber` instances. During an iteration of the steepest ascent algorithm each of the `HillClimber` instances is trained using the basic algorithm and then the `HillClimber` that has made the most progress is identified and the parameters of its policy function are shared with the other `HillClimber` instances. This processes is repeated until either the environment is solved or some maximum number of iteration is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "HillClimbers = typing.List[HillClimber]\n",
    "\n",
    "\n",
    "def synchronize_hillclimbers(hillclimbers: HillClimbers,\n",
    "                             state_dict: typing.Dict[str, torch.Tensor]) -> None:\n",
    "    _ = [hillclimber.load_state_dict(state_dict) for hillclimber in hillclimbers]\n",
    "\n",
    "\n",
    "def top_k_hillclimbers(hillclimbers: HillClimbers, k: int = 1) -> HillClimbers:\n",
    "    sorted_hillclimbers = sorted(hillclimbers)\n",
    "    return sorted_hillclimbers[-k:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_ascent_hillclimbing(env: gym.Env,\n",
    "                                 hillclimber_kwargs: dict,\n",
    "                                 target_score: float,\n",
    "                                 number_episodes: int = 2,\n",
    "                                 number_hillclimbers: int = 5,\n",
    "                                 number_iterations: int = 150,\n",
    "                                 verbose: bool = False):\n",
    "    hillclimbers = [HillClimber(**hillclimber_kwargs) for _ in range(number_hillclimbers)]\n",
    "    scores = []\n",
    "    most_recent_scores = collections.deque(maxlen=100)\n",
    "\n",
    "    n_jobs = multiprocessing.cpu_count()\n",
    "    \n",
    "    for i in range(number_iterations):\n",
    "        # train each hillclimber for some number of episodes (this can be done in parallel)\n",
    "        _ = joblib.Parallel(n_jobs)(joblib.delayed(train)(_, env, float(\"inf\"), number_episodes) for _ in hillclimbers)    \n",
    "\n",
    "        # identify the best hillclimber and evaluate the best policy\n",
    "        best_hillclimber, = top_k_hillclimbers(hillclimbers)\n",
    "        score = best_hillclimber.score\n",
    "        scores.append(score)\n",
    "        most_recent_scores.append(score)\n",
    "        \n",
    "        # if not solved, then synchronize the hill climbers and repeat\n",
    "        average_score = sum(most_recent_scores) / len(most_recent_scores)\n",
    "        if len(most_recent_scores) >= 100 and average_score >= target_score:\n",
    "            if verbose:\n",
    "                print(f\"\\nEnvironment solved in {i:d} iterations!\\tAverage Score: {average_score:.2f}\")\n",
    "            break\n",
    "        else:\n",
    "            synchronize_hillclimbers(hillclimbers, best_hillclimber._best_policy_state_dict)\n",
    "    \n",
    "    return scores, best_hillclimber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drpugh/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-af74c4f9502c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_hillclimber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteepest_ascent_hillclimbing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hillclimber_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m195.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-9da55bd30bdc>\u001b[0m in \u001b[0;36msteepest_ascent_hillclimbing\u001b[0;34m(env, hillclimber_kwargs, target_score, number_episodes, number_hillclimbers, number_iterations, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# train each hillclimber for some number of episodes (this can be done in parallel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_episodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhillclimbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# identify the best hillclimber and evaluate the best policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6), SIGABRT(-6)}"
     ]
    }
   ],
   "source": [
    "_policy_fn = make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES)\n",
    "\n",
    "_hillclimber_kwargs = {\n",
    "    \"policy_fn\": _policy_fn,\n",
    "    \"gamma\": 1.0,\n",
    "    \"is_deterministic\": False,\n",
    "    \"preprocessing_fn\": None,\n",
    "    \"seed\": None,\n",
    "    \"sigma\": 1.0\n",
    "}\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "scores, best_hillclimber = steepest_ascent_hillclimbing(env, _hillclimber_kwargs, target_score=195.0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = None\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(_seed)\n",
    "\n",
    "_policy_fn = make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES)\n",
    "\n",
    "_hillclimber_kwargs = {\n",
    "    \"policy_fn\": _policy_fn,\n",
    "    \"gamma\": 1.0,\n",
    "    \"is_deterministic\": False,\n",
    "    \"preprocessing_fn\": None,\n",
    "    \"seed\": None,\n",
    "    \"sigma\": 1.0\n",
    "}\n",
    "\n",
    "steepest_ascent_required_episodes = []\n",
    "for t in range(100):\n",
    "    scores, _ = steepest_ascent_hillclimbing(env, _hillclimber_kwargs, target_score=195.0, number_iterations=1000)\n",
    "    steepest_ascent_required_episodes.append(len(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3debgcVZ3/8fcnCTsJELmJYQkBDCiLRLygCIMsgiBIkBEEWQJEguMIuCFBGUHRhzAMjvjTESICQVkHRRgQAQOJorIECJuALLJJTMKaAIoC398fda6pdLr71l2q+97U5/U8/XQtXed8q7ru91afqjqliMDMzKpjSLsDMDOz1nLiNzOrGCd+M7OKceI3M6sYJ34zs4px4jczqxgn/oqRdIGkb7apbkk6X9KLkm7v57IPlnRDP5c5TlJIGtaf5baSpOskTerH8kLSO/qrvL6StJOkZ9odx2DjxN9mkp6QNF/Sarlpn5I0q41hlWUHYDdgvYjYtj8LjoiLImL3/ixzeRARe0bEjFbUJWmWpE+1oi7rGyf+gWEYcFy7g+gpSUN7uMgGwBMR8WoZ8QxG7fw1MZh/yVjfOPEPDGcAX5K0Zu2Mes0N+SMrSYdL+q2k/5b0kqTHJX0gTX9a0oI6P/XXlnSjpMWSZkvaIFf2O9O8FyQ9LOmA3LwLJP1A0i8kvQrsXCfedSRdnZZ/VNJRafpk4FxgO0mvSPp6vQ0h6UhJD6bmoOtrYgtJx6Z1fE7SGZKG5LbDLWlYaXsskPSypHslbZHmrSHpQkkLJT0p6aRcGUMl/Vcq+3Fgr5rY1pD0I0nzJP1Z0je7/vlJekfali+n5S9rsH5d3+dkSU8BNxVY790kPZTK/l6qp+v7P0XST+qUP6ybfeUF4BRJK6V1fir98jxb0iq58o5P6/uspCPrrVP63LeAfwG+l77f76XpH5B0R4r9DkkfaFLGCWm7Lk773q5p+kqSvpNieDYNr1Rn+amSrqiZdpak73b3/VVORPjVxhfwBPAh4GfAN9O0TwGz0vA4IIBhuWVmAZ9Kw4cDbwBHAEOBbwJPAd8HVgJ2BxYDq6fPX5DGd0zzzwJuSfNWA55OZQ0DtgaeAzbPLfsysD3ZQcPKddZnNvA/wMrABGAhsGsu1luabIt9gUeBd6X6TwJ+l5sfwM3ASGAs8Mea7dC1Hh8G7gTWBJTKG5PmXQhcBQxP2/aPwOQ079PAQ8D6qY6b89se+DlwTtpOo4DbgaPTvEuAr3ZtF2CHBuvY9X1emMpZpdl6A2sDi4CPAysAn0/fd9d6nwL8pE75XTHPYtl95ZhUzyrAd4Cr0/oOB/4POC19fg9gPrBFivXiVPY7GqzbP+tK4yOBF4FDU30HpfG31Vl2U7J9b53cemychr8B3Jq2eQfwO+DUNG8n4Jk0vAHwGjAijQ8F5gHv7+77q9qr7QFU/cWSxL8FWVLtoOeJ/5HcvC3T50fnpj0PTEjDFwCX5uatDrxJluw+AfymJr5zgJNzy17YZF3WT2UNz007DbggF2uzxH8dKQmn8SHpD3mDNB7AHrn5nwFm1pYN7EKW0N8PDMl9fijwOrBZbtrRuW19E/Dp3Lzdu7Y9MDotu0pu/kHAzWn4QmA62fmLZt931/e5UZH1Bg4Dbs3NE/AMvU/8T9WU9SopwaZp2wF/SsPnAdNy8zahZ4n/UOD2ms/8Hji8zrLvABaQ/S2sUDPvMeAjufEPkzUZQi7xp/FbgMPS8G7AY2m46fdXtZebegaIiLgfuAaY2ovF5+eG/5rKq522em786Vy9rwAvAOuQJZr3KWsyeknSS8DBwNvrLVvHOsALEbE4N+1JYN2C67EBcFau7hfIklN++Xz9T6Y6lxIRNwHfI/vVM1/SdEkjyI6eV0zL1YtvnTrl52NbAZiXi+8csiNHgC+nWG+X9ECzZpE669FsvZeKKbKM1ew76E5+2Q5gVeDOXN2/TNOprZult0cR69RZpu7+EBGPAp8j+0e2QNKlkrq+29py6n7vycVkCR3gk2kcuv/+KsWJf2A5GTiKpf8wuk6Erpqblk/EvbF+14Ck1cl+kj9L9kc+OyLWzL1Wj4h/yy3brDvXZ4GRkobnpo0F/lwwrqfJfnrn618lIn5XL/ZU9rP1CoqI70bEe4HNyY5UjydrtvoHWRKoF9+8OuXnY3sdWDsX24iI2DzV95eIOCoi1iH7FfE/an7ZY347NlvvpWKSpJoYX6Vn+0a+3ufIDgo2z9W7RkR0HSQ02x7dlQ3Zd7NBzbSG+0NEXBwRO6RlAji9QTkNv3fgf4GdJK0HfIwlib/p91c1TvwDSDrquQw4NjdtIdkfyiHp5OORwMZ9rOojknaQtCJwKnBbRDxN9otjE0mHSlohvbaR9K6C8T9N1v56mqSVJb0bmAxcVDCus4ETJW0O/zwZt3/NZ46XtJak9cmuhFrmJGqK+X2SViBLjH8D3oyIN4HLgW9JGp5OoH4B6Do5ejlwrKT1JK1F7tdXRMwDbgDOlDRC0hBJG0v6YKpz/5RsIGvHDrJmr76u97XA5pL2Sydsj2Xp5D4X2FHSWElrACcWrJOIeAv4IfDfkkaluteV9OHc9jhc0maSViU7MGlmPrBRbvwXZPvTJyUNk/QJYDOy/WwpkjaVtEs6afs3sn9IXdvvEuAkSR2S1ga+xpLvrHadFpI1OZ1P1mT1YJre9PurGif+gecbZCef8o4iO2J9nuwI9ne1C/XQxWR/xC8A7yVrziE10ewOHEh2RPUXsqOuZa6gaOIgsnbmZ4Eryc4P3FhkwYi4MtV3qaRFwP3AnjUfu4rsxO1csqT4ozpFjSBLaC+SNQs8D/xXmncM2T+Dx8nagy8ma8smLXM9cA9wF9kJ97zDyJqK/pDKvgIYk+ZtA9wm6RWyk6XHRcSf+rreEfEcsD8wLa3HeOC3uWVvJPvnd2/aLssk1W6cQHZi+dZU96/ITrQSEdeRnfy9KX3mpm7KOgv4uLIrk74bEc8DewNfTLF/Gdg7rVOtldI6Pke2340CvpLmfROYk9bxPrLvptlNiBeTnSu4uGZ6s++vUpROcpgNeJICGJ9+GVWWspv7fhIR57Y7FhucfMRvZlYxTvxmZhXjph4zs4rxEb+ZWcUMik6a1l577Rg3bly7wzAzG1TuvPPO5yKio3b6oEj848aNY86cOe0Ow8xsUJFU925rN/WYmVWME7+ZWcU48ZuZVYwTv5lZxTjxm5lVjBO/mVnFlJb4Uzerc3OvRZI+J2mksme6PpLe1yorBjMzW1ZpiT8iHo6ICRExgazr39fIuumdSva4vPHATHr3xCkzM+ulVjX17Er27MsngYnAjDR9BtmDps3MrEVadefugWRP0YHsIeDzIHsqTteTf2pJmgJMARg7trsnvjU2buq1vV62r56Ytlfb6jYza6T0I/70eL99yJ6FWVhETI+Izojo7OhYpqsJMzPrpVY09ewJ3BUR89P4fEljANL7ghbEYGZmSSsS/0EsaeaB7Hmkk9LwJLJnqJqZWYuUmvglrQrsxtIPrZ4G7CbpkTRvWpkxmJnZ0ko9uRsRrwFvq5n2PNlVPmZm1ga+c9fMrGKc+M3MKsaJ38ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKsaJ38ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKsaJ38ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKqbUxC9pTUlXSHpI0oOStpM0UtKNkh5J72uVGYOZmS2t7CP+s4BfRsQ7ga2AB4GpwMyIGA/MTONmZtYipSV+SSOAHYEfAUTE3yPiJWAiMCN9bAawb1kxmJnZsso84t8IWAicL+luSedKWg0YHRHzANL7qHoLS5oiaY6kOQsXLiwxTDOzaikz8Q8DtgZ+EBHvAV6lB806ETE9IjojorOjo6OsGM3MKqfMxP8M8ExE3JbGryD7RzBf0hiA9L6gxBjMzKxGaYk/Iv4CPC1p0zRpV+APwNXApDRtEnBVWTGYmdmyhpVc/jHARZJWBB4HjiD7Z3O5pMnAU8D+JcdgZmY5pSb+iJgLdNaZtWuZ9ZqZWWO+c9fMrGKc+M3MKsaJ38ysYpz4zcwqxonfzKxiuk38krZPXS0g6RBJ35a0QfmhmZlZGYoc8f8AeE3SVsCXgSeBC0uNyszMSlMk8b8REUHWq+ZZEXEWMLzcsMzMrCxFbuBaLOlE4BBgR0lDgRXKDcvMzMpS5Ij/E8DrwOTU/866wBmlRmVmZqXp9og/Jftv58afwm38ZmaDVpGrevZLz8d9WdIiSYslLWpFcGZm1v+KtPH/J/DRiHiw7GDMzKx8Rdr45zvpm5ktP4oc8c+RdBnwc7KTvABExM9Ki8rMzEpTJPGPAF4Dds9NC8CJ38xsECpyVc8RrQjEzMxao9vEL+l8siP8pUTEkaVEZGZmpSrS1HNNbnhl4GPAs+WEY2ZmZSvS1PPT/LikS4BflRaRmZmVqjf98Y8HxvZ3IGZm1hpF2vgXs3Qb/1+AE4oULukJYDHwJlkvn52SRgKXAeOAJ4ADIuLFHkVtZma91vSIX5KAzSNiRO61SW3zTzd2jogJEdGZxqcCMyNiPDAzjZuZWYs0TfypH/4r+7nOicCMNDwD2LefyzczsyaKtPHfKmmbXpYfwA2S7pQ0JU0bHRHzANL7qHoLSpoiaY6kOQsXLuxl9WZmVqvI5Zw7A0dLehJ4FRDZj4F3F1h2+4h4VtIo4EZJDxUNLCKmA9MBOjs7l7mPwMzMeqdI4t+zt4VHxLPpfYGkK4FtgfmSxkTEPEljgAW9Ld/MzHqu26aeiHiy3qu75SStJml41zBZXz/3A1cDk9LHJgFX9T58MzPrqSJH/L01GrgyuzCIYcDFEfFLSXcAl0uaDDwF7F9iDGZmVqO0xB8RjwNb1Zn+PLBrWfWamVlzvblz18zMBrHe3LkL8DIwB/hiOrI3M7NBokhTz7fJeuO8mOxSzgOBtwMPA+cBO5UVnJmZ9b8iTT17RMQ5EbE4Ihal6+s/EhGXAWuVHJ+ZmfWzIon/LUkHSBqSXgfk5vnGKjOzQaZI4j8YOJTsRqv5afgQSasAny0xNjMzK0GRB7E8Dny0wexb+jccMzMrW5GrejqAo8j6z//n5/3MXTOzwanIVT1XAb8he9zim+WGY2ZmZSuS+FeNiEJP3DIzs4GvyMndayR9pPRIzMysJYok/uPIkv9fJS2StFjSorIDMzOzchS5qmd4KwIxM7PWaJj4Jb0zIh6StHW9+RFxV3lhmZlZWZod8X8BmAKcWWdeALuUEpGZmZWqYeKPiCnpfefWhWNmZmUr9CAWSR9g2Ru4LiwpJjMzK1GRO3d/DGwMzGXJDVwBOPGbmQ1CRY74O4HNIsI9cZqZLQeKXMd/P9mDV8zMbDlQ5Ih/beAPkm4HXu+aGBH7FKlA0lCyxzT+OSL2ljQSuIzsnMETwAER8WIP4zYzs14qkvhP6WMdxwEPAiPS+FRgZkRMkzQ1jbsvIDOzFily5+7s3hYuaT1gL+BbZPcFAExkyXN6ZwCzcOI3M2uZhm38km5J74tTHz2LetFXz3eALwNv5aaNjoh5AOl9VIP6p0iaI2nOwoULC1ZnZmbdaZj4I2KH9D48IkbkXsMjYkSj5bpI2htYEBF39iawiJgeEZ0R0dnR0dGbIszMrI5CN3ABSBoFrNw1HhFPdbPI9sA+qUvnlYERkn4CzJc0JiLmSRpD9ixfMzNrkW4v55S0j6RHgD8Bs8muxLmuu+Ui4sSIWC8ixgEHAjdFxCHA1cCk9LFJZE/4MjOzFilyHf+pwPuBP0bEhsCuwG/7UOc0YLf0z2S3NG5mZi1SpKnnHxHxvKQhkoZExM2STu9JJRExi+zqHSLiebJ/HmZm1gZFEv9LklYHfg1cJGkB8Ea5YZmZWVmKNPVMBF4DPg/8EngM+GiZQZmZWXmaHvGn7hauiogPkV2LP6MlUZmZWWmaHvFHxJvAa5LWaFE8ZmZWsiJt/H8D7pN0I/Bq18SIOLa0qMzMrDRFEv+16ZXnvvnNzAapIol/zYg4Kz9B0nElxWNmZiUrclXPpDrTDu/nOMzMrEUaHvFLOgj4JLChpKtzs4YDz5cdmJmZlaNZU8/vgHlkT+A6Mzd9MXBvmUGZmVl5Gib+iHgSeBLYrnXhmJlZ2Yq08ZuZ2XLEid/MrGKaPXpxZnrvUU+cZmY2sDU7uTtG0gfJnqJ1KaD8zIi4q9TIzMysFM0S/9eAqcB6wLdr5gWwS1lBmZlZeZpd1XMFcIWk/4iIU1sYk5mZlajbLhsi4lRJ+wA7pkmzIuKacsMyM7OyFHnY+mnAccAf0uu4NM3MzAahIp207QVMiIi3ACTNAO4GTiwzMDMzK0fR6/jXzA37oSxmZoNYkSP+04C7Jd1MdknnjhQ42pe0MtkD2ldK9VwRESdLGglcBowDngAOiIgXexW9mZn1WLdH/BFxCfB+4GfptV1EXFqg7NeBXSJiK2ACsIek95NdIjozIsYDM9O4mZm1SJEjfiJiHnB1tx9cepkAXkmjK6RXABOBndL0GcAs4ISelG1mZr1Xal89koZKmgssAG6MiNuA0ekfSdc/lFENlp0iaY6kOQsXLiwzTDOzSik18UfEmxExgezu320lbdGDZadHRGdEdHZ0dJQXpJlZxTRN/JKGSLq/r5VExEtkTTp7APMljUnljyH7NWBmZi3SNPGna/fvkTS2pwVL6pC0ZhpeBfgQ8BDZuYKu5/hOAq7qadlmZtZ7RU7ujgEekHQ78GrXxIjYp8ByMyQNJfsHc3lEXCPp98DlkiYDTwH79y50MzPrjSKJ/+u9KTgi7gXeU2f688CuvSnTzMz6rkgnbbMlbQCMj4hfSVoVGFp+aGZmVoYinbQdBVwBnJMmrQv8vMygzMysPEUu5/x3YHtgEUBEPEKDa+/NzGzgK5L4X4+Iv3eNSBpGdgeumZkNQkUS/2xJXwFWkbQb8L/A/5UblpmZlaVI4p8KLATuA44GfgGcVGZQZmZWniJX9byVHr5yG1kTz8OpAzYzMxuEuk38kvYCzgYeI+uPf0NJR0fEdWUHZ2Zm/a/IDVxnAjtHxKMAkjYGrgWc+M3MBqEibfwLupJ+8jjuWM3MbNBqeMQvab80+ICkXwCXk7Xx7w/c0YLYzMysBM2aej6aG54PfDANLwTWKi0iMzMrVcPEHxFHtDIQMzNrjSJX9WwIHAOMy3++QLfMZmY2ABW5qufnwI/I7tZ9q9xwzMysbEUS/98i4rulR2JmZi1RJPGfJelk4Abg9a6JEXFXaVGZmVlpiiT+LYFDgV1Y0tQTadzMzAaZIon/Y8BG+a6Zzcxs8Cpy5+49wJplB2JmZq1R5Ih/NPCQpDtYuo2/6eWcktYHLgTeTtZEND0izpI0EriM7PLQJ4ADIuLFXkVvZmY9ViTxn9zLst8AvhgRd0kaDtwp6UbgcGBmREyTNJWsv/8TelmHmZn1UJH++Gf3puCImAfMS8OLJT1I9qD2icBO6WMzgFk48ZuZtUyRO3cXs+QZuysCKwCvRsSIopVIGge8h+xhLqPTPwUiYp4kP7jdzKyFihzxD8+PS9oX2LZoBZJWB34KfC4iFkkqutwUYArA2LFji1ZnZmbdKHJVz1Ii4ucUvIZf0gpkSf+iiPhZmjxf0pg0fwwN+vaPiOkR0RkRnR0dHT0N08zMGijS1LNfbnQI0MmSpp9my4msj58HI+LbuVlXA5OAaen9qp4EbGZmfVPkqp58v/xvkF2CObHActuT3fF7n6S5adpXyBL+5ZImA0+RPdjFzMxapEgbf6/65Y+IW8gezl7Prr0p08zM+q7Zoxe/1mS5iIhTS4jHzMxK1uyI/9U601YDJgNvA5z4zcwGoWaPXjyzazjdeXsccARwKXBmo+XMzGxga9rGn/rV+QJwMNldtlu7Xx0zs8GtWRv/GcB+wHRgy4h4pWVRmZlZaZrdwPVFYB3gJOBZSYvSa7GkRa0Jz8zM+luzNv4e39VrZmYDn5O7mVnFOPGbmVWME7+ZWcU48ZuZVYwTv5lZxRTpndN6adzUa9tS7xPT9mpLvWY2OPiI38ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKsaJ38ysYpz4zcwqprTEL+k8SQsk3Z+bNlLSjZIeSe9rlVW/mZnVV+YR/wXAHjXTpgIzI2I8MDONm5lZC5WW+CPi18ALNZMnkj27l/S+b1n1m5lZfa1u4x8dEfMA0vuoRh+UNEXSHElzFi5c2LIAzcyWdwP25G5ETI+Izojo7OjoaHc4ZmbLjVYn/vmSxgCk9wUtrt/MrPJanfivBial4UnAVS2u38ys8sq8nPMS4PfAppKekTQZmAbsJukRYLc0bmZmLVTag1gi4qAGs3Ytq04zM+vegD25a2Zm5XDiNzOrGCd+M7OK8cPWl0N+yLuZNeMjfjOzinHiNzOrGCd+M7OKceI3M6sYJ34zs4px4jczqxgnfjOzinHiNzOrGCd+M7OKceI3M6sYJ34zs4pxXz3Wb9rVRxC4nyCznvARv5lZxTjxm5lVjJt6bLlQxa6oq7jO1j98xG9mVjFO/GZmFdOWph5JewBnAUOBcyNiWjviMOurdl7JZK2zvF2x1vIjfklDge8DewKbAQdJ2qzVcZiZVVU7mnq2BR6NiMcj4u/ApcDENsRhZlZJ7WjqWRd4Ojf+DPC+2g9JmgJMSaOvSHq4l/WtDTzXy2XL5Lh6xnH1TGlx6fQ+LV657dVXOr1PsW1Qb2I7Er/qTItlJkRMB6b3uTJpTkR09rWc/ua4esZx9Yzj6pmBGheUE1s7mnqeAdbPja8HPNuGOMzMKqkdif8OYLykDSWtCBwIXN2GOMzMKqnlTT0R8YakzwLXk13OeV5EPFBilX1uLiqJ4+oZx9UzjqtnBmpcUEJsilimed3MzJZjvnPXzKxinPjNzCpmUCV+SedJWiDp/ty0kZJulPRIel8rN+9ESY9KeljShxuU2XD5MuKStJukOyXdl953aVDmKZL+LGluen2k5LjGSfprrr6zG5TZ6u11cC6muZLekjShTpllba/9JT2Q6u2s+Xw796+6cQ2A/atRXO3evxrF1e796wxJD0m6V9KVktbMzStv/4qIQfMCdgS2Bu7PTftPYGoangqcnoY3A+4BVgI2BB4DhtYps+7yJcb1HmCdNLwF8OcGZZ4CfKmF22tc/nNNymzp9qpZbkvg8RZvr3cBmwKzgM7c9HbvX43iavf+1Siudu9fdeMaAPvX7sCwNHw6Lcpfg+qIPyJ+DbxQM3kiMCMNzwD2zU2/NCJej4g/AY+SdRdRq9HypcQVEXdHRNd9Cw8AK0taqad19ndcPdDS7VXjIOCSntbXl7gi4sGIqHfXeFv3r0ZxtXv/arK9imrp9qrRjv3rhoh4I43eSnZfE5S8fw2qxN/A6IiYB5DeR6Xp9bqGWLcHy5cVV96/AndHxOsNyvhs+gl4Xm9+8vYirg0l3S1ptqR/6cXyZcXV5RM0/8MsY3s10u79q4h27F/NtHP/KqLd+9eRwHVpuNT9a3lI/I0U6hqiXSRtTvbT7ugGH/kBsDEwAZgHnFlySPOAsRHxHuALwMWSRpRcZ2GS3ge8FhH3N/hIq7eX96+e8f7VvP6vAm8AF3VNqvOxftu/lofEP1/SGID0viBNL9o1RKPly4oLSesBVwKHRcRj9RaOiPkR8WZEvAX8kPo/8/otrvST8vk0fCdZm+ImPVmvMuLKOZAmR2Mlbq9G2r1/NdTm/auuAbB/dadt+5ekScDewMGRGuspef9aHhL/1cCkNDwJuCo3/UBJK0naEBgP3N6D5UuJK521vxY4MSJ+22jhri8z+RjQ6Eikv+LqUPasBCRtRLa9Hi+6fFlxpXiGAPuTdeFdV4nbq5F27191DYD9q1F97d6/msXWtv1L2UOpTgD2iYjXcrPK3b/6cpa61S+y/8jzgH+Q/UecDLwNmAk8kt5H5j7/VbIji4eBPXPTzyWd2W+2fBlxAScBrwJzc69RdeL6MXAfcG/6cseUHNe/kp0MvAe4C/joQNhe6fM7AbfWKacV2+tjafh1YD5w/QDZv+rGNQD2r0ZxtXv/avY9tnP/epSsLb/ruzq7FfuXu2wwM6uY5aGpx8zMesCJ38ysYpz4zcwqxonfzKxinPjNzCrGid/aRlJIOjM3/iVJp/RT2RdI+nh/lNVNPftLelDSzf1Q1rmSNutjGePyvT+a1ePEb+30OrCfpLXbHUhe141GBU0GPhMRO/e13oj4VET8oa/lmHXHid/a6Q2y54l+vnZG7RG7pFfS+06pk6/LJf1R0rTUp/rtyvqg3zhXzIck/SZ9bu+0/NDUB/odqcOto3Pl3izpYrIbdWrjOSiVf7+k09O0rwE7AGdLOqPOMsfn6vl6mjZOWf/rM9L0KyStmubNktSZYrwg1XWfpM+n+RMk3aolfbd3PbPgvZLukfR74N9z9Tda1zGSfq2sX/n71bjDNFtOOfFbu30fOFjSGj1YZivgOLL+0w8FNomIbcnuaDwm97lxwAeBvciS88pkR+gvR8Q2wDbAUemWeMj6X/lqRCzV3CJpHbIOz3Yh66RrG0n7RsQ3gDlkfawcX7PM7mS32W+blnmvpB3T7E2B6RHxbmAR8Jma9ZsArBsRW0TElsD5afqFwAlpufuAk9P084FjI2K7mnIaresnye5cnZC25VysUpz4ra0iYhFZQju2B4vdERHzIutu+DHghjT9PrJk3+XyiHgrIh4h6xfmnWQPvjhM0lzgNrJb3senz98eWd/ntbYBZkXEwsj6Tr+I7KEazeyeXneTdVHwzlw9T8eSfnR+QvarIe9xYCNJ/y/15bIo/WNcMyJmp8/MAHasM/3HNTHUW9c7gCPS+ZQtI2JxN+tiy5lh7Q7ADPgOWXI8PzftDdKBiSQBK+bm5fuXfys3/hZL79O1/ZEEWXe3x0TE9fkZknYi6+Omnnpd5HZHwGkRcU5NPeMaxLVkJOJFSVsBHyZrujmAOs1huXoa9btSd11THDuS/RL6saQzIuLCpmtjyxUf8VvbRcQLwOVkTRNdngDem4YnAiv0ouj9JQ1J7f4bkXV2dZARSGEAAAExSURBVD3wb5JWAJC0iaTVuinnNuCDktZOJ34PAmZ3s8z1wJGSVk/1rCup6yEZYyV1NcscBNySXzCd7B4SET8F/gPYOiJeBl7MtccfCsyOiJeAlyV1/Wo4uCaGZdZV0gbAgoj4IfAjsscBWoX4iN8GijOBz+bGfwhcJel2sl4HGx2NN/MwWYIeDXw6Iv4m6Vyy5qC70i+JhXTzuLqImCfpROBmsqPoX0RE0+5vI+IGSe8Cfp9VwyvAIcCbwIPAJEnnkPWq+IOaxdcFzlfWXTDAiel9Etm5ilXJmoOOSNOPAM6T9BpZsu/SaF13Ao6X9I8U12HN1sWWP+6d06yFUlPPNRGxRZtDsQpzU4+ZWcX4iN/MrGJ8xG9mVjFO/GZmFePEb2ZWMU78ZmYV48RvZlYx/x+EEmtm3ayqlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = ax.hist(steepest_ascent_required_episodes)\n",
    "_ = ax.set(xlabel=\"Number of episodes\",\n",
    "           ylabel=\"Number of training runs\",\n",
    "           title=\"Number of episodes required to solve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Methods\n",
    "\n",
    "While steepest ascent hill-climbing is a significant improvement over the basic hill-climbing algorithm, steepest ascent throws away quite a lot of useful information by only keep track of the parameters of the \"best\" `HillClimber` policy function. Instead of throwing away the parameters of the other `HillClimber` agents, what if you keep track of the policy function parameters of the top-$k$ performing `HillClimber` agents. You could then average together the values of these $k$ sets of policy function parameters to produce an estimate of the \"best\" policy function parameters. Repeat as necessary until the environment is solved or you have reached a maximum desired number of iterations. This is the basic idea behind [cross-entropy methods](https://en.wikipedia.org/wiki/Cross-entropy_method). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_state_dicts(hillclimbers: typing.List[HillClimber]) -> typing.Dict[str, torch.Tensor]:\n",
    "    _folded_state_dicts = collections.defaultdict(lambda: [])\n",
    "    for hillclimber in hillclimbers:\n",
    "        for k, v in hillclimber._best_policy_state_dict.items():\n",
    "            _folded_state_dicts[k].append(v)\n",
    "    averaged_state_dict = {k: torch.stack(v, dim=0).mean(dim=0) for k, v in _folded_state_dicts.items()}\n",
    "    return averaged_state_dict\n",
    "\n",
    "\n",
    "def cross_entropy_method(env: gym.Env,\n",
    "                         hillclimber_kwargs: dict,\n",
    "                         target_score: float,\n",
    "                         k: int = 1,\n",
    "                         number_episodes: int = 2,\n",
    "                         number_hillclimbers: int = 5,\n",
    "                         number_iterations: int = 150,\n",
    "                         verbose: bool = False):\n",
    "    hillclimbers = [HillClimber(**hillclimber_kwargs) for _ in range(number_hillclimbers)]\n",
    "    scores = []\n",
    "    most_recent_scores = collections.deque(maxlen=100)\n",
    "    \n",
    "    for i in range(number_iterations):\n",
    "        # train each hillclimber for some number of episodes (this can be done in parallel!)\n",
    "        _ = [train(hillclimber, env, float(\"inf\"), number_episodes, verbose=verbose) for hillclimber in hillclimbers]\n",
    "        \n",
    "        # identify the best hillclimbers\n",
    "        best_hillclimbers = top_k_hillclimbers(hillclimbers, k)\n",
    "        \n",
    "        # average the policies of the best hillclimbers to get the best overall policy\n",
    "        best_policy_state_dict = average_state_dicts(best_hillclimbers)\n",
    "        \n",
    "        # evaluate the best overall policy\n",
    "        best_hillclimber = HillClimber(**hillclimber_kwargs)\n",
    "        _ = best_hillclimber.load_state_dict(best_policy_state_dict)\n",
    "        score, = train(best_hillclimber, env, float(\"inf\"), 1)\n",
    "        scores.append(score)\n",
    "        most_recent_scores.append(score)\n",
    "        \n",
    "        # if not solved, then synchronize the hill climbers and repeat\n",
    "        average_score = sum(most_recent_scores) / len(most_recent_scores)\n",
    "        if len(most_recent_scores) >= 100 and average_score >= target_score:\n",
    "            if verbose:\n",
    "                print(f\"\\nEnvironment solved in {i:d} iterations!\\tAverage Score: {average_score:.2f}\")\n",
    "            break\n",
    "        else:\n",
    "            synchronize_hillclimbers(hillclimbers, best_policy_state_dict)\n",
    "    \n",
    "    return scores, best_hillclimber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy algorithm with `k=1` reduces to steepest ascent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment solved in 125 iterations!\tAverage Score: 195.44\n"
     ]
    }
   ],
   "source": [
    "_seed = None\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(_seed)\n",
    "\n",
    "_policy_fn = make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES)\n",
    "\n",
    "_hillclimber_kwargs = {\n",
    "    \"policy_fn\": _policy_fn,\n",
    "    \"gamma\": 1.0,\n",
    "    \"is_deterministic\": False,\n",
    "    \"preprocessing_fn\": None,\n",
    "    \"seed\": _seed,\n",
    "    \"sigma\": 1.0\n",
    "}\n",
    "\n",
    "scores, best_hillclimber = cross_entropy_method(env, _hillclimber_kwargs, target_score=195.0, k=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment solved in 139 iterations!\tAverage Score: 195.48\n"
     ]
    }
   ],
   "source": [
    "_seed = None\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(_seed)\n",
    "\n",
    "_policy_fn = make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES)\n",
    "\n",
    "_hillclimber_kwargs = {\n",
    "    \"policy_fn\": _policy_fn,\n",
    "    \"gamma\": 1.0,\n",
    "    \"is_deterministic\": False,\n",
    "    \"preprocessing_fn\": None,\n",
    "    \"seed\": _seed,\n",
    "    \"sigma\": 1.0\n",
    "}\n",
    "\n",
    "scores, best_hillclimber = cross_entropy_method(env, _hillclimber_kwargs, target_score=195.0, k=3, number_hillclimbers=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-8860991c72f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcross_entropy_required_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hillclimber_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m195.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_hillclimbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mcross_entropy_required_episodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-31d0f2b4adcc>\u001b[0m in \u001b[0;36mcross_entropy_method\u001b[0;34m(env, hillclimber_kwargs, target_score, k, number_episodes, number_hillclimbers, number_iterations, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# train each hillclimber for some number of episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhillclimber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhillclimber\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhillclimbers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# identify the best hillclimbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-31d0f2b4adcc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# train each hillclimber for some number of episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhillclimber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhillclimber\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhillclimbers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# identify the best hillclimbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-de6ca5ba5243>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, target_score, number_episodes, maximum_timesteps, verbose)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaximum_timesteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train_until_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train_for_at_most\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-de6ca5ba5243>\u001b[0m in \u001b[0;36m_train_until_done\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-9c54b4010e1a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGymState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocessing_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         sampling_probs = (self._policy(state_tensor)\n\u001b[0m\u001b[1;32m     45\u001b[0m                               \u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                               \u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_seed = None\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(_seed)\n",
    "\n",
    "_policy_fn = make_policy_fn(NUMBER_ACTIONS, NUMBER_STATES)\n",
    "\n",
    "_hillclimber_kwargs = {\n",
    "    \"policy_fn\": _policy_fn,\n",
    "    \"gamma\": 1.0,\n",
    "    \"is_deterministic\": False,\n",
    "    \"preprocessing_fn\": None,\n",
    "    \"seed\": None,\n",
    "    \"sigma\": 1.0\n",
    "}\n",
    "\n",
    "cross_entropy_required_episodes = []\n",
    "for t in range(100):\n",
    "    scores, _ = cross_entropy_method(env, _hillclimber_kwargs, target_score=195.0, k=10, number_hillclimbers=100, number_iterations=1000)\n",
    "    cross_entropy_required_episodes.append(len(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = ax.hist(cross_entropy_required_episodes)\n",
    "_ = ax.set(xlabel=\"Number of episodes\",\n",
    "           ylabel=\"Number of training runs\",\n",
    "           title=\"Number of episodes required to solve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "simulate(best_hillclimber, env, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary strategies\n",
    "\n",
    "OpenAI [blog post](https://openai.com/blog/evolution-strategies/) on evolutionary strategies. Link to paper https://arxiv.org/abs/1703.03864, https://arxiv.org/pdf/1703.03864.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolutionary_strategies(env: gym.Env,\n",
    "                            hillclimber_kwargs: dict,\n",
    "                            target_score: float,\n",
    "                            k: int = 1,\n",
    "                            number_episodes: int = 2,\n",
    "                            number_hillclimbers: int = 5,\n",
    "                            number_iterations: int = 150,\n",
    "                            verbose: bool = False):\n",
    "    hillclimbers = [HillClimber(**hillclimber_kwargs) for _ in range(number_hillclimbers)]\n",
    "    scores = []\n",
    "    most_recent_scores = collections.deque(maxlen=100)\n",
    "    \n",
    "    for i in range(number_iterations):\n",
    "        # train each hillclimber for some number of episodes\n",
    "        _ = [train(hillclimber, env, float(\"inf\"), number_episodes, verbose=verbose) for hillclimber in hillclimbers]\n",
    "        \n",
    "        current_scores = torch.Tensor([hillclimber.score for hillclimber in hillclimbers])\n",
    "        weights = current_scores / current_scores.sum() \n",
    "        \n",
    "        # identify the best hillclimbers\n",
    "        best_hillclimbers = top_k_hillclimbers(hillclimbers, k)\n",
    "        \n",
    "        # average the policies of the best hillclimbers to get the best overall policy\n",
    "        best_policy_state_dict = average_state_dicts(best_hillclimbers)\n",
    "        \n",
    "        # evaluate the best overall policy\n",
    "        best_hillclimber = HillClimber(**hillclimber_kwargs)\n",
    "        _ = best_hillclimber.load_state_dict(best_policy_state_dict)\n",
    "        score, = train(best_hillclimber, env, float(\"inf\"), 1)\n",
    "        scores.append(score)\n",
    "        most_recent_scores.append(score)\n",
    "        \n",
    "        # if not solved, then synchronize the hill climbers and repeat\n",
    "        average_score = sum(most_recent_scores) / len(most_recent_scores)\n",
    "        if len(most_recent_scores) >= 100 and average_score >= target_score:\n",
    "            if verbose:\n",
    "                print(f\"\\nEnvironment solved in {i:d} iterations!\\tAverage Score: {average_score:.2f}\")\n",
    "            break\n",
    "        else:\n",
    "            synchronize_hillclimbers(hillclimbers, best_policy_state_dict)\n",
    "    \n",
    "    return scores, best_hillclimber\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
