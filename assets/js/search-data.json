{
  
    
        "post0": {
            "title": "Getting familiar with deep Q-networks",
            "content": "Colab specific environment setup . If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment. . !pip install gym[box2d]==0.17.* . import collections import typing import gym import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch from torch import nn, optim from torch.nn import functional as F . %matplotlib inline . Motivation . I am currently using my COVID-19 imposed quarantine to expand my deep learning skills by completing the Deep Reinforcement Learning Nanodegree from Udacity. This past week I have been working my way through the seminal 2015 Nature paper from researchers at Deepmind Human-level control through deep reinforcement learning (Minh et al 2015). . Why is Minh et al 2015 important? . While Minh et al 2015 was not the first paper to use neural networks to approximate the action-value function, this paper was the first to demonstrate that the same neural network architecture could be trained in a computationally efficient manner to &quot;solve&quot; a large number or different tasks. . The paper also contributed several practical &quot;tricks&quot; for getting deep neural networks to consistently converge during training. This was a non-trivial contribution as issues with training convergence had plaugued previous attempts to use neural networks as function approximators in reinforcement learning tasks and were blocking widespread adoption of deep learning techniques within the reinforcemnt learning community. . Summary of the paper . Minh et al 2015 uses deep (convolutional) neural network to approximate the optimal action-value function . $$ Q^*(s, a) = max_{ pi} mathbb{E} Bigg[ sum_{s=0}^{ infty} gamma^s r_{t+s} | s_t=s, a_t=a, pi Bigg] $$ . which is the maximum sum of rewards $r_t$ discounted by $ gamma$ at each time-step $t$ achievable by a behaviour policy $ pi = P(a|s)$, after making an observation of the state $s$ and taking an action $a$. . Prior to this seminal paper it was well known that standard reinforcement learning algorithms were unstable or even diverged when a non-linear function approximators such as a neural networks were used to represent the action-value function $Q$. Why? . Minh et al 2015 discuss several reasons. . Correlations present in the sequence of observations of the state $s$. In reinforcement learning applications the sequence state observations is a time-series which will almost surely be auto-correlated. But surely this would also be true of any application of deep neural networks to model time series data. | Small updates to $Q$ may significantly change the policy, $ pi$ and therefore change the data distribution. | Correlations between the action-values, $Q$, and the target values $r + gamma max_{a&#39;} Q(s&#39;, a&#39;)$ | In the paper the authors address these issues by using... . a biologically inspired mechanism they refer to as experience replay that randomizes over the data which removes correlations in the sequence of observations of the state $s$ and smoothes over changes in the data distribution (issues 1 and 2 above). | an iterative update rule that adjusts the action-values, $Q$, towards target values, $Q&#39;$ that are only periodically updated thereby reducing correlations with the target (issue 3 above). | . Approximating the action-value function, $Q(s,a)$ . There are several possible ways of approximating the action-value function $Q$ using a neural network. The only input to the DQN architecture is the state representation and the output layer has a separate output for each possible action. The output units correspond to the predicted $Q$-values of the individual actions for the input state. A representaion of the DQN architecture from the paper is reproduced in the figure below. . . The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map $ phi$. The first hidden layer combines a convolutional layer with 32 filters (each of which uses an 8 x 8 kernel and a stride of 4) and a ReLU activation function. The second hidden layer convolves 64 filters (each of which using a 4 x 4 kernel with stride of 2) followed by a ReLU activation function. This is followed by a third convolutional layer that combines 64 filters (each of which uses a 3 x 3 kernel and a stride of 1) with yet anotehr ReLU activation. The final hidden layer is fully-connected (i.e., dense) linear layer with 512 neurons followed by a ReLU activation. The output layer is a fully-connected layer with a single output for each action. . A PyTorch implementation of the DQN architecture would look something like the following. . def deep_q_network(in_channels: int, action_size: int) -&gt; nn.Module: deep_q_network = nn.Sequential( nn.Conv2d(in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1), nn.ReLU(), nn.Linear(in_features=64, out_features=512), nn.ReLU(), nn.Linear(in_features=512, out_features=action_size) ) return deep_q_network . The Loss Function . The $Q$-learning update at iteration $i$ uses the following loss function . $$ mathcal{L_i}( theta_i) = mathbb{E}_{(s, a, r, s&#39;) sim U(D)} Bigg[ bigg(r + gamma max_{a&#39;} Q big(s&#39;, a&#39;; theta_i^{-} big) - Q big(s, a; theta_i big) bigg)^2 Bigg] $$ . where $ gamma$ is the discount factor determining the agent’s horizon, $ theta_i$ are the parameters of the $Q$-network at iteration $i$ and $ theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $ theta_i^{-}$ are only updated with the $Q$-network parameters $ theta_i$ every $C$ steps and are held fixed between individual updates. . Experience Replay . To perform experience replay the authors store the agent&#39;s experiences $e_t$ as represented by the tuple . $$ e_t = (s_t, a_t, r_t, s_{t+1}) $$ . consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences. . $$ D_t = {e1, e2, ..., e_t } $$ . Depending on the task it may note be feasible for the agent to store the entire history of past experiences. . During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s&#39;)$, drawn uniformly at random from the pool of stored samples $D_t$. . The following is my Python implmentation of these ideas. . _field_names = [ &quot;state&quot;, &quot;action&quot;, &quot;reward&quot;, &quot;next_state&quot;, &quot;done&quot; ] Experience = collections.namedtuple(&quot;Experience&quot;, field_names=_field_names) class ExperienceReplayBuffer: &quot;&quot;&quot;Fixed-size buffer to store experience tuples.&quot;&quot;&quot; def __init__(self, batch_size: int, buffer_size: int = None, random_state: np.random.RandomState = None) -&gt; None: &quot;&quot;&quot; Initialize an ExperienceReplayBuffer object. Parameters: -- buffer_size (int): maximum size of buffer batch_size (int): size of each training batch seed (int): random seed &quot;&quot;&quot; self._batch_size = batch_size self._buffer_size = buffer_size self._buffer = collections.deque(maxlen=buffer_size) self._random_state = np.random.RandomState() if random_state is None else random_state def __len__(self) -&gt; int: return len(self._buffer) @property def batch_size(self) -&gt; int: return self._batch_size @property def buffer_size(self) -&gt; int: return self._buffer_size def is_full(self) -&gt; bool: return len(self._buffer) == self._buffer_size def append(self, experience: Experience) -&gt; None: &quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot; self._buffer.append(experience) def sample(self) -&gt; typing.List[Experience]: &quot;&quot;&quot;Randomly sample a batch of experiences from memory.&quot;&quot;&quot; idxs = self._random_state.randint(len(self._buffer), size=self._batch_size) experiences = [self._buffer[idx] for idx in idxs] return experiences . The Deep Q-Network Algorithm . The following is Python pseudo-code for the Deep Q-Network (DQN) algorithm. For more fine-grained details of the DQN algorithm see the methods section of Minh et al 2015. . # hyper-parameters batch_size = 32 # number of experience tuples used in computing the gradient descent parameter update. buffer_size = 10000 # number of experience tuples stored in the replay buffer gamma = 0.99 # discount factor used in the Q-learning update target_network_update_frequency = 4 # frequency (measured in parameter updates) with which target network is updated. update_frequency = 4 # frequency (measured in number of timesteps) with which q-network parameters are updated. # initilizing the various data structures replay_buffer = ExperienceReplayBuffer(batch_size, buffer_size, seed) local_q_network = initialize_q_network() target_q_network = initialize_q_network() synchronize_q_networks(target_q_network, local_q_network) for i in range(number_episodes) # initialize the environment state state = env.reset() # simulate a single training episode done = False timesteps = 0 parameter_updates = 0 while not done: # greedy action based on Q(s, a; theta) action = agent.choose_epsilon_greedy_action(state) # update the environment based on the chosen action next_state, reward, done = env.step(action) # agent records experience in its replay buffer experience = (state, action, reward, next_state, done) agent.replay_buffer.append(experience) # agent samples a mini-batch of experiences from its replay buffer experiences = agent.replay_buffer.sample() states, actions, rewards, next_states, dones = experiences # agent learns every update_frequency timesteps if timesteps % update_frequency == 0: # compute the Q^- values using the Q-learning formula target_q_values = q_learning_update(target_q_network, rewards, next_states, dones) # compute the Q values local_q_values = local_q_network(states, actions) # agent updates the parameters theta using gradient descent loss = mean_squared_error(target_q_values, local_q_values) gradient_descent_update(loss) parameter_updates += 1 # every target_network_update_frequency timesteps set theta^- = theta if parameter_updates % target_network_update_frequency == 0: synchronize_q_networks(target_q_network, local_q_network) . Solving the LunarLander-v2 environment . In the rest of this blog post I will use the DQN algorithm to train an agent to solve the LunarLander-v2 environment from OpenAI. . In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land &quot;gracefully&quot; (and not crash in the landing area!). . A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered &quot;solved&quot; if the lander is able to achieve 200 points (I will actually be more stringent and define &quot;solved&quot; as achieving over 200 points on average in the most recent 100 training episodes). . Action Space . There are four discrete actions available: . Do nothing. | Fire the left orientation engine. | Fire main engine. | Fire the right orientation engine. | env = gym.make(&#39;LunarLander-v2&#39;) _ = env.seed(42) . /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32 warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;)) . Defining a generic Agent and train loop . In the cell below I define a fairly generic training loop for training and Agent to solve a task in a given gym.Env environment. In working through the hands-on portions of the Udacity Deep Reinforcement Learning Nanodegree I found myself writing similar code over and over again to train the agent to solve a task. This is my first attempt to write something that I might be able to reuse on the course going forward. . class Agent: def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot; raise NotImplementedError def save(self, filepath) -&gt; None: &quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot; raise NotImplementedError def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot;Update agent&#39;s state after observing the effect of its action on the environment.&quot;&quot;&quot; raise NotImplmentedError . def _train_for_at_most(agent: Agent, env: gym.Env, max_timesteps: int) -&gt; int: &quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot; state = env.reset() score = 0 for t in range(max_timesteps): action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward if done: break return score def _train_until_done(agent: Agent, env: gym.Env) -&gt; float: &quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot; state = env.reset() score = 0 done = False while not done: action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward return score def train(agent: Agent, env: gym.Env, checkpoint_filepath: str, target_score: float, number_episodes: int, maximum_timesteps=None) -&gt; typing.List[float]: &quot;&quot;&quot; Reinforcement learning training loop. Parameters: -- agent (Agent): an agent to train. env (gym.Env): an environment in which to train the agent. checkpoint_filepath (str): filepath used to save the state of the trained agent. number_episodes (int): maximum number of training episodes. maximum_timsteps (int): maximum number of timesteps per episode. Returns: -- scores (list): collection of episode scores from training. &quot;&quot;&quot; scores = [] most_recent_scores = collections.deque(maxlen=100) for i in range(number_episodes): if maximum_timesteps is None: score = _train_until_done(agent, env) else: score = _train_for_at_most(agent, env, maximum_timesteps) scores.append(score) most_recent_scores.append(score) average_score = sum(most_recent_scores) / len(most_recent_scores) if average_score &gt;= target_score: print(f&quot; nEnvironment solved in {i:d} episodes! tAverage Score: {average_score:.2f}&quot;) agent.save(checkpoint_filepath) break if (i + 1) % 100 == 0: print(f&quot; rEpisode {i + 1} tAverage Score: {average_score:.2f}&quot;) return scores . Creating a DeepQAgent . The code in the cell below encapsulates much of the logic of the DQN algorithm in a DeepQAgent class. Since the LunarLander-v2 task is not well suited for convolutional neural networks, the agent uses a simple three layer dense neural network with ReLU activation functions to approximate the action-value function $Q$. . class DeepQAgent(Agent): def __init__(self, state_size: int, action_size: int, number_hidden_units: int, optimizer_fn: typing.Callable[[typing.Iterable[torch.nn.Parameter]], optim.Optimizer], batch_size: int, buffer_size: int, epsilon_decay_schedule: typing.Callable[[int], float], alpha: float, gamma: float, update_frequency: int, seed: int = None) -&gt; None: &quot;&quot;&quot; Initialize a DeepQAgent. Parameters: -- state_size (int): the size of the state space. action_size (int): the size of the action space. number_hidden_units (int): number of units in the hidden layers. optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer. batch_size (int): number of experience tuples in each mini-batch. buffer_size (int): maximum number of experience tuples stored in the replay buffer. epsilon_decay_schdule (callable): function that takes episode number and returns epsilon. alpha (float): rate at which the target q-network parameters are updated. gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1). update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated. seed (int): random seed &quot;&quot;&quot; self._state_size = state_size self._action_size = action_size self._device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # set seeds for reproducibility self._random_state = np.random.RandomState() if seed is None else np.random.RandomState(seed) if seed is not None: torch.manual_seed(seed) if torch.cuda.is_available(): torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # initialize agent hyperparameters self._experience_replay_buffer = ExperienceReplayBuffer(batch_size, buffer_size, seed) self._epsilon_decay_schedule = epsilon_decay_schedule self._alpha = alpha self._gamma = gamma # initialize Q-Networks self._update_frequency = update_frequency self._local_q_network = self._initialize_q_network(number_hidden_units) self._target_q_network = self._initialize_q_network(number_hidden_units) self._synchronize_q_networks() # send the networks to the device self._local_q_network.to(self._device) self._target_q_network.to(self._device) # initialize the optimizer self._optimizer = optimizer_fn(self._local_q_network.parameters()) # initialize some counters self._number_episodes = 0 self._number_timesteps = 0 self._number_parameter_updates = 0 def _initialize_q_network(self, number_hidden_units: int) -&gt; nn.Module: &quot;&quot;&quot;Create a neural network for approximating the action-value function.&quot;&quot;&quot; q_network = nn.Sequential( nn.Linear(in_features=self._state_size, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=self._action_size) ) return q_network def _learn_from(self, experiences: typing.List[Experience]) -&gt; None: &quot;&quot;&quot;Heart of the Deep Q-learning algorithm.&quot;&quot;&quot; states, actions, rewards, next_states, dones = (torch.Tensor(vs).to(self._device) for vs in zip(*experiences)) # get max predicted Q values (for next states) from target model next_target_q_values, _ = (self._target_q_network(next_states) .detach() .max(dim=1)) # compute the new Q&#39; values using the Q-learning formula target_q_values = rewards + (self._gamma * next_target_q_values * (1 - dones)) # get expected Q values from local model _index = (actions.long() .unsqueeze(dim=1)) expected_q_values = (self._local_q_network(states) .gather(dim=1, index=_index)) # compute the mean squared loss loss = F.mse_loss(expected_q_values, target_q_values.unsqueeze(dim=1)) # agent updates the parameters theta of Q using gradient descent self._optimizer.zero_grad() loss.backward() self._optimizer.step() self._soft_update_target_q_network_parameters() def _soft_update_target_q_network_parameters(self) -&gt; None: &quot;&quot;&quot;Soft-update of target q-network parameters with the local q-network parameters.&quot;&quot;&quot; for target_param, local_param in zip(self._target_q_network.parameters(), self._local_q_network.parameters()): target_param.data.copy_(self._alpha * local_param.data + (1 - self._alpha) * target_param.data) def _synchronize_q_networks(self) -&gt; None: &quot;&quot;&quot;Synchronize the target_q_network and the local_q_network.&quot;&quot;&quot; _ = self._target_q_network.load_state_dict(self._local_q_network.state_dict()) def _uniform_random_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot; return self._random_state.randint(self._action_size) def _greedy_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action that maximizes the action_values given the current state.&quot;&quot;&quot; # evaluate the network to compute the action values self._local_q_network.eval() with torch.no_grad(): action_values = self._local_q_network(state) self._local_q_network.train() # choose the greedy action action = (action_values.cpu() # action_values might reside on the GPU! .argmax() .item()) return action def _epsilon_greedy_policy(self, state: torch.Tensor, epsilon: float) -&gt; int: &quot;&quot;&quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&quot;&quot;&quot; if self._random_state.random() &lt; epsilon: action = self._uniform_random_policy(state) else: action = self._greedy_policy(state) return action def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot; Return the action for given state as per current policy. Parameters: -- state (np.array): current state of the environment. Return: -- action (int): an integer representing the chosen action. &quot;&quot;&quot; # need to reshape state array and convert to tensor state_tensor = (torch.from_numpy(state) .unsqueeze(dim=0) .to(self._device)) # choose uniform at random if agent has insufficient experience if not self.has_sufficient_experience(): action = self._uniform_random_policy(state_tensor) else: epsilon = self._epsilon_decay_schedule(self._number_episodes) action = self._epsilon_greedy_policy(state_tensor, epsilon) return action def has_sufficient_experience(self) -&gt; bool: &quot;&quot;&quot;True if agent has enough experience to train on a batch of samples; False otherwise.&quot;&quot;&quot; return len(self._experience_replay_buffer) &gt;= self._experience_replay_buffer.batch_size def save(self, filepath: str) -&gt; None: &quot;&quot;&quot; Saves the state of the DeepQAgent. Parameters: -- filepath (str): filepath where the serialized state should be saved. Notes: The method uses `torch.save` to serialize the state of the q-network, the optimizer, as well as the dictionary of agent hyperparameters. &quot;&quot;&quot; checkpoint = { &quot;q-network-state&quot;: self._local_q_network.state_dict(), &quot;optimizer-state&quot;: self._optimizer.state_dict(), &quot;agent-hyperparameters&quot;: { &quot;alpha&quot;: self._alpha, &quot;batch_size&quot;: self._experience_replay_buffer.batch_size, &quot;buffer_size&quot;: self._experience_replay_buffer.buffer_size, &quot;gamma&quot;: self._gamma, &quot;update_frequency&quot;: self._update_frequency } } torch.save(checkpoint, filepath) def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot; Updates the agent&#39;s state based on feedback received from the environment. Parameters: -- state (np.array): the previous state of the environment. action (int): the action taken by the agent in the previous state. reward (float): the reward received from the environment. next_state (np.array): the resulting state of the environment following the action. done (bool): True is the training episode is finised; false otherwise. &quot;&quot;&quot; # save experience in the experience replay buffer experience = Experience(state, action, reward, next_state, done) self._experience_replay_buffer.append(experience) if done: self._number_episodes += 1 else: self._number_timesteps += 1 # every so often the agent should learn from experiences if self._number_timesteps % self._update_frequency == 0 and self.has_sufficient_experience(): experiences = self._experience_replay_buffer.sample() self._learn_from(experiences) . Epsilon decay schedule . In the DQN algorithm the agent chooses its action using an $ epsilon$-greedy policy. When using an $ epsilon$-greedy policy, with probability $ epsilon$, the agent explores the state space by choosing an action uniformly at random from the set of feasible actions; with probability $1- epsilon$, the agent exploits its current knowledge by choosing the optimal action given that current state. . As the agent learns and acquires additional knowledge about it environment it makes sense to decrease exploration and increase exploitation by decreasing $ epsilon$. In practice, it isn&#39;t a good idea to decrease $ epsilon$ to zero; instead one typically decreases $ epsilon$ over time according to some schedule until it reaches some minimum value. . The Deepmind researchers used a simple linear decay schedule and set a minimum value of $ epsilon=0.1$. In the cell below I code up a linear decay schedule as well as a power decay schedule that I have seen used in many other practical applications. . def linear_decay_schedule(episode_number: int, slope: float, minimum_epsilon: float) -&gt; float: &quot;&quot;&quot;Simple linear decay schedule used in the Deepmind paper.&quot;&quot;&quot; return max(1 - slope * episode_number, minimum_epsilon) def power_decay_schedule(episode_number: int, decay_factor: float, minimum_epsilon: float) -&gt; float: &quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot; return max(decay_factor**episode_number, minimum_epsilon) _epsilon_decay_schedule_kwargs = { &quot;decay_factor&quot;: 0.995, &quot;minimum_epsilon&quot;: 1e-2, } epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs) . Choosing an optimizer . As is the case in training any neural network, the choice of optimizer and the tuning of its hyper-parameters (in particular the learning rate) is important. Here I am going to more or less follow the Minh et al 2015 paper and use the RMSProp optimizer. . _optimizer_kwargs = { &quot;lr&quot;: 1e-2, &quot;alpha&quot;: 0.99, &quot;eps&quot;: 1e-08, &quot;weight_decay&quot;: 0, &quot;momentum&quot;: 0, &quot;centered&quot;: False } optimizer_fn = lambda parameters: optim.RMSprop(parameters, **_optimizer_kwargs) . At this point I am ready to create an instance of the DeepQAgent. . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;alpha&quot;: 1e-3, &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;seed&quot;: None, } deep_q_agent = DeepQAgent(**_agent_kwargs) . Training the DeepQAgent . Now I am finally ready to train the deep_q_agent. The target score for the LunarLander-v2 environment is 200 points on average for at least 100 consecutive episodes. If the deep_q_agent is able to &quot;solve&quot; the environment, then training will terminate early. . scores = train(deep_q_agent, env, &quot;checkpoint.pth&quot;, number_episodes=2000, target_score=200) . Episode 100 Average Score: -174.38 Episode 200 Average Score: -101.55 Episode 300 Average Score: -110.48 Episode 400 Average Score: -62.51 Episode 500 Average Score: -28.77 Episode 600 Average Score: 56.29 Episode 700 Average Score: 132.25 Episode 800 Average Score: 102.05 Episode 900 Average Score: 88.39 Episode 1000 Average Score: 103.97 Episode 1100 Average Score: 148.96 Episode 1200 Average Score: 167.50 Episode 1300 Average Score: 175.88 Episode 1400 Average Score: 156.95 Episode 1500 Average Score: 183.58 Episode 1600 Average Score: 184.38 Episode 1700 Average Score: 142.57 Episode 1800 Average Score: 155.47 Environment solved in 1886 episodes! Average Score: 202.91 . Analyzing DeepQAgent performance . Plotting the time series of scores . I can use Pandas to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score. . scores = pd.Series(scores, name=&quot;scores&quot;) . scores.describe() . count 1887.000000 mean 79.280025 std 182.558296 min -629.814823 25% -57.980604 50% 62.985922 75% 251.261991 max 314.992618 Name: scores, dtype: float64 . fig, ax = plt.subplots(1, 1) _ = scores.plot(ax=ax, label=&quot;Scores&quot;) _ = (scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=ax)) ax.axhline(200, color=&#39;k&#39;, linestyle=&quot;dashed&quot;, label=&quot;Target Score&quot;) ax.legend() _ = ax.set_xlabel(&quot;Episode Number&quot;) _ = ax.set_ylabel(&quot;Score&quot;) . Kernel density plot of the scores . Kernel density plot of scores is bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent &quot;solved&quot; the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score). . fig, ax = plt.subplots(1,1) _ = scores.plot(kind=&quot;kde&quot;, ax=ax) _ = ax.set_xlabel(&quot;Score&quot;) . Where to go from here? . I am a bit frustrated by lack of stability that I am seeing in my implmentation of the Deep Q algorithm: sometimes the algorithm converges and sometimes not. Perhaps more tuning of hyper-parameters or use of a different optimization algorithm would exhibit better convergence. I have already spent more time than I had allocated on playing around with this agorithm so I am not going to try and fine-tune the hyperparamters or explore alternative optimization algorithms for now. . Rather than spending time tuning hyperparameters I think it would be better use of my time to explore algorithmic improvements. In future posts I plan to cover the following extensions of the DQN algorithm: Double Q-Learning, Prioritized Experience Replay, and Dueling Network Architectures .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html",
            "relUrl": "/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Conda (+ pip) and Docker FTW!",
            "content": "Conda (+ pip) and Docker FTW! . Conda is an open source package and environment management system that runs on Windows, Mac OS and Linux. . Conda can quickly install, run, and update packages and their dependencies. | Conda can create, save, load, and switch between project specific software environments on your local computer. | Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN. | . Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. . While Conda is my default package and environment management solution, not every Python package that I might need to use is available via Conda. Fortunately, Conda plays nicely with pip which is the default Python package management tool. . Why not just use Conda (+ pip)? . While Conda (+ pip) solves most of my day-to-day data science environment and package management issues, incorporating Docker into my Conda (+ pip) development workflow has made it much easier to port my data science workflows from from my laptop/workstation to remote cloud computing resources. Getting Conda (+ pip) to work as expected inside Docker containers turned out to be much more challenging that I expected. . This blog post shows how I eventually combined Conda (+ pip) and Docker. In the following I assume that you have organized your project directory similar to my Python data science project template. In particular, I will assume that you store all Docker related files in a docker sub-directory within your project root directory. . Writing the Dockerfile . The trick to getting Conda (+ pip) and Docker to work smoothly together is to write a good Dockerfile. In this section I will take you step by step through the various pieces of the Dockerfile that I developed. Hopefully you can use this Dockerfile without modification on you next data science project. . Use a standard base image . Every Dockefile has a base or parent image. For the parent image I use Ubuntu 16.04 which is one of the most commonly used flavor of Linux in the data science community (and also happens to be the same OS installed on my workstation). . FROM ubuntu:16.04 . Make bash the default shell . The default shell used to run Dockerfile commands when building Docker images is /bin/sh. Unfortunately /bin/sh is currently not one of the shells supported by the conda init command. Fortunately it is possible to change the default shell used to run Dockerfile commands using the SHELL instruction. . SHELL [ &quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot; ] . Note the use of the --login flag which insures that both ~/.profile and ~/.bashrc are sourced properly. Proper sourcing of both ~/.profile and ~/.bashrc is necessary in order to use various conda commands to build the Conda environment inside the Docker image. . Create a non-root user . It is a Docker security “best practice” to create a non-root user inside your Docker images. My preferred approach to create a non-root user uses build arguments to customize the username, uid, and gidthe non-root user. I use standard defaults for the uid and gid; the default username is set to al-khawarizmi. . # Create a non-root user ARG username=al-khawarizmi ARG uid=1000 ARG gid=100 ENV USER $username ENV UID $uid ENV GID $gid ENV HOME /home/$USER RUN adduser --disabled-password --gecos &quot;Non-root user&quot; --uid $UID --gid $GID --home $HOME $USER . Copy over the config files for the Conda environment . After creating the non-root user I copy over all of the config files that I will need to create the Conda environment (i.e., environment.yml, requirements.txt, postBuild). I also copy over a Bash script that I will use as the Docker ENTRYPOINT (more on this below). . COPY environment.yml requirements.txt /tmp/ RUN chown $UID:$GID /tmp/environment.yml /tmp/requirements.txt COPY postBuild /usr/local/bin/postBuild.sh RUN chown $UID:$GID /usr/local/bin/postBuild.sh &amp;&amp; chmod u+x /usr/local/bin/postBuild.sh COPY docker/entrypoint.sh /usr/local/bin/ RUN chown $UID:$GID /usr/local/bin/entrypoint.sh &amp;&amp; chmod u+x /usr/local/bin/entrypoint.sh . Newer versions of Docker support copying files as a non-root user, however the version of Docker available on DockerHub does not yet support copying as a non-root user so if you want to setup automated builds for your Git repositories you will need to copy everything as root. . Install Miniconda as the non-root user. . After copying over the config files as root, I switch over to the non-root user and install Miniconda. . USER $USER # install miniconda ENV MINICONDA_VERSION 4.8.2 ENV CONDA_DIR $HOME/miniconda3 RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-$MINICONDA_VERSION-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; chmod +x ~/miniconda.sh &amp;&amp; ~/miniconda.sh -b -p $CONDA_DIR &amp;&amp; rm ~/miniconda.sh # make non-activate conda commands available ENV PATH=$CONDA_DIR/bin:$PATH # make conda activate command available from /bin/bash --login shells RUN echo &quot;. $CONDA_DIR/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.profile # make conda activate command available from /bin/bash --interative shells RUN conda init bash . Create a project directory . Next I create a project directory inside the non-root user home directory. The Conda environment will be created in a env sub-directory inside the project directory and all other project files and directories can then be mounted into this directory. . # create a project directory inside user home ENV PROJECT_DIR $HOME/app RUN mkdir $PROJECT_DIR WORKDIR $PROJECT_DIR . Build the Conda environment . Now I am ready to build the Conda environment. Note that I can use nearly the same sequence of conda commands that I would use to build a Conda environment for a project on my laptop or workstation. . # build the conda environment ENV ENV_PREFIX $PWD/env RUN conda update --name base --channel defaults conda &amp;&amp; conda env create --prefix $ENV_PREFIX --file /tmp/environment.yml --force &amp;&amp; conda clean --all --yes # run the postBuild script to install any JupyterLab extensions RUN conda activate $ENV_PREFIX &amp;&amp; /usr/local/bin/postBuild.sh &amp;&amp; conda deactivate . Insure Conda environment is properly activated at runtime . Almost finished! Second to last step is to use an ENTRYPOINT script to insure that the Conda environment is properly activated at runtime. . ENTRYPOINT [ &quot;/usr/local/bin/entrypoint.sh&quot; ] . Here is the /usr/local/bin/entrypoint.sh script for reference. . #!/bin/bash --login set -e conda activate $ENV_PREFIX exec &quot;$@&quot; . Specify a default command for the Docker container . Finally, I use the CMD instruction to specify a default command to run when a Docker container is launched. Since I install JupyerLab in all of my Conda environments I tend to launch a JupyterLab server by default when executing containers. . # default command will be to launch JupyterLab server for development CMD [ &quot;jupyter&quot;, &quot;lab&quot;, &quot;--no-browser&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot; ] . Building the Docker image . The following command builds a new image for your project with a custom $USER (and associated $UID and $GID) as well as a particular $IMAGE_NAME and $IMAGE_TAG. This command should be run within the docker sub-directory of the project as the Docker build context is set to ../ which should be the project root directory. . docker image build --build-arg username=$USER --build-arg uid=$UID --build-arg gid=$GID --file Dockerfile --tag $IMAGE_NAME:$IMAGE_TAG ../ . Running a Docker container . Once the image is built, the following command will run a container based on the image $IMAGE_NAME:$IMAGE_TAG. This command should be run from within the project’s root directory. . docker container run --rm --tty --volume ${pwd}/bin:/home/$USER/app/bin --volume ${pwd}/data:/home/$USER/app/data --volume ${pwd}/doc:/home/$USER/app/doc --volume ${pwd}/notebooks:/home/$USER/app/notebooks --volume ${pwd}/results:/home/$USER/app/results --volume ${pwd}/src:/home/$USER/app/src --publish 8888:8888 $IMAGE_NAME:$IMAGE_TAG . Using Docker Compose . It is quite easy to make typos whilst writing the above docker commands by hand. A less error-prone approach is to use Docker Compose. The above docker commands can be encapsulated into the docker-compose.yml configuration file as follows. . version: &quot;3.7&quot; services: jupyterlab-server: build: args: - username=${USER} - uid=${UID} - gid=${GID} context: ../ dockerfile: docker/Dockerfile ports: - &quot;8888:8888&quot; volumes: - ../bin:/home/${USER}/app/bin - ../data:/home/${USER}/app/data - ../doc:/home/${USER}/app/doc - ../notebooks:/home/${USER}/app/notebooks - ../results:/home/${USER}/app/results - ../src:/home/${USER}/app/src init: true stdin_open: true tty: true . The above docker-compose.yml file relies on variable substitution. to obtain the values for $USER, $UID, and $GID. These values can be stored in an a file called .env as follows. . USER=$USER UID=$UID GID=$GID . You can test your docker-compose.yml file by running the following command in the docker sub-directory of the project. . docker-compose config . This command takes the docker-compose.yml file and substitutes the values provided in the .env file and then returns the result. . Once you are confident that values in the .env file are being substituted properly into the docker-compose.yml file, the following command can be used to bring up a container based on your project’s Docker image and launch the JupyterLab server. This command should also be run from within the docker sub-directory of the project. . docker-compose up --build . When you are done developing and have shutdown the JupyterLab server, the following command tears down the networking infrastructure for the running container. . docker-compose down . Summary . In this post I walked through a Dockerfile that can be used to inject a Conda (+ pip) environment into into a Docker image. I also detailed how to build the resulting image and launch containers using Docker Compose. . If you are looking for a production-quality solution that generalizes the approach outlined above, then I would encourage you to check out jupyter-repo2docker. . jupyter-repo2docker is a tool to build, run, and push Docker images from source code repositories. repo2docker fetches a repository (from GitHub, GitLab, Zenodo, Figshare, Dataverse installations, a Git repository or a local directory) and builds a container image in which the code can be executed. The image build process is based on the configuration files found in the repository. . The Conda (+ pip) and Docker combination has significantly increased my data science development velocity while at the same time increasing the portability and reproducibility of my data science workflows. . Hopefully this post can help you combine these three great tools together on your next data science project! .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "relUrl": "/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Building a Conda environment for Horovod",
            "content": "What is Horovod? . Horovod is an open-source distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Horovod improves the speed, scale, and resource utilization of deep learning training. . In this post I describe how I build Conda environments for my deep learning projects where I plan to use Horovod to enable distributed training across multiple GPUs (either on the same node or spread across multuple nodes). If you like my approach then you can make use of the template repository on GitHub to get started with you rnext Horovod data science project! . Installing the NVIDIA CUDA Toolkit . First thing you need to do is to install the appropriate version of the NVIDIA CUDA Toolkit on your workstation. For this blog post I am using NVIDIA CUDA Toolkit 10.1 (documentation) which works with all three deep learning frameworks that are currently supported by Horovod. . Why not just use the cudatoolkit package? . Typically when installing PyTorch, TensorFlow, or Apache MXNet with GPU support using Conda you simply add the appropriate version cudatoolkit package to your environment.yml file. . Unfortunately, the cudatoolkit package available from conda-forge does not include NVCC and in order to use Horovod with either PyTorch, TensorFlow, or MXNet you need to compile extensions. . What about the cudatoolkit-dev package? . While there are cudatoolkit-dev packages available from conda-forge that do include NVCC, I have had difficult getting these packages to consistently install properly. . Use the nvcc_linux-64 meta-pacakge! . The most robust approach to obtain NVCC and still use Conda to manage all the other dependencies is to install the NVIDIA CUDA Toolkit on your system and then install a meta-package nvcc_linux-64 from conda-forge which configures your Conda environment to use the NVCC installed on the system together with the other CUDA Toolkit components installed inside the Conda environment. . The environment.yml file . I prefer to specify as many dependencies as possible in the Conda environment.yml file and only specify dependencies in requirements.txt that are not available via Conda channels. Check the official Horovod installation guide for details of required dependencies. . Channel Priority . I use the recommended channel priorities. Note that conda-forge has priority over defaults. . name: null channels: - pytorch - conda-forge - defaults . Dependencies . There are a few things worth noting about the dependencies. . Even though I have installed the NVIDIA CUDA Toolkit manually I still use Conda to manage the other required CUDA components such as cudnn and nccl (and the optional cupti). | I use two meta-pacakges, cxx-compiler and nvcc_linux-64, to make sure that suitable C, and C++ compilers are installed and that the resulting Conda environment is aware of the manually installed CUDA Toolkit. | Horovod requires some controller library to coordinate work between the various Horovod processes. Typically this will be some MPI implementation such as OpenMPI. However, rather than specifying the openmpi package directly I instead opt for mpi4py Conda package which provides a cuda-aware build of OpenMPI (where possible). | Horovod also support that Gloo collective communications library that can be used in place of MPI. I include cmake in order to insure that the Horovod extensions for Gloo are built. | Below are the core required dependencies. The complete environment.yml file is available on GitHub. . dependencies: - bokeh=1.4 - cmake=3.16 # insures that the Gloo library extensions will be built - cudnn=7.6 - cupti=10.1 - cxx-compiler=1.0 # meta-pacakge that insures suitable C and C++ compilers are available - jupyterlab=1.2 - mpi4py=3.0 # installs cuda-aware openmpi - nccl=2.5 - nodejs=13 - nvcc_linux-64=10.1 # meta-package that configures environment to be &quot;cuda-aware&quot; - pip=20.0 - pip: - mxnet-cu101mkl==1.6.* # makes sure MXNET is installed prior to horovod - -r file:requirements.txt - python=3.7 - pytorch=1.4 - tensorboard=2.1 - tensorflow-gpu=2.1 - torchvision=0.5 . The requirements.txt File . The requirements.txt file is where all of the pip dependencies, including Horovod itself, are listed for installation. In addition to Horovod I typically will also use pip to install JupyterLab extensions to enable GPU and CPU resource monitoring via jupyterlab-nvdashboard and Tensorboard support via jupyter-tensorboard. . horovod==0.19.* jupyterlab-nvdashboard==0.2.* # server-side component; client-side component installed in postBuild jupyter-tensorboard==0.2.* # make sure horovod is re-compiled if environment is re-built --no-binary=horovod . Note the use of the --no-binary option at the end of the file. Including this option insures that Horovod will be re-built whenever the Conda environment is re-built. . The complete requirements.txt file is available on GitHub. . Building Conda Environment . After adding any necessary dependencies that should be downloaded via conda to the environment.yml file and any dependencies that should be downloaded via pip to the requirements.txt file you create the Conda environment in a sub-directory ./envof your project directory by running the following commands. . export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force . By default Horovod will try and build extensions for all detected frameworks. See the Horovod documentation on environment variables for the details on additional environment variables that can be set prior to building Horovod. . Once the new environment has been created you can activate the environment with the following command. . conda activate $ENV_PREFIX . The postBuild File . If you wish to use any JupyterLab extensions included in the environment.yml and requirements.txt files then you need to rebuild the JupyterLab application using the following commands to source the postBuild script. . conda activate $ENV_PREFIX # optional if environment already active . postBuild . Wrapping it all up in a Bash script . I typically wrap these commands into a shell script ./bin/create-conda-env.sh. Running the shell script will set the Horovod build variables, create the Conda environment, activate the Conda environment, and built JupyterLab with any additional extensions. . #!/bin/bash --login set -e export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force conda activate $ENV_PREFIX . postBuild . I typically put scripts inside a ./bin directory in my project root directory. The script should be run from the project root directory as follows. . ./bin/create-conda-env.sh # assumes that $CUDA_HOME is set properly . Verifying the Conda environment . After building the Conda environment you can check that Horovod has been built with support for the deep learning frameworks TensorFlow, PyTorch, Apache MXNet, and the contollers MPI and Gloo with the following command. . conda activate $ENV_PREFIX # optional if environment already active horovodrun --check-build . You should see output similar to the following. . Horovod v0.19.1: Available Frameworks: [X] TensorFlow [X] PyTorch [X] MXNet Available Controllers: [X] MPI [X] Gloo Available Tensor Operations: [X] NCCL [ ] DDL [ ] CCL [X] MPI [X] Gloo . Listing the contents of the Conda environment . To see the full list of packages installed into the environment run the following command. . conda activate $ENV_PREFIX # optional if environment already active conda list . Updating the Conda environment . If you add (remove) dependencies to (from) the environment.yml file or the requirements.txt file after the environment has already been created, then you can re-create the environment with the following command. . conda env create --prefix $ENV_PREFIX --file environment.yml --force . However, whenever I add new dependencies I prefer to re-run the Bash script which will re-build both the Conda environment and JupyterLab. . ./bin/create-conda-env.sh . Summary . Finding a reproducible process for building Horovod extensions for my deep learning projects was tricky. Key to my solution is the use of meta-packages from conda-forge to insure that the appropriate compilers are installed and that the resulting Conda environment is aware of the system installed NVIDIA CUDA Toolkit. The second key is to use the --no-binary flag in the requirements.txt file to insure that Horovod is re-built whenever the Conda environment is re-built. . If you like my approach then you can make use of the template repository on GitHub to get started with your next Horovod data science project! .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "relUrl": "/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Managing Project-Specific Environments With Conda",
            "content": "Getting Started with Conda . Conda is an open source package and environment management system that runs on Windows, Mac OS and Linux. . Conda can quickly install, run, and update packages and their dependencies. | Conda can create, save, load, and switch between project specific software environments on your local computer. | Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN. | . Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. . Conda? Miniconda? Anaconda? What’s the difference? . Users are often confused about the differences between Conda, Miniconda, and Anaconda. . . I suggest installing Miniconda which combines Conda with Python 3 (and a small number of core systems packages) instead of the full Anaconda distribution. Installing only Miniconda will encourage you to create separate environments for each project (and to install only those packages that you actually need for each project!) which will enhance portability and reproducibility of your research and workflows. . Besides, if you really want a particular version of the full Anaconda distribution you can always create an new conda environment and install it using the following command. . conda create --name anaconda202002 anaconda=2020.02 . Installing Miniconda . Download the 64-bit, Python 3 version of the appropriate Miniconda installer for your operating system from and follow the instructions. I will walk through the steps for installing on Linux systems below as installing on Linux systems is slightly more involved. . Download the 64-bit Python 3 install script for Miniconda. . wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . Run the Miniconda install script. The -b runs the install script in batch mode which doesn’t require manual intervention (and assumes that the user agrees to the terms of the license). . bash Miniconda3-latest-Linux-x86_64.sh -b . Remove the install script. . rm Miniconda3-latest-Linux-x86_64 . Initializing your shell for Conda . After installing Miniconda you next need to configure your preferred shell to be “conda-aware”. . conda init bash source ~/.bashrc (base) $ # now the prompt indicates that the base environment is active! . Updating Conda . It is a good idea to keep your conda installation updated to the most recent version. . conda update --name base conda --yes . Uninstalling Miniconda . Whenever installing new software it is always a good idea to understand how to uninstall the software (just in case you have second thoughts!). Uninstalling Miniconda is fairly straighforward. . Uninitialize your shell to remove Conda related content from ~/.bashrc. . conda init --reverse bash . Remove the entire ~/miniconda3 directory. . rm -rf ~/miniconda3 . Remove the entire ~/.conda directory. . rm -rf ~/.conda . If present, remove your Conda configuration file. . rm ~/.condarc . Conda “Best Practices” . In the following section I detail a minimal set of best practices for using Conda to manage data science environments that I use in my own work. . TLDR; . Here is the basic recipe for using Conda to manage a project specific software stack. . (base) $ mkdir project-dir (base) $ cd project-dir (base) $ nano environment.yml # create the environment file (base) $ conda env create --prefix ./env --file environment.yml (base) $ conda activate ./env # activate the environment (/path/to/env) $ nano environment.yml # forgot to add some deps (/path/to/env) $ conda env update --prefix ./env --file environment.yml --prune) # update the environment (/path/to/env) $ conda deactivate # done working on project (for now!) . New project, new directory . Every new project (no matter how small!) should live in its own directory. A good reference to get started with organizing your project directory is Good Enough Practices for Scientific Computing. . mkdir project-dir cd project-dir . New project, new environment . Now that you have a new project directory you are ready to create a new environment for your project. We will do this in two steps. . Create an environment file that describes the software dependencies (including specific version numbers!) for the project. | Use the newly created environment file to build the software environment. | Here is an example of a typical environment file that could be used to run GPU accelerated, distributed training of deep learning models developed using PyTorch. . name: null channels: - pytorch - conda-forge - defaults dependencies: - cudatoolkit=10.1 - jupyterlab=1.2 - pip=20.0 - python=3.7 - pytorch=1.4 - torchvision=0.5 . Once you have created an environment.yml file inside your project directory you can use the following commands to create the environment as a sub-directory called env inside your project directory. . conda env create --prefix ./env --file environment.yml . Activating an environment . Activating environments is essential to making the software in environments work well (or sometimes at all!). Activation of an environment does two things. . Adds entries to PATH for the environment. | Runs any activation scripts that the environment may contain. | Step 2 is particularly important as activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. . conda activate ./env # activate the environment (/path/to/env) $ # now the prompt indicates which environment is active! . Updating an environment . You are unlikely to know ahead of time which packages (and version numbers!) you will need to use for your research project. For example it may be the case that… . one of your core dependencies just released a new version (dependency version number update). | you need an additional package for data analysis (add a new dependency). | you have found a better visualization package and no longer need to old visualization package (add new dependency and remove old dependency). | . If any of these occurs during the course of your research project, all you need to do is update the contents of your environment.yml file accordingly and then run the following command. . conda env update --prefix ./env --file environment.yml --prune # update the environment . Alternatively, you can simply rebuild the environment from scratch with the following command. . conda env create --prefix ./env --file environment.yml --force . Deactivating an environment . When you are done working on your project it is a good idea to deactivate the current environment. To deactivate the currently active environment use the deactivate command as follows. . conda deactivate # done working on project (for now!) (base) $ # now you are back to the base environment . Interested in Learning More? . For more details on using Conda to manage software stacks for you data science projects, checkout the Introduction to Conda for (Data) Scientists training materials that I have contributed to The Carpentries Incubator. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "relUrl": "/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "date": " • Mar 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}