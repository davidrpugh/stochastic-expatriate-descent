{
  
    
        "post0": {
            "title": "Dueling Network Architectures",
            "content": "I am continuing to work my way through the Udacity Deep Reinforcement Learning Nanodegree. In this blog post I discuss and implement an the dueling network architecture from Dueling Network Architectures for Deep Reinforcement Learning (Wang et al 2016). . Background . The overall setup in Wang et al 2016 is similar to previous work in the deep RL literature. The agent seeks maximize the expected discounted return, where we define the discounted return as . $$ R_t = sum_{s=0}^{ infty} gamma^s r_{t+s} $$ . where $ gamma in [0, 1]$ is a discount factor that determines how the RL agent should value immediate versus long-term rewards. For an RL agent behaving according to a stochastic policy $ pi$, the values of the state-action pair $(s, a)$ and the state $s$ are defined as follows. . begin{align} Q^{ pi}(s, a) =&amp; mathbb{E} big[R_t big| s_t = s, a_t = a, pi big] V^{ pi}(s) =&amp; mathbb{E}_{a sim pi(s)} big[Q^{ pi}(s, a) big] end{align}The preceding state-action value function, or Q function, can be computed recursively with dynamic programming. . $$ Q^{ pi}(s, a) = mathbb{E}_{s&#39;} bigg[r + gamma mathbb{E}_{a&#39; sim pi(s&#39;)} big[Q^{ pi}(s&#39;, a&#39;) big] bigg| s, a, pi bigg] $$ . Advantage Function . At this point, Wang et al 2016, begin to deviate a bit previous work from previous work by defining an advantage function which relates the $V$ and $Q$ functions as follows. . $$ A^{ pi}(s, a) = Q^{ pi}(s, a) − V^{ pi}(s) $$ . Note that assuming that the agent chooses its actions using policy $ pi$ it follows that . $$ mathbb{E}_{a sim pi(s)} big[A^{ pi}(s, a) big] = 0. $$ . Intuitively, the value function $V$ measures how &quot;good&quot; it is to be in a particular state $s$. The $Q$ function measures the the value of choosing a particular action when in state $s$. The advantage function, $A$, subtracts the value of being in state $s$ from the $Q$ function to obtain a relative measure of the importance of each action in state $s$. . Deep Q-Networks . Minh et al 2015 approximate the value functions defined above using deep neural networks which they refere to as deep Q-networks. In particular Minh et al 2015 parameterize the Q-function using $Q(s, a; theta)$ and then seek to find parameters $ theta$ of some neural network that minimize the follwing sequence of loss functions at iteration $i$: . $$ L_i( theta_i) = mathbb{E}_{s,a,r,s&#39;} bigg[y_i^{DQN} − Q(s, a; theta_i) bigg]^2 $$ . with . $$ y_i^{DQN} = r + gamma underset{a&#39;}{max} Q(s&#39;, a&#39;; theta^{-}) $$ . where $ theta^{-}$ represents the parameters of a separate, target network whose parameters, because of convergence issues, are only occasionally updated. If you are interested in more details about deep Q-networks check out my previous post (or even better read the original paper). . In the cell below I implement several useful type annotations and functions that come from Minh et al 2015 that will be used later in this post. . import torch from torch import nn States = torch.Tensor Actions = torch.Tensor Rewards = torch.Tensor Dones = torch.Tensor QNetwork = nn.Module QValues = torch.Tensor def synchronize_q_networks(q1: QNetwork, q2: QNetwork) -&gt; None: &quot;&quot;&quot;In place, synchronization of q1 and q2.&quot;&quot;&quot; _ = q1.load_state_dict(q2.state_dict()) def select_greedy_actions(states: torch.Tensor, q_network: QNetwork) -&gt; Actions: &quot;&quot;&quot;Select the greedy action for the current state given some Q-network.&quot;&quot;&quot; _, actions = q_network(states).max(dim=1, keepdim=True) return actions def evaluate_selected_actions(states: States, actions: Actions, rewards: Rewards, dones: Dones, gamma: float, q_network: QNetwork) -&gt; QValues: &quot;&quot;&quot;Compute the Q-values by evaluating the actions given the current states and Q-network.&quot;&quot;&quot; next_q_values = q_network(states).gather(dim=1, index=actions) q_values = rewards + (gamma * next_q_values * (1 - dones)) return q_values def q_learning_update(states: States, rewards: Rewards, dones: Dones, gamma: float, q_network: QNetwork) -&gt; QValues: &quot;&quot;&quot;Q-Learning uses a q-network to select and evaluate actions.&quot;&quot;&quot; actions = select_greedy_actions(states, q_network) q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q_network) return q_values . Double Deep Q-Networks . Van Hasselt et al (2015) combined double Q-learning and deep Q-networks to obtain a much improved algorithm called double deep Q-networks (DDQN). For more detailed discussion of the DDQN algorithm see either my previous blog post (or better yet read the original paper). . The DDQN algorithm uses the online Q-network parameterized by $ theta$ to choose actions but uses the target Q-network parameterized by $ theta^{-}$ to evaluated the chosen actions and compute the q-values. DDQN uses the following rule for computing the target. . $$ y_i^{DDQN} = r + gamma Q big(s&#39;, underset{a&#39;}{ mathrm{argmax}}Q(s&#39;, a&#39;; theta_i); theta^{-} big) $$ . In the cell below I provide my Python implementation of this update rule. . def double_q_learning_update(states: States, rewards: Rewards, dones: Dones, gamma: float, q1: QNetwork, q2: QNetwork) -&gt; torch.Tensor: &quot;&quot;&quot;Double Q-Learning uses q1 to select actions and q2 to evaluate the selected actions.&quot;&quot;&quot; actions = select_greedy_actions(states, q1) q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q2) return q_values . Prioritized Experience Replay . Schaul et al (2016) introduced prioritized experience replay which increased the replay probability of experience tuples that have a high expected learning progress (as measured via the proxy of absolute temporal difference (TD) error). For more details see my previous blog post (or better yet read the original paper). . In the cells below I provide and implementation of the TD errors for both Q and double Q-learning as well a an implementation of a PrioritizedExperienceReplayBuffer that encapsulates the key ideas from Schaul et al (2016). . TDErrors = torch.Tensor def q_learning_error(states: States, actions: Actions, rewards: Rewards, next_states: States, dones: Dones, gamma: float, q_network: QNetwork) -&gt; TDErrors: &quot;&quot;&quot;Q-learning temporal-difference (TD) error.&quot;&quot;&quot; expected_q_values = q_learning_update(next_states, rewards, dones, gamma, q_network) q_values = q_network(states).gather(dim=1, index=actions) delta = expected_q_values - q_values return delta def double_q_learning_error(states: States, actions: Actions, rewards: Rewards, next_states: States, dones: Dones, gamma: float, q1: QNetwork, q2: QNetwork) -&gt; torch.Tensor: &quot;&quot;&quot;Double Q-learning temporal-difference (TD) error.&quot;&quot;&quot; expected_q_values = double_q_learning_update(next_states, rewards, dones, gamma, q1, q2) q_values = q1(states).gather(dim=1, index=actions) delta = expected_q_values - q_values return delta . import collections import typing import numpy as np Experiences = np.ndarray SamplingWeights = np.array _field_names = [ &quot;state&quot;, &quot;action&quot;, &quot;reward&quot;, &quot;next_state&quot;, &quot;done&quot; ] Experience = collections.namedtuple(&quot;Experience&quot;, field_names=_field_names) class PrioritizedExperienceReplayBuffer: &quot;&quot;&quot;Fixed-size buffer to store priority, Experience tuples.&quot;&quot;&quot; def __init__(self, batch_size: int, buffer_size: int, alpha: float = 0.0, beta: float = 0.0, beta_annealing_schedule: typing.Callable[[int], float] = None, random_state: np.random.RandomState = None) -&gt; None: &quot;&quot;&quot; Initialize an ExperienceReplayBuffer object. Parameters: -- buffer_size (int): maximum size of buffer batch_size (int): size of each training batch alpha (float): Strength of prioritized sampling. Default to 0.0 (i.e., uniform sampling). beta (float): Strength of the sampling correction. Default to 0.0 (i.e., no correction). random_state (np.random.RandomState): random number generator. &quot;&quot;&quot; self._batch_size = batch_size self._buffer_size = buffer_size self._buffer_length = 0 # current number of prioritized experience tuples in buffer self._buffer = np.empty(self._buffer_size, dtype=[(&quot;priority&quot;, np.float32), (&quot;experience&quot;, Experience)]) self._alpha = alpha self._beta = beta # if not provided, assume constant beta annealing schedule if beta_annealing_schedule is None: self._beta_annealing_schedule = lambda n: self._beta else: self._beta_annealing_schedule = beta_annealing_schedule self._random_state = np.random.RandomState() if random_state is None else random_state def __len__(self) -&gt; int: &quot;&quot;&quot;Current number of prioritized experience tuple stored in buffer.&quot;&quot;&quot; return self._buffer_length @property def alpha(self): &quot;&quot;&quot;Strength of prioritized sampling.&quot;&quot;&quot; return self._alpha @property def batch_size(self) -&gt; int: &quot;&quot;&quot;Number of experience samples per training batch.&quot;&quot;&quot; return self._batch_size @property def buffer_size(self) -&gt; int: &quot;&quot;&quot;Maximum number of prioritized experience tuples stored in buffer.&quot;&quot;&quot; return self._buffer_size def add(self, experience: Experience) -&gt; None: &quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot; priority = 1.0 if self.is_empty() else self._buffer[&quot;priority&quot;].max() if self.is_full(): if priority &gt; self._buffer[&quot;priority&quot;].min(): idx = self._buffer[&quot;priority&quot;].argmin() self._buffer[idx] = (priority, experience) else: pass # low priority experiences should not be included in buffer else: self._buffer[self._buffer_length] = (priority, experience) self._buffer_length += 1 def is_empty(self) -&gt; bool: &quot;&quot;&quot;True if the buffer is empty; False otherwise.&quot;&quot;&quot; return self._buffer_length == 0 def is_full(self) -&gt; bool: &quot;&quot;&quot;True if the buffer is full; False otherwise.&quot;&quot;&quot; return self._buffer_length == self._buffer_size def sample(self, episode_number: int) -&gt; typing.Tuple[np.array, Experiences, SamplingWeights]: &quot;&quot;&quot;Sample a batch of experiences from memory.&quot;&quot;&quot; # use sampling scheme to determine which experiences to use for learning ps = self._buffer[:self._buffer_length][&quot;priority&quot;] sampling_probs = ps**self._alpha / np.sum(ps**self._alpha) idxs = self._random_state.choice(np.arange(ps.size), size=self._batch_size, replace=True, p=sampling_probs) # select the experiences and compute sampling weights experiences = self._buffer[&quot;experience&quot;][idxs] # compute the sampling weights beta = self._beta_annealing_schedule(episode_number) weights = (self._buffer_length * sampling_probs[idxs])**-beta normalized_weights = weights / weights.max() return idxs, experiences, normalized_weights def update_priorities(self, idxs: np.array, priorities: np.array) -&gt; None: &quot;&quot;&quot;Update the priorities associated with particular experiences.&quot;&quot;&quot; self._buffer[&quot;priority&quot;][idxs] = priorities . Dueling Network Architecture . The motivation for the new Q-network architecture developed by Wang et al (2016) is the observation that in many practical applications it is unnecessary to estimate the value of each action choice in every state. In some states, it is obviously critical to know which action to take (think of an autonomous vehicle about to hit a pedistrian!), but in many other states the choice of action has no real, observable impact. Bootstrapping-based RL algorithms, however, estimate the value of each action for every state. . . The lower layers of the network are shared and can be though of as a kind of feature extractor. Instead of following these layers with a single sequence of fully-connected, dense layers, the dueling network architecture splits into two streams of fully-connected, dense layers. One stream of fully connected layers is used to estimate the value function directly, while the second stream of layers is used to estimate the advantage function. The two streams are then re-combined using equation ??? above to estimate the Q-function. . There are some technical issues given any Q-function it is not possible to recover both V and A functions uniquely (technically, this means that the parameters $ theta^{F}, theta^{A}, theta^{V}$ are not identifiable from the data). In order to solve this issue Wang et al (2016) force the advantage function estimator to have zero advantage for the chosen action. That is, we let the last module of the network implement the forward mapping . $$ Q(s, a; theta^{F}, theta^{A}, theta^{V}) = V (s; theta^{F}, theta^{V}) + bigg(A(s, a; theta^{F}, theta^{A}) − underset{a&#39;}{ max} A(s, a&#39;; theta^{F}, theta^{A}) bigg) $$ . trying to An alternative module replaces the max operator with an average: . $$ Q(s, a; theta^{F}, theta^{A}, theta^{V}) = V (s; theta^{F}, theta^{V}) + bigg(A(s, a; theta^{F}, theta^{A}) − frac{1}{|A|} sum_{a&#39;}A(s, a&#39;; theta^{F}, theta^{A}) bigg) $$ . Since the output of the dueling network architecture is a Q-function, it can be trained with either the DQN or DDQN training algorithms and can also take advantage of other advances such as better replay memories, better exploration policies, etc. . In the cell below I wrap up these ideas into a PyTorch nn.Module. All the action is in the implementation of the forward method of the DuelingDeepQNetwork module which will be used to compute the forward pass during training. . import typing from torch import nn class DuelingDeepQNetwork(nn.Module): def __init__(self, advantage_q_network_fn: typing.Callable[[], QNetwork], feature_extractor_q_network_fn: typing.Callable[[], QNetwork], value_q_network_fn: typing.Callable[[], QNetwork]): super().__init__() self._advantage_q_network_fn = advantage_q_network_fn self._feature_extractor_q_network_fn = feature_extractor_q_network_fn self._value_q_network_fn = value_q_network_fn self._initialize() def _initialize(self): &quot;&quot;&quot;Create the various Q-network instances.&quot;&quot;&quot; self._feature_extractor_q_network = self._feature_extractor_q_network_fn() self._advantage_q_network = self._advantage_q_network_fn() self._value_q_network = self._value_q_network_fn() def clone(self) -&gt; &quot;DuelingDeepQNetwork&quot;: &quot;&quot;&quot;Return a DuelingDeepQNetwork with the same network architecture as self.&quot;&quot;&quot; q_network = DuelingDeepQNetwork(self._advantage_q_network_fn, self._feature_extractor_q_network_fn, self._value_q_network_fn) return q_network def forward(self, X: torch.Tensor): &quot;&quot;&quot;Forward pass combines the three Q-networks.&quot;&quot;&quot; Z = self._feature_extractor_q_network(X) advantage = self._advantage_q_network(Z) value = self._value_q_network(Z) return value + advantage - advantage.mean() def synchronize_with(self, other: &quot;DuelingDeepQNetwork&quot;) -&gt; None: &quot;&quot;&quot;Synchronize the weights of self with those of other.&quot;&quot;&quot; synchronize_q_networks(self, other) . Refactoring the DeepQAgent class . Other than continuing to clean up internal implementation details, nothing really changed from the implementation of the DeepQAgent from my previous posts. I added two additional parameters to the constructor: alpha which controls the strength of the prioritization sampling and beta_annealing_schedule (discussed in detail below) which allows the strength of the sampling bias correction (i.e., the importance sampling weights) to increase as training progresses. . The GymState is represented by an np.ndarray with shape (210, 160, 3) and dtype np.uint8. Need to convert the GymState into a DeepQAgent internal representation of state which is a torch.Tensor with shape (1, 3, 210, 160) and dtype torch.float32. We can accomplish this by defining a pre-processing function that takes an ndarray input and returns a torch.Tensor. This function can encapsulate what ever pre-processing steps that need to be included to convert a raw GymState to a suitable torch.Tensor. Here I make use of the torchvision.transforms module which contains exactly the transformation that I need! . from torchvision import transforms GymState = np.ndarray def preprocessing_fn(state: GymState) -&gt; torch.Tensor: &quot;&quot;&quot;Converts a Gym state with shape (H, W, C) to a torch.Tensor with shape (1, C, H, W).&quot;&quot;&quot; state_tensor = (transforms.ToTensor()(state) .unsqueeze(dim=0)) return state_tensor . import typing import numpy as np import torch from torch import nn, optim Action = int Reward = float Done = bool class Agent: def __call__(self, state: GymState) -&gt; Action: &quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot; raise NotImplementedError def load(self, filepath) -&gt; None: &quot;&quot;&quot;Load an Agent state from a saved checkpoint.&quot;&quot;&quot; raise NotImplementedError def save(self, filepath) -&gt; None: &quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot; raise NotImplementedError def step(self, state: GymState, action: Action, reward: Reward, next_state: GymState, done: Done) -&gt; None: &quot;&quot;&quot;Update internal state after observing effect of action on the environment.&quot;&quot;&quot; raise NotImplmentedError class DeepQAgent(Agent): def __init__(self, dueling_dqn: DuelingDeepQNetwork, replay_buffer: PrioritizedExperienceReplayBuffer, preprocessing_fn: typing.Callable[[GymState], torch.Tensor], optimizer_fn: typing.Callable[[typing.Iterable[nn.Parameter]], optim.Optimizer], number_actions: int, epsilon_decay_schedule: typing.Callable[[int], float], gamma: float, update_frequency: int, seed: int = None) -&gt; None: &quot;&quot;&quot; Initialize a DeepQAgent. Parameters: -- dueling_dqn (DuelingDeepQNetwork): optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer. epsilon_decay_schdule (callable): function that takes episode number and returns 0 &lt;= epsilon &lt; 1. alpha (float): rate at which the target q-network parameters are updated. gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1). update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated. seed (int): random seed &quot;&quot;&quot; self._device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # set seeds for reproducibility self._random_state = np.random.RandomState() if seed is None else np.random.RandomState(seed) if seed is not None: torch.manual_seed(seed) if torch.cuda.is_available(): torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # initialize agent hyperparameters self._memory = replay_buffer self._number_actions = number_actions self._epsilon_decay_schedule = epsilon_decay_schedule self._gamma = gamma self._update_frequency = update_frequency # initialize Q-Networks self._preprocessing_fn = preprocessing_fn self._online_q_network = dueling_dqn self._target_q_network = (self._online_q_network .clone()) self._target_q_network.synchronize_with(self._online_q_network) self._online_q_network.to(self._device) self._target_q_network.to(self._device) # initialize the optimizer self._optimizer = optimizer_fn(self._online_q_network.parameters()) # initialize some counters self._number_episodes = 0 self._number_timesteps = 0 def __call__(self, state: GymState) -&gt; Action: &quot;&quot;&quot;Epsilon-greedy action given the current state of the environment.&quot;&quot;&quot; _state = (self._preprocessing_fn(state) .to(self._device)) # choose uniform at random if agent has insufficient experience if not self.has_sufficient_experience(): action = self._uniform_random_policy(_state) else: epsilon = self._epsilon_decay_schedule(self._number_episodes) action = self._epsilon_greedy_policy(_state, epsilon) return action def _uniform_random_policy(self, state: torch.Tensor) -&gt; Action: &quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot; return self._random_state.randint(self._number_actions) def _greedy_policy(self, state: torch.Tensor) -&gt; Action: &quot;&quot;&quot;Choose action that maximizes the Q-values given the current state.&quot;&quot;&quot; actions = select_greedy_actions(state, self._online_q_network) action = (actions.cpu() # actions might reside on the GPU! .item()) return action def _epsilon_greedy_policy(self, state: torch.Tensor, epsilon: float) -&gt; Action: &quot;&quot;&quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&quot;&quot;&quot; if self._random_state.random() &lt; epsilon: action = self._uniform_random_policy(state) else: action = self._greedy_policy(state) return action def _ddqn_algorithm(self, idxs: np.ndarray, states: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor, next_states: torch.Tensor, dones: torch.Tensor, sampling_weights: torch.Tensor) -&gt; None: &quot;&quot;&quot;Double deep Q-network (DDQN) algorithm with prioritized experience replay.&quot;&quot;&quot; # compute the temporal difference errors deltas = double_q_learning_error(states, actions, rewards, next_states, dones, self._gamma, self._online_q_network, self._target_q_network) # update experience priorities priorities = (deltas.abs() .cpu() .detach() .numpy() .flatten()) self._memory.update_priorities(idxs, priorities + 1e-6) # priorities must be positive! # compute the mean squared loss loss = torch.mean((deltas * sampling_weights)**2) # updates the parameters of the online network self._optimizer.zero_grad() loss.backward() self._optimizer.step() # don&#39;t forget to synchronize the target and online networks self._target_q_network.synchronize_with(self._online_q_network) def has_sufficient_experience(self) -&gt; bool: &quot;&quot;&quot;True if agent has enough experience to train on a batch of samples; False otherwise.&quot;&quot;&quot; return len(self._memory) &gt;= self._memory.batch_size def load(self, filepath: str) -&gt; None: checkpoint = torch.load(filepath) self._online_q_network.load_state_dict(checkpoint[&quot;q-network-state&quot;]) self._target_q_network.synchronize_with(self._online_q_network) self._optimizer.load_state_dict(checkpoint[&quot;optimizer-state&quot;]) def save(self, filepath: str) -&gt; None: &quot;&quot;&quot; Saves the state of the DeepQAgent. Parameters: -- filepath (str): filepath where the serialized state should be saved. Notes: The method uses `torch.save` to serialize the state of the q-network, the optimizer, as well as the dictionary of agent hyperparameters. &quot;&quot;&quot; checkpoint = { &quot;q-network-state&quot;: self._online_q_network.state_dict(), &quot;optimizer-state&quot;: self._optimizer.state_dict(), &quot;experience_replay_buffer&quot;: { &quot;alpha&quot;: self._memory.alpha, &quot;batch_size&quot;: self._memory.batch_size, &quot;beta_annealing_schedule&quot;: self._beta_annealing_schedule, &quot;buffer_size&quot;: self._memory.buffer_size, }, &quot;agent-hyperparameters&quot;: { &quot;epsilon_decay_schedule&quot;: self._epsilon_decay_schedule, &quot;gamma&quot;: self._gamma, &quot;update_frequency&quot;: self._update_frequency } } torch.save(checkpoint, filepath) def step(self, state: GymState, action: Action, reward: Reward, next_state: GymState, done: Done) -&gt; None: &quot;&quot;&quot;Update internal state after observing effect of action on the environment.&quot;&quot;&quot; state_tensor = self._preprocessing_fn(state) next_state_tensor = self._preprocessing_fn(next_state) experience = Experience(state_tensor, action, reward, next_state_tensor, done) self._memory.add(experience) if done: self._number_episodes += 1 else: self._number_timesteps += 1 # every so often the agent should learn from experiences if self._number_timesteps % self._update_frequency == 0 and self.has_sufficient_experience(): idxs, _experiences, _sampling_weights = self._memory.sample(self._number_episodes) # unpack the experiences _states, _actions, _rewards, _next_states, _dones = tuple(zip(*_experiences)) states = (torch.cat(_states, dim=0) .to(self._device)) actions = (torch.Tensor(_actions) .long() .unsqueeze(dim=1) .to(self._device)) rewards = (torch.Tensor(_rewards) .unsqueeze(dim=1) .to(self._device)) next_states = (torch.cat(_next_states, dim=0) .to(self._device)) dones = (torch.Tensor(_dones) .unsqueeze(dim=1) .to(self._device)) # reshape sampling weights sampling_weights = (torch.Tensor(_sampling_weights) .view((-1, 1)) .to(self._device)) self._ddqn_algorithm(idxs, states, actions, rewards, next_states, dones, sampling_weights) . The Training Loop . The code for the training loop remains unchanged from previous posts. . import collections import typing import gym Score = int def _train_for_at_most(agent: Agent, env: gym.Env, max_timesteps: int) -&gt; Score: &quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot; state = env.reset() score = 0 for t in range(max_timesteps): action = agent(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward if done: break return score def _train_until_done(agent: Agent, env: gym.Env) -&gt; Score: &quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot; state = env.reset() score = 0 done = False while not done: action = agent(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward return score def train(agent: Agent, env: gym.Env, checkpoint_filepath: str, target_score: Score, number_episodes: int, maximum_timesteps=None) -&gt; typing.List[Score]: &quot;&quot;&quot; Reinforcement learning training loop. Parameters: -- agent (Agent): an agent to train. env (gym.Env): an environment in which to train the agent. checkpoint_filepath (str): filepath used to save the state of the trained agent. number_episodes (int): maximum number of training episodes. maximum_timesteps (int): maximum number of timesteps per episode. Returns: -- scores (list): collection of episode scores from training. &quot;&quot;&quot; scores = [] most_recent_scores = collections.deque(maxlen=100) for i in range(number_episodes): if maximum_timesteps is None: score = _train_until_done(agent, env) else: score = _train_for_at_most(agent, env, maximum_timesteps) scores.append(score) most_recent_scores.append(score) average_score = sum(most_recent_scores) / len(most_recent_scores) if average_score &gt;= target_score: print(f&quot; nEnvironment solved in {i:d} episodes! tAverage Score: {average_score:.2f}&quot;) agent.save(checkpoint_filepath) break if (i + 1) % 100 == 0: print(f&quot; rEpisode {i + 1} tAverage Score: {average_score:.2f}&quot;) return scores . Solving atari environments . In the rest of this blog post I will use the Double DQN algorithm with prioritized experience replay to train an agent to solve the LunarLander-v2 environment from OpenAI. . https://gym.openai.com/envs/#atari . Google Colab Preamble . If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment. . %%bash # install required system dependencies apt-get install -y xvfb x11-utils # install required python dependencies (might need to install additional gym extras depending) pip install gym[atari]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* . The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set visible=False. . This code only needs to be run once per session to start the display. . import pyvirtualdisplay _display = pyvirtualdisplay.Display(visible=False, # use False with Xvfb size=(1400, 900)) _ = _display.start() . Binder Preamble . The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set visible=False. . This code only needs to be run once per session to start the display. . import pyvirtualdisplay _display = pyvirtualdisplay.Display(visible=False, # use False with Xvfb size=(1400, 900)) _ = _display.start() . import gym env = gym.make(&#39;Atlantis-v0&#39;) _ = env.seed(42) . Creating a DeepQAgent . Before creating an instance of the DeepQAgent with prioritized experience replay I need to define a $ beta$-annealing schedule, an $ epsilon$-decay schedule, and choose an optimizer. . $ beta$-annealing schedule . Due to the inherent non-stationarity of the RL training process, Schaul et al 2016 hypothesize that a small sampling bias can be ignored during early training episodes. Instead of fixing $ beta=1$ (and fully correcting for the bias throughout training) they increase the amount of importance sampling correction as the number of training episodes increase by defining a schedule for $ beta$ that reaches 1 (i.e., full bias correction) only near the end of training. . Note that the choice of $ beta$ interacts with choice of prioritization exponent $ alpha$: increasing both simultaneously prioritizes sampling more aggressively while at the same time as correcting for it more strongly. . # define some annealing schedue for beta rate = 1e-2 _beta_annealing_schedule = lambda n: 1 - np.exp(-rate * n) _replay_buffer_kwargs = { &quot;alpha&quot;: 0.5, &quot;beta_annealing_schedule&quot;: _beta_annealing_schedule, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 1000000, &quot;random_state&quot;: None } replay_buffer = PrioritizedExperienceReplayBuffer(**_replay_buffer_kwargs) . $ epsilon$-decay schedule . As was the case with the DQN and Double DQN algorithms, the agent chooses its action using an $ epsilon$-greedy policy. When using an $ epsilon$-greedy policy, with probability $ epsilon$, the agent explores the state space by choosing an action uniformly at random from the set of feasible actions; with probability $1- epsilon$, the agent exploits its current knowledge by choosing the optimal action given that current state. . As the agent learns and acquires additional knowledge about it environment it makes sense to decrease exploration and increase exploitation by decreasing $ epsilon$. In practice, it isn&#39;t a good idea to decrease $ epsilon$ to zero; instead one typically decreases $ epsilon$ over time according to some schedule until it reaches some minimum value. . def power_decay_schedule(episode_number: int, decay_factor: float, minimum_epsilon: float) -&gt; float: &quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot; return max(decay_factor**episode_number, minimum_epsilon) _epsilon_decay_schedule_kwargs = { &quot;decay_factor&quot;: 0.99, &quot;minimum_epsilon&quot;: 1e-2, } epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs) . Choosing an optimizer . Given the good results I achieved in my previous post using the Adam optimizer I decided to continue to use that optimizer here. . from torch import optim _optimizer_kwargs = { &quot;lr&quot;: 1e-3, &quot;betas&quot;:(0.9, 0.999), &quot;eps&quot;: 1e-08, &quot;weight_decay&quot;: 0, &quot;amsgrad&quot;: False, } optimizer_fn = lambda parameters: optim.Adam(parameters, **_optimizer_kwargs) . Creating a dueling Q-network architecture . Now I am ready to create an instance of the DuelingQNetwork class. I need to define three, no-arg functions that return the feature_extractor_q_network, value_q_network and advantage_q_network, respectively. I am going to use the same network structure from the paper. . feature_extractor_q_network consists of three convolutional layers with ReLU activations. First convolutional layer has 32 filters each using a kernel size of 8 and a stride of 4. Second convolutional layer has 64 filters each using a kernel of size 4 and a stride of 2. The final convolutional layer also has 64 filters but uses a kernel of size 2 and a stride of 1. | value_q_network consists of two layers. The first layer simply flattens the inputs from the feature_extractor_q_network. The second layer is just a dense, fully-connected layer followed by a ReLU activation function. The final layer of the value_q_network outputs a single number representing the value of a particular state. | advantage_q_network also consists of two layers. The first layer flattens the inputs from the feature_extractor_q_network. The second layer is just a dense, fully-connected layer followed by a ReLU activation function. The final layer of the value_q_network has the same number of outputs as there are valid actions. | . class LambdaLayer(nn.Module): def __init__(self, f): super().__init__() self._f = f def forward(self, X): return self._f(X) . def atari_feature_extractor_q_network_fn() -&gt; QNetwork: &quot;&quot;&quot;Defines the feature extractor Q-network.&quot;&quot;&quot; q_network = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4), nn.ReLU(), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1), nn.ReLU(), ) return q_network def atari_value_q_network_fn() -&gt; QNetwork: &quot;&quot;&quot;Defines the value Q-network (computes the value of each state).&quot;&quot;&quot; q_network = nn.Sequential( LambdaLayer(lambda tensor: tensor.view(tensor.size(0), -1)), nn.Linear(in_features=25024, out_features=512), nn.ReLU(), nn.Linear(in_features=512, out_features=1) ) return q_network def make_atari_advantage_q_network_fn(number_actions: int) -&gt; typing.Callable[[], QNetwork]: &quot;&quot;&quot;Return a function representing the advantage Q-network.&quot;&quot;&quot; def atari_advantage_q_network_fn() -&gt; QNetwork: &quot;&quot;&quot;Defines the advantage Q-network (computes the benefit of taking each action a given the state).&quot;&quot;&quot; q_network = nn.Sequential( LambdaLayer(lambda tensor: tensor.view(tensor.size(0), -1)), nn.Linear(in_features=25024, out_features=512), nn.ReLU(), nn.Linear(in_features=512, out_features=number_actions) ) return q_network return atari_advantage_q_network_fn . NUMBER_ACTIONS = (env.action_space .n) dueling_dqn = DuelingDeepQNetwork( make_atari_advantage_q_network_fn(NUMBER_ACTIONS), atari_feature_extractor_q_network_fn, atari_value_q_network_fn ) . Training the DeepQAgent . Now I am finally ready to train the deep_q_agent. The target score for the LunarLander-v2 environment is 200 points on average for at least 100 consecutive episodes. First, I will train an RL agent with $ alpha=0.0$ and $ beta=0$ (throught training) which will recover the uniform random sampling baseline. Then I will re-train the RL agent using prioritized sampling for comparison. . Uniform random sampling . %%time _agent_kwargs = { &quot;dueling_dqn&quot;: dueling_dqn, &quot;replay_buffer&quot;: replay_buffer, &quot;preprocessing_fn&quot;: preprocessing_fn, &quot;number_actions&quot;: NUMBER_ACTIONS, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;seed&quot;: None, } double_dqn_agent = DeepQAgent(**_agent_kwargs) uniform_sampling_scores = train(double_dqn_agent, env, checkpoint_filepath=&quot;uniform-sampling-checkpoint.pth&quot;, number_episodes=1, target_score=float(&quot;inf&quot;)) . CPU times: user 11min 27s, sys: 2min 18s, total: 13min 45s Wall time: 10min 41s . Simulating the behavior of the trained agent . import matplotlib.pyplot as plt from IPython import display def simulate(agent: Agent, env: gym.Env, ax: plt.Axes) -&gt; None: state = env.reset() img = ax.imshow(env.render(mode=&#39;rgb_array&#39;)) done = False while not done: action = agent(state) img.set_data(env.render(mode=&#39;rgb_array&#39;)) plt.axis(&#39;off&#39;) display.display(plt.gcf()) display.clear_output(wait=True) state, reward, done, _ = env.step(action) env.close() . fig, ax = plt.subplots(1, 1, figsize=(10, 8)) simulate(double_dqn_agent, env, ax) . Plotting the time series of scores . I can use Pandas to quickly plot the time series of scores along with a 100 episode moving average. Most obvious difference between the two different sampling strategies is that prioritized sampling reduces, perhaps even eliminates, the significant number of large negative scores. Perhaps this is because prioritized sampling replays exactly those experiences that generate, at least initially, large losses (in magnitude). Over time the RL agent using prioritized sampling learns how to handle those awkward state transitions that led to really poor scores much better than an RL agent that uses uniform random sampling throughout. . uniform_sampling_scores . [15000.0, 25700.0, 12900.0, 20100.0, 15100.0, 14500.0, 21300.0, 40900.0, 27800.0, 20300.0] . import pandas as pd . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-39-7dd3504c366f&gt; in &lt;module&gt; -&gt; 1 import pandas as pd ModuleNotFoundError: No module named &#39;pandas&#39; . uniform_sampling_scores = pd.Series(uniform_sampling_scores, name=&quot;scores&quot;) prioritized_sampling_scores = pd.Series(prioritized_sampling_scores, name=&quot;scores&quot;) . fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True) _ = uniform_sampling_scores.plot(ax=axes[0], label=&quot;Uniform Sampling&quot;) _ = (uniform_sampling_scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=axes[0])) _ = axes[0].legend() _ = axes[0].set_ylabel(&quot;Score&quot;) _ = prioritized_sampling_scores.plot(ax=axes[1], label=&quot;Double DQN Scores&quot;) _ = (prioritized_sampling_scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=axes[1])) _ = axes[1].legend() _ = axes[1].set_ylabel(&quot;Score&quot;) _ = axes[1].set_xlabel(&quot;Episode Number&quot;) . Kernel density plot of the scores . In general, the kernel density plot will be bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent &quot;solved&quot; the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score). . fig, ax = plt.subplots(1,1) _ = uniform_sampling_scores.plot(kind=&quot;kde&quot;, ax=ax, label=&quot;Uniform Sampling&quot;) _ = prioritized_sampling_scores.plot(kind=&quot;kde&quot;, ax=ax, label=&quot;Priority Sampling&quot;) _ = ax.set_xlabel(&quot;Score&quot;) _ = ax.set_xscale(&quot;symlog&quot;) _ = ax.legend() . Where to go from here? . Up next in this series will be Dueling Network Architectures for Deep Reinforcement Learning. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html",
            "relUrl": "/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/20/dueling-network-architectures.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Rendering OpenAI Gym Envs on Binder and Google Colab",
            "content": "Getting OpenAI Gym environments to render properly in remote environments such as Google Colab and Binder turned out to be more challenging than I expected. In this post I lay out my solution in the hopes that I might save others time and effort to work it out independently. . Google Colab Preamble . If you wish to use Google Colab, then this section is for you! Otherwise, you can skip to the next section for the Binder Preamble. . Install X11 system dependencies . Install necessary X11 dependencies, in particular Xvfb, which is an X server that can run on machines with no display hardware and no physical input devices. . !apt-get install -y xvfb x11-utils . Install additional Python dependencies . Now that you have installed Xvfb, you need to install a Python wrapper pyvirtualdisplay in order to interact with Xvfb virtual displays from within Python. Next you need to install the Python bindings for OpenGL: PyOpenGL and PyOpenGL-accelerate. The former are the actual Python bindings, the latter is and optional set of C (Cython) extensions providing acceleration of common operations for slow points in PyOpenGL 3.x. . !pip install pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* . Install OpenAI Gym . Next you need to install the OpenAI Gym package. Note that depending on which Gym environment you are interested in working with you may need to add additional dependencies. Since I am going to simulate the LunarLander-v2 environment in my demo below I need to install the box2d extra which enables Gym environments that depend on the Box2D physics simulator. . !pip install gym[box2d]==0.17.* . Create a virtual display in the background . Next you need to create a virtual display in the background which the Gym Envs can connect to for rendering purposes. You can check that there is no display at present by confirming that the value of the DISPLAY environment variable has not yet been set. . !echo $DISPLAY . The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set visible=False when working with Xvfb. . This code only needs to be run once per session to start the display. . import pyvirtualdisplay _display = pyvirtualdisplay.Display(visible=False, # use False with Xvfb size=(1400, 900)) _ = _display.start() . After running the cell above you can echo out the value of the DISPLAY environment variable again to confirm that you now have a display running. . !echo $DISPLAY . For convenience I have gathered the above steps into two cells that you can copy and paste into the top of you Google Colab notebooks. . %%bash # install required system dependencies apt-get install -y xvfb x11-utils # install required python dependencies (might need to install additional gym extras depending) pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* . import pyvirtualdisplay _display = pyvirtualdisplay.Display(visible=False, # use False with Xvfb size=(1400, 900)) _ = _display.start() . Binder Preamble . If you wish to use Binder, then this section is for you! Although there really isn&#39;t much of anything that needs doing. . No additional installation required! . Unlike Google Colab, with Binder you can bake all the required dependencies (including the X11 system dependencies!) into the Docker image on which the Binder instance is based using Binder config files. These config files can either live in the root directory of your Git repo or in a binder sub-directory as is this case here. If you are interested in learning more about Binder, then check out the documentation for BinderHub which is the underlying technology behind the Binder project. . # config file for system dependencies !cat ../binder/apt.txt . freeglut3-dev xvfb x11-utils . # config file describing the conda environment !cat ../binder/environment.yml . name: null channels: - conda-forge - defaults dependencies: - gym-box2d=0.17 - jupyterlab=2.0 - matplotlib=3.2 - pip=20.0 - python=3.7 - pyvirtualdisplay=0.2 . # config file containing python deps not avaiable via conda channels !cat ../binder/requirements.txt . PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* . Create a virtual display in the background . Next you need to create a virtual display in the background which the Gym Envs can connect to for rendering purposes. You can check that there is no display at present by confirming that the value of the DISPLAY environment variable has not yet been set. . !echo $DISPLAY . The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set visible=False when working with Xvfb. . This code only needs to be run once per session to start the display. . import pyvirtualdisplay _display = pyvirtualdisplay.Display(visible=False, # use False with Xvfb size=(1400, 900)) _display.start() . After running the cell above you can echo out the value of the DISPLAY environment variable again to confirm that you now have a display running. . !echo $DISPLAY . Demo . Just to prove that the above setup works as advertised I will run a short simulation. First I will define an Agent that chooses an action randomly from the set of possible actions and the define a function that can be used to create such agents. . import typing import numpy as np # represent states as arrays and actions as ints State = np.array Action = int # agent is just a function! Agent = typing.Callable[[State], Action] def uniform_random_policy(state: State, number_actions: int, random_state: np.random.RandomState) -&gt; Action: &quot;&quot;&quot;Select an action at random from the set of feasible actions.&quot;&quot;&quot; feasible_actions = np.arange(number_actions) probs = np.ones(number_actions) / number_actions action = random_state.choice(feasible_actions, p=probs) return action def make_random_agent(number_actions: int, random_state: np.random.RandomState = None) -&gt; Agent: &quot;&quot;&quot;Factory for creating an Agent.&quot;&quot;&quot; _random_state = np.random.RandomState() if random_state is None else random_state return lambda state: uniform_random_policy(state, number_actions, _random_state) . In the cell below I wrap up the code to simulate a single epsiode of an OpenAI Gym environment. Note that the implementation assumes that the provided environment supports rgb_array rendering (which not all Gym environments support!). . import gym import matplotlib.pyplot as plt from IPython import display def simulate(agent: Agent, env: gym.Env) -&gt; None: state = env.reset() img = plt.imshow(env.render(mode=&#39;rgb_array&#39;)) done = False while not done: action = agent(state) img.set_data(env.render(mode=&#39;rgb_array&#39;)) plt.axis(&#39;off&#39;) display.display(plt.gcf()) display.clear_output(wait=True) state, reward, done, _ = env.step(action) env.close() . Finally you can setup your desired environment... . lunar_lander_v2 = gym.make(&#39;LunarLander-v2&#39;) _ = lunar_lander_v2.seed(42) . ...and run a simulation! . random_agent = make_agent(lunar_lander_v2.action_space.n, random_state=None) simulate(random_agent, lunar_lander_v2) . Currently there appears to be a non-trivial amount of flickering during the simulation. Not entirely sure what is causing this undesireable behavior. If you have any idea how to improve this, please leave a comment below. I will be sure to update this post accordingly if I find a good fix. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/openai/binder/google-colab/2020/04/16/remote-rendering-gym-envs.html",
            "relUrl": "/openai/binder/google-colab/2020/04/16/remote-rendering-gym-envs.html",
            "date": " • Apr 16, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Improving the Double DQN algorithm using prioritized experience replay",
            "content": "I am continuing to work my way through the Udacity Deep Reinforcement Learning Nanodegree. In this blog post I discuss and implement an important enhancement of the experience replay idea from Prioritized Experience Replay (Schaul et al 2016). . The following quote from the paper nicely summarizes the key idea. . Experience replay liberates online learning agents from processing transitions in the exact order they are experienced. Prioritized replay further liberates agents from considering transitions with the same frequency that they are experienced. ... In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which we correct with importance sampling. . Without further ado let&#39;s dive into discussing how to implement prioritized experience replay. . Prioritized Experience Replay . Using an experience replay buffer naturally leads to two issues that need to be addressed. . Which experiences should the agent store in the replay buffer? | Which experiences should the agent replay from the buffer in order to learn efficiently? | Schaul et al 2016 take the contents of the replay buffer more or less as given and focus solely on answering the second question. That is, the paper focuses on developing a procedure for making the most effective use of the experience replay buffer for learning. . Before discussing the procedure to sample from prioritized experiences, I need to discuss what information a reinforcement learning (RL) agent has available to prioritize its experiences for replay. . Prioritization using the temporal-difference (TD) error term . You can&#39;t prioritize experiences for learning unless you can measure the importance of each experience in the learning process. The ideal criterion would be the amount that RL agent can learn from experience given the current state (i.e., the expected learning value of the experience). . Unfortunately such an ideal criterion is not directly measurable. However, a reasonable proxy is the magnitude of an experience’s temporal-difference (TD) error $ delta_i$. The TD-error indicates how &quot;surprising&quot; or &quot;unexpected&quot; the experience is given the current state of the RL agent. Using the TD-error term to prioritize experiences for replay is particularly suitable for incremental, online RL algorithms, such as SARSA or Q-learning, as these algorithms already compute the TD-error and update the parameters proportionally. . Using the notation developed in my previous post on Improving the DQN algorihtm using Double Q-learning the TD-error term can be written as follows. . $$ delta_{i,t} = R_{i,t+1} + gamma Q big(S_{i,t+1}, underset{a}{ mathrm{argmax}} Q(S_{i,t+1}, a; theta_t); theta^{-}_t big) - Q(S_{i,t}, a_{i,t}; theta_t big)$$ . In the cell below I define a function for computing the TD-error (as well as several addition functions that will be used by the RL agent later in the post). . import torch from torch import nn def synchronize_q_networks(q_network_1: nn.Module, q_network_2: nn.Module) -&gt; None: &quot;&quot;&quot;In place, synchronization of q_network_1 and q_network_2.&quot;&quot;&quot; _ = q_network_1.load_state_dict(q_network_2.state_dict()) def select_greedy_actions(states: torch.Tensor, q_network: nn.Module) -&gt; torch.Tensor: &quot;&quot;&quot;Select the greedy action for the current state given some Q-network.&quot;&quot;&quot; _, actions = q_network(states).max(dim=1, keepdim=True) return actions def evaluate_selected_actions(states: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor, dones: torch.Tensor, gamma: float, q_network: nn.Module) -&gt; torch.Tensor: &quot;&quot;&quot;Compute the Q-values by evaluating the actions given the current states and Q-network.&quot;&quot;&quot; next_q_values = q_network(states).gather(dim=1, index=actions) q_values = rewards + (gamma * next_q_values * (1 - dones)) return q_values def double_q_learning_update(states: torch.Tensor, rewards: torch.Tensor, dones: torch.Tensor, gamma: float, q_network_1: nn.Module, q_network_2: nn.Module) -&gt; torch.Tensor: &quot;&quot;&quot;Double Q-Learning uses Q-network 1 to select actions and Q-network 2 to evaluate the selected actions.&quot;&quot;&quot; actions = select_greedy_actions(states, q_network_1) q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q_network_2) return q_values def double_q_learning_error(states: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor, next_states: torch.Tensor, dones: torch.Tensor, gamma: float, q_network_1: nn.Module, q_network_2: nn.Module) -&gt; torch.Tensor: expected_q_values = double_q_learning_update(next_states, rewards, dones, gamma, q_network_1, q_network_2) q_values = q_network_1(states).gather(dim=1, index=actions) delta = expected_q_values - q_values return delta . Now that I have defined a measurable criterion by which an RL agent can prioritize its experiences, I can move on to discussing the major contribution of the Schaul et al 2016 paper which was an efficient procedure for randomly sampling and replaying prioritized experiences. . Stochastic prioritization . Schaul et al 2016 introduce a stochastic sampling method that interpolates between pure greedy experience prioritization (i.e., always sampling the highest priority experiences) and uniform random sampling of experience. The probability of sampling experience $i$ is defined as follows. . $$ P(i) = frac{p_i^{ alpha}}{ sum_{j=0}^{N} p_j^{ alpha}} $$ . where $p_i &gt; 0$ is the priority of transition $i$. The exponent $ alpha$ determines how much prioritization is used, with $ alpha = 0$ corresponding to the uniform random sampling case. Note that the probability of being sampled is monotonic in an experience’s priority while guaranteeing a non-zero probability for the lowest-priority experience. . Correcting for sampling bias . Estimation of the expected value from stochastic updates relies on those updates being drawn from the same underlying distribution whose expectation you wish to estimate. Prioritized experience replay introduces a form of sampling bias that changes the underlying distribution (whose expectation needs to be estimated) in an uncontrolled fashion. When the underlying distribution changes, the solution to which the algorithm will converge also changes (even if the policy and state distribution are fixed). In order for the algorithm to converge properly, the bias introduced by the prioritized experience replay procedure needs to be corrected. . Schaul et al 2016 correct for this bias using an importance sampling scheme that computes a weight for each sampled experience that can be used when computing the loss for that sample. . $$ w_i = left( frac{1}{N} frac{1}{P(i)} right)^ beta $$ . The hyperparameter $ beta ge 0$ controls how strongly to correct for the bias: $ beta=0$ implies no correction; $ beta=1$ fully compensates for the bias. For stability reasons, since these importance sampling weights are included in the loss, they are be normalized by $ max_i w_i$. . Implementation . The PrioritizedExperienceReplayBuffer defined below is a substantial re-write of the ExperienceReplayBuffer that I used in my previous posts. . The most important implementation detail is that instead of using a fixed-length, double-ended queue as the underlying data structure for storing experiences, I am now using a NumPy structured array to store priority-experience tuples. In addition to cleaning up a lot of the internal implementation details, using a structured array as an internal buffer led to significant performance improvements. . import collections import typing import numpy as np _field_names = [ &quot;state&quot;, &quot;action&quot;, &quot;reward&quot;, &quot;next_state&quot;, &quot;done&quot; ] Experience = collections.namedtuple(&quot;Experience&quot;, field_names=_field_names) class PrioritizedExperienceReplayBuffer: &quot;&quot;&quot;Fixed-size buffer to store priority, Experience tuples.&quot;&quot;&quot; def __init__(self, batch_size: int, buffer_size: int, alpha: float = 0.0, random_state: np.random.RandomState = None) -&gt; None: &quot;&quot;&quot; Initialize an ExperienceReplayBuffer object. Parameters: -- buffer_size (int): maximum size of buffer batch_size (int): size of each training batch alpha (float): Strength of prioritized sampling. Default to 0.0 (i.e., uniform sampling). random_state (np.random.RandomState): random number generator. &quot;&quot;&quot; self._batch_size = batch_size self._buffer_size = buffer_size self._buffer_length = 0 # current number of prioritized experience tuples in buffer self._buffer = np.empty(self._buffer_size, dtype=[(&quot;priority&quot;, np.float32), (&quot;experience&quot;, Experience)]) self._alpha = alpha self._random_state = np.random.RandomState() if random_state is None else random_state def __len__(self) -&gt; int: &quot;&quot;&quot;Current number of prioritized experience tuple stored in buffer.&quot;&quot;&quot; return self._buffer_length @property def alpha(self): &quot;&quot;&quot;Strength of prioritized sampling.&quot;&quot;&quot; return self._alpha @property def batch_size(self) -&gt; int: &quot;&quot;&quot;Number of experience samples per training batch.&quot;&quot;&quot; return self._batch_size @property def buffer_size(self) -&gt; int: &quot;&quot;&quot;Maximum number of prioritized experience tuples stored in buffer.&quot;&quot;&quot; return self._buffer_size def add(self, experience: Experience) -&gt; None: &quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot; priority = 1.0 if self.is_empty() else self._buffer[&quot;priority&quot;].max() if self.is_full(): if priority &gt; self._buffer[&quot;priority&quot;].min(): idx = self._buffer[&quot;priority&quot;].argmin() self._buffer[idx] = (priority, experience) else: pass # low priority experiences should not be included in buffer else: self._buffer[self._buffer_length] = (priority, experience) self._buffer_length += 1 def is_empty(self) -&gt; bool: &quot;&quot;&quot;True if the buffer is empty; False otherwise.&quot;&quot;&quot; return self._buffer_length == 0 def is_full(self) -&gt; bool: &quot;&quot;&quot;True if the buffer is full; False otherwise.&quot;&quot;&quot; return self._buffer_length == self._buffer_size def sample(self, beta: float) -&gt; typing.Tuple[np.array, np.array, np.array]: &quot;&quot;&quot;Sample a batch of experiences from memory.&quot;&quot;&quot; # use sampling scheme to determine which experiences to use for learning ps = self._buffer[:self._buffer_length][&quot;priority&quot;] sampling_probs = ps**self._alpha / np.sum(ps**self._alpha) idxs = self._random_state.choice(np.arange(ps.size), size=self._batch_size, replace=True, p=sampling_probs) # select the experiences and compute sampling weights experiences = self._buffer[&quot;experience&quot;][idxs] weights = (self._buffer_length * sampling_probs[idxs])**-beta normalized_weights = weights / weights.max() return idxs, experiences, normalized_weights def update_priorities(self, idxs: np.array, priorities: np.array) -&gt; None: &quot;&quot;&quot;Update the priorities associated with particular experiences.&quot;&quot;&quot; self._buffer[&quot;priority&quot;][idxs] = priorities . Refactoring the DeepQAgent class . Other than continuing to clean up internal implementation details, nothing really changed from the implementation of the DeepQAgent from my previous posts. I added two additional parameters to the constructor: alpha which controls the strength of the prioritization sampling and beta_annealing_schedule (discussed in detail below) which allows the strength of the sampling bias correction (i.e., the importance sampling weights) to increase as training progresses. . import typing import numpy as np import torch from torch import nn, optim A = typing.TypeVar(&#39;A&#39;, bound=&#39;Agent&#39;) class Agent: def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot; raise NotImplementedError def learn(self, experiences: typing.List[Experience]) -&gt; None: &quot;&quot;&quot;Update the agent&#39;s state based on a collection of recent experiences.&quot;&quot;&quot; raise NotImplementedError def save(self, filepath) -&gt; None: &quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot; raise NotImplementedError def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot;Update agent&#39;s state after observing the effect of its action on the environment.&quot;&quot;&quot; raise NotImplmentedError class DeepQAgent(Agent): def __init__(self, state_size: int, action_size: int, number_hidden_units: int, optimizer_fn: typing.Callable[[typing.Iterable[nn.Parameter]], optim.Optimizer], batch_size: int, buffer_size: int, alpha: float, beta_annealing_schedule: typing.Callable[[int], float], epsilon_decay_schedule: typing.Callable[[int], float], gamma: float, update_frequency: int, seed: int = None) -&gt; None: &quot;&quot;&quot; Initialize a DeepQAgent. Parameters: -- state_size (int): the size of the state space. action_size (int): the size of the action space. number_hidden_units (int): number of units in the hidden layers. optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer. batch_size (int): number of experience tuples in each mini-batch. buffer_size (int): maximum number of experience tuples stored in the replay buffer. alpha (float): Strength of prioritized sampling; alpha &gt;= 0.0. beta_annealing_schedule (callable): function that takes episode number and returns beta &gt;= 0. epsilon_decay_schdule (callable): function that takes episode number and returns 0 &lt;= epsilon &lt; 1. alpha (float): rate at which the target q-network parameters are updated. gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1). update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated. seed (int): random seed &quot;&quot;&quot; self._state_size = state_size self._action_size = action_size self._device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # set seeds for reproducibility self._random_state = np.random.RandomState() if seed is None else np.random.RandomState(seed) if seed is not None: torch.manual_seed(seed) if torch.cuda.is_available(): torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # initialize agent hyperparameters _replay_buffer_kwargs = { &quot;alpha&quot;: alpha, &quot;batch_size&quot;: batch_size, &quot;buffer_size&quot;: buffer_size, &quot;random_state&quot;: self._random_state } self._memory = PrioritizedExperienceReplayBuffer(**_replay_buffer_kwargs) self._beta_annealing_schedule = beta_annealing_schedule self._epsilon_decay_schedule = epsilon_decay_schedule self._gamma = gamma # initialize Q-Networks self._update_frequency = update_frequency self._online_q_network = self._initialize_q_network(number_hidden_units) self._target_q_network = self._initialize_q_network(number_hidden_units) synchronize_q_networks(self._target_q_network, self._online_q_network) self._online_q_network.to(self._device) self._target_q_network.to(self._device) # initialize the optimizer self._optimizer = optimizer_fn(self._online_q_network.parameters()) # initialize some counters self._number_episodes = 0 self._number_timesteps = 0 def _initialize_q_network(self, number_hidden_units: int) -&gt; nn.Module: &quot;&quot;&quot;Create a neural network for approximating the action-value function.&quot;&quot;&quot; q_network = nn.Sequential( nn.Linear(in_features=self._state_size, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=self._action_size) ) return q_network def _uniform_random_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot; return self._random_state.randint(self._action_size) def _greedy_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action that maximizes the action_values given the current state.&quot;&quot;&quot; actions = select_greedy_actions(state, self._online_q_network) action = (actions.cpu() # actions might reside on the GPU! .item()) return action def _epsilon_greedy_policy(self, state: torch.Tensor, epsilon: float) -&gt; int: &quot;&quot;&quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&quot;&quot;&quot; if self._random_state.random() &lt; epsilon: action = self._uniform_random_policy(state) else: action = self._greedy_policy(state) return action def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot; Return the action for given state as per current policy. Parameters: -- state (np.array): current state of the environment. Return: -- action (int): an integer representing the chosen action. &quot;&quot;&quot; # need to reshape state array and convert to tensor state_tensor = (torch.from_numpy(state) .unsqueeze(dim=0) .to(self._device)) # choose uniform at random if agent has insufficient experience if not self.has_sufficient_experience(): action = self._uniform_random_policy(state_tensor) else: epsilon = self._epsilon_decay_schedule(self._number_episodes) action = self._epsilon_greedy_policy(state_tensor, epsilon) return action def learn(self, idxs: np.array, experiences: np.array, sampling_weights: np.array) -&gt; None: &quot;&quot;&quot;Update the agent&#39;s state based on a collection of recent experiences.&quot;&quot;&quot; states, actions, rewards, next_states, dones = (torch.Tensor(vs).to(self._device) for vs in zip(*experiences)) # need to add second dimension to some tensors actions = (actions.long() .unsqueeze(dim=1)) rewards = rewards.unsqueeze(dim=1) dones = dones.unsqueeze(dim=1) deltas = double_q_learning_error(states, actions, rewards, next_states, dones, self._gamma, self._online_q_network, self._target_q_network) # update experience priorities priorities = (deltas.abs() .cpu() .detach() .numpy() .flatten()) self._memory.update_priorities(idxs, priorities + 1e-6) # priorities must be positive! # compute the mean squared loss _sampling_weights = (torch.Tensor(sampling_weights) .view((-1, 1))) loss = torch.mean((deltas * _sampling_weights)**2) # updates the parameters of the online network self._optimizer.zero_grad() loss.backward() self._optimizer.step() synchronize_q_networks(self._target_q_network, self._online_q_network) def has_sufficient_experience(self) -&gt; bool: &quot;&quot;&quot;True if agent has enough experience to train on a batch of samples; False otherwise.&quot;&quot;&quot; return len(self._memory) &gt;= self._memory.batch_size def save(self, filepath: str) -&gt; None: &quot;&quot;&quot; Saves the state of the DeepQAgent. Parameters: -- filepath (str): filepath where the serialized state should be saved. Notes: The method uses `torch.save` to serialize the state of the q-network, the optimizer, as well as the dictionary of agent hyperparameters. &quot;&quot;&quot; checkpoint = { &quot;q-network-state&quot;: self._online_q_network.state_dict(), &quot;optimizer-state&quot;: self._optimizer.state_dict(), &quot;agent-hyperparameters&quot;: { &quot;alpha&quot;: self._memory.alpha, &quot;beta_annealing_schedule&quot;: self._beta_annealing_schedule, &quot;batch_size&quot;: self._memory.batch_size, &quot;buffer_size&quot;: self._memory.buffer_size, &quot;epsilon_decay_schedule&quot;: self._epsilon_decay_schedule, &quot;gamma&quot;: self._gamma, &quot;update_frequency&quot;: self._update_frequency } } torch.save(checkpoint, filepath) def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot; Updates the agent&#39;s state based on feedback received from the environment. Parameters: -- state (np.array): the previous state of the environment. action (int): the action taken by the agent in the previous state. reward (float): the reward received from the environment. next_state (np.array): the resulting state of the environment following the action. done (bool): True is the training episode is finised; false otherwise. &quot;&quot;&quot; experience = Experience(state, action, reward, next_state, done) self._memory.add(experience) if done: self._number_episodes += 1 else: self._number_timesteps += 1 # every so often the agent should learn from experiences if self._number_timesteps % self._update_frequency == 0 and self.has_sufficient_experience(): beta = self._beta_annealing_schedule(self._number_episodes) idxs, experiences, sampling_weights = self._memory.sample(beta) self.learn(idxs, experiences, sampling_weights) . The Training Loop . The code for the training loop remains unchanged from previous posts. . import collections import typing import gym def _train_for_at_most(agent: Agent, env: gym.Env, max_timesteps: int) -&gt; int: &quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot; state = env.reset() score = 0 for t in range(max_timesteps): action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward if done: break return score def _train_until_done(agent: Agent, env: gym.Env) -&gt; float: &quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot; state = env.reset() score = 0 done = False while not done: action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward return score def train(agent: Agent, env: gym.Env, checkpoint_filepath: str, target_score: float, number_episodes: int, maximum_timesteps=None) -&gt; typing.List[float]: &quot;&quot;&quot; Reinforcement learning training loop. Parameters: -- agent (Agent): an agent to train. env (gym.Env): an environment in which to train the agent. checkpoint_filepath (str): filepath used to save the state of the trained agent. number_episodes (int): maximum number of training episodes. maximum_timesteps (int): maximum number of timesteps per episode. Returns: -- scores (list): collection of episode scores from training. &quot;&quot;&quot; scores = [] most_recent_scores = collections.deque(maxlen=100) for i in range(number_episodes): if maximum_timesteps is None: score = _train_until_done(agent, env) else: score = _train_for_at_most(agent, env, maximum_timesteps) scores.append(score) most_recent_scores.append(score) average_score = sum(most_recent_scores) / len(most_recent_scores) if average_score &gt;= target_score: print(f&quot; nEnvironment solved in {i:d} episodes! tAverage Score: {average_score:.2f}&quot;) agent.save(checkpoint_filepath) break if (i + 1) % 100 == 0: print(f&quot; rEpisode {i + 1} tAverage Score: {average_score:.2f}&quot;) return scores . Solving the LunarLander-v2 environment . In the rest of this blog post I will use the Double DQN algorithm with prioritized experience replay to train an agent to solve the LunarLander-v2 environment from OpenAI. . In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land &quot;gracefully&quot; (and not crash in the landing area!). . A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered &quot;solved&quot; if the lander is able to achieve 200 points (I will actually be more stringent and define &quot;solved&quot; as achieving over 200 points on average in the most recent 100 training episodes). . Action Space . There are four discrete actions available: . Do nothing. | Fire the left orientation engine. | Fire main engine. | Fire the right orientation engine. | Colab specific environment setup . If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment. . !pip install gym[box2d]==0.17.* . import gym env = gym.make(&#39;LunarLander-v2&#39;) _ = env.seed(42) . /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32 warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;)) . Creating a DeepQAgent . Before creating an instance of the DeepQAgent with prioritized experience replay I need to define a $ beta$-annealing schedule, an $ epsilon$-decay schedule, and choose an optimizer. . $ beta$-annealing schedule . Due to the inherent non-stationarity of the RL training process, Schaul et al 2016 hypothesize that a small sampling bias can be ignored during early training episodes. Instead of fixing $ beta=1$ (and fully correcting for the bias throughout training) they increase the amount of importance sampling correction as the number of training episodes increase by defining a schedule for $ beta$ that reaches 1 (i.e., full bias correction) only near the end of training. . Note that the choice of $ beta$ interacts with choice of prioritization exponent $ alpha$: increasing both simultaneously prioritizes sampling more aggressively while at the same time as correcting for it more strongly. . def constant_annealing_schedule(n, constant): return constant def exponential_annealing_schedule(n, rate): return 1 - np.exp(-rate * n) . import matplotlib.pyplot as plt fig, ax = plt.subplots(1, 1, figsize=(10,6)) ns = np.arange(2000) rate = 1e-2 _ = ax.plot(ns, exponential_annealing_schedule(ns, rate)) _ = ax.set_ylabel(r&quot;$ beta$&quot;, rotation=&quot;horizontal&quot;, fontsize=20) _ = ax.set_xlabel(&quot;Number of Episodes&quot;, fontsize=15) _ = ax.set_title(r&quot;Typical $ beta$ annealing schedule&quot;, fontsize=15) . $ epsilon$-decay schedule . As was the case with the DQN and Double DQN algorithms, the agent chooses its action using an $ epsilon$-greedy policy. When using an $ epsilon$-greedy policy, with probability $ epsilon$, the agent explores the state space by choosing an action uniformly at random from the set of feasible actions; with probability $1- epsilon$, the agent exploits its current knowledge by choosing the optimal action given that current state. . As the agent learns and acquires additional knowledge about it environment it makes sense to decrease exploration and increase exploitation by decreasing $ epsilon$. In practice, it isn&#39;t a good idea to decrease $ epsilon$ to zero; instead one typically decreases $ epsilon$ over time according to some schedule until it reaches some minimum value. . def power_decay_schedule(episode_number: int, decay_factor: float, minimum_epsilon: float) -&gt; float: &quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot; return max(decay_factor**episode_number, minimum_epsilon) _epsilon_decay_schedule_kwargs = { &quot;decay_factor&quot;: 0.99, &quot;minimum_epsilon&quot;: 1e-2, } epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs) . Choosing an optimizer . Given the good results I achieved in my previous post using the Adam optimizer I decided to continue to use that optimizer here. . from torch import optim _optimizer_kwargs = { &quot;lr&quot;: 1e-3, &quot;betas&quot;:(0.9, 0.999), &quot;eps&quot;: 1e-08, &quot;weight_decay&quot;: 0, &quot;amsgrad&quot;: False, } optimizer_fn = lambda parameters: optim.Adam(parameters, **_optimizer_kwargs) . Training the DeepQAgent . Now I am finally ready to train the deep_q_agent. The target score for the LunarLander-v2 environment is 200 points on average for at least 100 consecutive episodes. First, I will train an RL agent with $ alpha=0.0$ and $ beta=0$ (throught training) which will recover the uniform random sampling baseline. Then I will re-train the RL agent using prioritized sampling for comparison. . Uniform random sampling . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;alpha&quot;: 0.0, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;beta_annealing_schedule&quot;: lambda n: constant_annealing_schedule(n, 0.0), &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;seed&quot;: None, } double_dqn_agent = DeepQAgent(**_agent_kwargs) uniform_sampling_scores = train(double_dqn_agent, env, &quot;uniform-sampling-checkpoint.pth&quot;, number_episodes=2000, target_score=float(&quot;inf&quot;)) . Episode 100 Average Score: -113.36 Episode 200 Average Score: 51.15 Episode 300 Average Score: 119.37 Episode 400 Average Score: 158.45 Episode 500 Average Score: 154.08 Episode 600 Average Score: 216.22 Episode 700 Average Score: 220.47 Episode 800 Average Score: 229.12 Episode 900 Average Score: 223.13 Episode 1000 Average Score: 230.66 Episode 1100 Average Score: 229.96 Episode 1200 Average Score: 209.60 Episode 1300 Average Score: 190.09 Episode 1400 Average Score: 212.46 Episode 1500 Average Score: 219.84 Episode 1600 Average Score: 226.77 Episode 1700 Average Score: 231.39 Episode 1800 Average Score: 208.05 Episode 1900 Average Score: 183.24 Episode 2000 Average Score: 194.43 . Prioritized sampling . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;alpha&quot;: 0.5, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;beta_annealing_schedule&quot;: lambda n: exponential_annealing_schedule(n, 1e-2), &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;seed&quot;: None, } double_dqn_agent = DeepQAgent(**_agent_kwargs) prioritized_sampling_scores = train(double_dqn_agent, env, &quot;prioritized-sampling-checkpoint.pth&quot;, number_episodes=2000, target_score=float(&quot;inf&quot;)) . Episode 100 Average Score: -130.92 Episode 200 Average Score: 15.15 Episode 300 Average Score: 71.90 Episode 400 Average Score: 191.28 Episode 500 Average Score: 226.89 Episode 600 Average Score: 238.60 Episode 700 Average Score: 194.70 Episode 800 Average Score: 227.00 Episode 900 Average Score: 223.78 Episode 1000 Average Score: 228.35 Episode 1100 Average Score: 220.97 Episode 1200 Average Score: 220.92 Episode 1300 Average Score: 241.30 Episode 1400 Average Score: 209.41 Episode 1500 Average Score: 200.94 Episode 1600 Average Score: 202.66 Episode 1700 Average Score: 235.63 Episode 1800 Average Score: 249.41 Episode 1900 Average Score: 237.83 Episode 2000 Average Score: 259.26 . Plotting the time series of scores . I can use Pandas to quickly plot the time series of scores along with a 100 episode moving average. Most obvious difference between the two different sampling strategies is that prioritized sampling reduces, perhaps even eliminates, the significant number of large negative scores. Perhaps this is because prioritized sampling replays exactly those experiences that generate, at least initially, large losses (in magnitude). Over time the RL agent using prioritized sampling learns how to handle those awkward state transitions that led to really poor scores much better than an RL agent that uses uniform random sampling throughout. . import pandas as pd . uniform_sampling_scores = pd.Series(uniform_sampling_scores, name=&quot;scores&quot;) prioritized_sampling_scores = pd.Series(prioritized_sampling_scores, name=&quot;scores&quot;) . fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True) _ = uniform_sampling_scores.plot(ax=axes[0], label=&quot;Uniform Sampling&quot;) _ = (uniform_sampling_scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=axes[0])) _ = axes[0].legend() _ = axes[0].set_ylabel(&quot;Score&quot;) _ = prioritized_sampling_scores.plot(ax=axes[1], label=&quot;Double DQN Scores&quot;) _ = (prioritized_sampling_scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=axes[1])) _ = axes[1].legend() _ = axes[1].set_ylabel(&quot;Score&quot;) _ = axes[1].set_xlabel(&quot;Episode Number&quot;) . Kernel density plot of the scores . In general, the kernel density plot will be bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent &quot;solved&quot; the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score). . fig, ax = plt.subplots(1,1) _ = uniform_sampling_scores.plot(kind=&quot;kde&quot;, ax=ax, label=&quot;Uniform Sampling&quot;) _ = prioritized_sampling_scores.plot(kind=&quot;kde&quot;, ax=ax, label=&quot;Priority Sampling&quot;) _ = ax.set_xlabel(&quot;Score&quot;) _ = ax.set_xscale(&quot;symlog&quot;) _ = ax.legend() . Where to go from here? . Up next in this series will be Dueling Network Architectures for Deep Reinforcement Learning. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html",
            "relUrl": "/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Improving the DQN algorithm using Double Q-Learning",
            "content": "I am continuing to work my way through the Udacity Deep Reinforcement Learning Nanodegree. In this blog post I discuss and implement the Double DQN algorithm from Deep Reinforcement Learning with Double Q-Learning (Van Hasselt et al 2015). The Double DQN algorithm is a minor, but important, modification of the original DQN algorithm that I covered in a previous post. . The Van Hasselt et al 2015 paper makes several important contributions. . Demonstration of how Q-learning can be overly optimistic in large-scale, even deterministic, problems due to the inherent estimation errors of learning. | Demonstration that overestimations are more common and severe in practice than previously acknowledged. | Implementation of Double Q-learning called Double DQN that extends, with minor modifications, the popular DQN algorithm and that can be used at scale to successfully reduce overestimations with the result being more stable and reliable learning. | Demonstation that Double DQN finds better policies by obtaining new state-of-the-art results on the Atari 2600 dataset. | Q-learning overestimates Q-values . No matter what type of function approximation scheme used to approximate the action-value function $Q$ there will always be approximation error. The presence of the max operator in the Bellman equation used to compute the $Q$-values means that the approximate $Q$-values will almost always be strictly greater than the corresponding $Q$ values from the true action-value function (i.e., the approximation errors will almost always be positive). This potentially significant source of bias can impede learning and is often exacerbated by the use of flexible, non-linear function approximators such as neural networks. . Double Q-learning addresses these issues by explicitly separating action selection from action evaluation which allows each step to use a different function approximator resulting in a better overall approximation of the action-value function. Figure 2 (with caption) below, which is taken from Van Hasselt et al 2015, summarizes these ideas. See the paper for more details. . . Implementing the Double DQN algorithm . The key idea behind Double Q-learning is to reduce overestimations of Q-values by separating the selection of actions from the evaluation of those actions so that a different Q-network can be used in each step. When applying Double Q-learning to extend the DQN algorithm one can use the online Q-network, $Q(S, a; theta)$, to select the actions and then the target Q-network, $Q(S, a; theta^{-})$, to evaluate the selected actions. . Before implement the Double DQN algorithm, I am going to re-implement the Q-learning update from the DQN algorithm in a way that explicitly separates action selection from action evaluation. Once I have implemented this new version of Q-learning, implementing the Double DQN algorithm will be much easier. Formally separating action selection from action evaluation involves re-writing the Q-learning Bellman equation as follows. . $$ Y_t^{DQN} = R_{t+1} + gamma Q big(S_{t+1}, underset{a}{ mathrm{argmax}} Q(S_{t+1}, a; theta_t); theta_t big) $$ . In Python this can be implemented as three separate functions. . import torch from torch import nn def select_greedy_actions(states: torch.Tensor, q_network: nn.Module) -&gt; torch.Tensor: &quot;&quot;&quot;Select the greedy action for the current state given some Q-network.&quot;&quot;&quot; _, actions = q_network(states).max(dim=1, keepdim=True) return actions def evaluate_selected_actions(states: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor, dones: torch.Tensor, gamma: float, q_network: nn.Module) -&gt; torch.Tensor: &quot;&quot;&quot;Compute the Q-values by evaluating the actions given the current states and Q-network.&quot;&quot;&quot; next_q_values = q_network(states).gather(dim=1, index=actions) q_values = rewards + (gamma * next_q_values * (1 - dones)) return q_values def q_learning_update(states: torch.Tensor, rewards: torch.Tensor, dones: torch.Tensor, gamma: float, q_network: nn.Module) -&gt; torch.Tensor: &quot;&quot;&quot;Q-Learning update with explicitly decoupled action selection and evaluation steps.&quot;&quot;&quot; actions = select_greedy_actions(states, q_network) q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q_network) return q_values . From here it is straight forward to implement the Double DQN algorithm. All I need is a second action-value function. The target network in the DQN architecture provides a natural candidate for the second action-value function. Hasselt et al 2015 suggest using the online Q-network to select the greedy policy actions before using the target Q-network to estimate the value of the selected actions. Once again here are the maths... . $$ Y_t^{DoubleDQN} = R_{t+1} + gamma Q big(S_{t+1}, underset{a}{ mathrm{argmax}} Q(S_{t+1}, a; theta_t), theta_t^{-} big) $$ . ...and here is the the Python implementation. . def double_q_learning_update(states: torch.Tensor, rewards: torch.Tensor, dones: torch.Tensor, gamma: float, q_network_1: nn.Module, q_network_2: nn.Module) -&gt; torch.Tensor: &quot;&quot;&quot;Double Q-Learning uses Q-network 1 to select actions and Q-network 2 to evaluate the selected actions.&quot;&quot;&quot; actions = select_greedy_actions(states, q_network_1) q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q_network_2) return q_values . Note that the function double_q_learning_update is almost identical to the q_learning_update function above: all that is needed is to introduce a second Q-network parameter, q_network_2, to the function. This second Q-network will be use to evaluate the actions chosen using the original Q-network parameter, now called q_network_1. . Experience Replay . Just like the DQN algorithm, the Double DQN algorithm uses an ExperienceReplayBuffer to stabilize the learning process. . import collections import typing import numpy as np _field_names = [ &quot;state&quot;, &quot;action&quot;, &quot;reward&quot;, &quot;next_state&quot;, &quot;done&quot; ] Experience = collections.namedtuple(&quot;Experience&quot;, field_names=_field_names) class ExperienceReplayBuffer: &quot;&quot;&quot;Fixed-size buffer to store Experience tuples.&quot;&quot;&quot; def __init__(self, batch_size: int, buffer_size: int = None, random_state: np.random.RandomState = None) -&gt; None: &quot;&quot;&quot; Initialize an ExperienceReplayBuffer object. Parameters: -- buffer_size (int): maximum size of buffer batch_size (int): size of each training batch random_state (np.random.RandomState): random number generator. &quot;&quot;&quot; self._batch_size = batch_size self._buffer_size = buffer_size self._buffer = collections.deque(maxlen=buffer_size) self._random_state = np.random.RandomState() if random_state is None else random_state def __len__(self) -&gt; int: return len(self._buffer) @property def batch_size(self) -&gt; int: &quot;&quot;&quot;Number of experience samples per training batch.&quot;&quot;&quot; return self._batch_size @property def buffer_size(self) -&gt; int: &quot;&quot;&quot;Total number of experience samples stored in memory.&quot;&quot;&quot; return self._buffer_size def append(self, experience: Experience) -&gt; None: &quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot; self._buffer.append(experience) def sample(self) -&gt; typing.List[Experience]: &quot;&quot;&quot;Randomly sample a batch of experiences from memory.&quot;&quot;&quot; idxs = self._random_state.randint(len(self._buffer), size=self._batch_size) experiences = [self._buffer[idx] for idx in idxs] return experiences . Refactoring the DeepQAgent class . Now that I have an implementation of the Double Q-learning algorithm I can refactor the DeepQAgent class from my previous post to incorporate the functionality above. The functions defined above can be added to the DeepQAgent as either static methods or simply included as module level functions, depending. I tend to prefer module level functions instead of static methods as module level function can be imported independently of class definitions which makes them a bit more re-usable. . import typing import numpy as np import torch from torch import nn, optim from torch.nn import functional as F class Agent: def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot; raise NotImplementedError def learn(self, experiences: typing.List[Experience]) -&gt; None: &quot;&quot;&quot;Update the agent&#39;s state based on a collection of recent experiences.&quot;&quot;&quot; raise NotImplementedError def save(self, filepath) -&gt; None: &quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot; raise NotImplementedError def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot;Update agent&#39;s state after observing the effect of its action on the environment.&quot;&quot;&quot; raise NotImplmentedError class DeepQAgent(Agent): def __init__(self, state_size: int, action_size: int, number_hidden_units: int, optimizer_fn: typing.Callable[[typing.Iterable[nn.Parameter]], optim.Optimizer], batch_size: int, buffer_size: int, epsilon_decay_schedule: typing.Callable[[int], float], alpha: float, gamma: float, update_frequency: int, double_dqn: bool = False, seed: int = None) -&gt; None: &quot;&quot;&quot; Initialize a DeepQAgent. Parameters: -- state_size (int): the size of the state space. action_size (int): the size of the action space. number_hidden_units (int): number of units in the hidden layers. optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer. batch_size (int): number of experience tuples in each mini-batch. buffer_size (int): maximum number of experience tuples stored in the replay buffer. epsilon_decay_schdule (callable): function that takes episode number and returns epsilon. alpha (float): rate at which the target q-network parameters are updated. gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1). update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated. double_dqn (bool): whether to use vanilla DQN algorithm or use the Double DQN algorithm. seed (int): random seed &quot;&quot;&quot; self._state_size = state_size self._action_size = action_size self._device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # set seeds for reproducibility self._random_state = np.random.RandomState() if seed is None else np.random.RandomState(seed) if seed is not None: torch.manual_seed(seed) if torch.cuda.is_available(): torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # initialize agent hyperparameters _replay_buffer_kwargs = { &quot;batch_size&quot;: batch_size, &quot;buffer_size&quot;: buffer_size, &quot;random_state&quot;: self._random_state } self._memory = ExperienceReplayBuffer(**_replay_buffer_kwargs) self._epsilon_decay_schedule = epsilon_decay_schedule self._alpha = alpha self._gamma = gamma self._double_dqn = double_dqn # initialize Q-Networks self._update_frequency = update_frequency self._online_q_network = self._initialize_q_network(number_hidden_units) self._target_q_network = self._initialize_q_network(number_hidden_units) self._synchronize_q_networks(self._target_q_network, self._online_q_network) self._online_q_network.to(self._device) self._target_q_network.to(self._device) # initialize the optimizer self._optimizer = optimizer_fn(self._online_q_network.parameters()) # initialize some counters self._number_episodes = 0 self._number_timesteps = 0 def _initialize_q_network(self, number_hidden_units: int) -&gt; nn.Module: &quot;&quot;&quot;Create a neural network for approximating the action-value function.&quot;&quot;&quot; q_network = nn.Sequential( nn.Linear(in_features=self._state_size, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=self._action_size) ) return q_network @staticmethod def _soft_update_q_network_parameters(q_network_1: nn.Module, q_network_2: nn.Module, alpha: float) -&gt; None: &quot;&quot;&quot;In-place, soft-update of q_network_1 parameters with parameters from q_network_2.&quot;&quot;&quot; for p1, p2 in zip(q_network_1.parameters(), q_network_2.parameters()): p1.data.copy_(alpha * p2.data + (1 - alpha) * p1.data) @staticmethod def _synchronize_q_networks(q_network_1: nn.Module, q_network_2: nn.Module) -&gt; None: &quot;&quot;&quot;In place, synchronization of q_network_1 and q_network_2.&quot;&quot;&quot; _ = q_network_1.load_state_dict(q_network_2.state_dict()) def _uniform_random_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot; return self._random_state.randint(self._action_size) def _greedy_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action that maximizes the action_values given the current state.&quot;&quot;&quot; action = (self._online_q_network(state) .argmax() .cpu() # action_values might reside on the GPU! .item()) return action def _epsilon_greedy_policy(self, state: torch.Tensor, epsilon: float) -&gt; int: &quot;&quot;&quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&quot;&quot;&quot; if self._random_state.random() &lt; epsilon: action = self._uniform_random_policy(state) else: action = self._greedy_policy(state) return action def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot; Return the action for given state as per current policy. Parameters: -- state (np.array): current state of the environment. Return: -- action (int): an integer representing the chosen action. &quot;&quot;&quot; # need to reshape state array and convert to tensor state_tensor = (torch.from_numpy(state) .unsqueeze(dim=0) .to(self._device)) # choose uniform at random if agent has insufficient experience if not self.has_sufficient_experience(): action = self._uniform_random_policy(state_tensor) else: epsilon = self._epsilon_decay_schedule(self._number_episodes) action = self._epsilon_greedy_policy(state_tensor, epsilon) return action def learn(self, experiences: typing.List[Experience]) -&gt; None: &quot;&quot;&quot;Update the agent&#39;s state based on a collection of recent experiences.&quot;&quot;&quot; states, actions, rewards, next_states, dones = (torch.Tensor(vs).to(self._device) for vs in zip(*experiences)) # need to add second dimension to some tensors actions = (actions.long() .unsqueeze(dim=1)) rewards = rewards.unsqueeze(dim=1) dones = dones.unsqueeze(dim=1) if self._double_dqn: target_q_values = double_q_learning_update(next_states, rewards, dones, self._gamma, self._online_q_network, self._target_q_network) else: target_q_values = q_learning_update(next_states, rewards, dones, self._gamma, self._target_q_network) online_q_values = (self._online_q_network(states) .gather(dim=1, index=actions)) # compute the mean squared loss loss = F.mse_loss(online_q_values, target_q_values) # updates the parameters of the online network self._optimizer.zero_grad() loss.backward() self._optimizer.step() self._soft_update_q_network_parameters(self._target_q_network, self._online_q_network, self._alpha) def has_sufficient_experience(self) -&gt; bool: &quot;&quot;&quot;True if agent has enough experience to train on a batch of samples; False otherwise.&quot;&quot;&quot; return len(self._memory) &gt;= self._memory.batch_size def save(self, filepath: str) -&gt; None: &quot;&quot;&quot; Saves the state of the DeepQAgent. Parameters: -- filepath (str): filepath where the serialized state should be saved. Notes: The method uses `torch.save` to serialize the state of the q-network, the optimizer, as well as the dictionary of agent hyperparameters. &quot;&quot;&quot; checkpoint = { &quot;q-network-state&quot;: self._online_q_network.state_dict(), &quot;optimizer-state&quot;: self._optimizer.state_dict(), &quot;agent-hyperparameters&quot;: { &quot;alpha&quot;: self._alpha, &quot;batch_size&quot;: self._memory.batch_size, &quot;buffer_size&quot;: self._memory.buffer_size, &quot;gamma&quot;: self._gamma, &quot;update_frequency&quot;: self._update_frequency } } torch.save(checkpoint, filepath) def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot; Updates the agent&#39;s state based on feedback received from the environment. Parameters: -- state (np.array): the previous state of the environment. action (int): the action taken by the agent in the previous state. reward (float): the reward received from the environment. next_state (np.array): the resulting state of the environment following the action. done (bool): True is the training episode is finised; false otherwise. &quot;&quot;&quot; experience = Experience(state, action, reward, next_state, done) self._memory.append(experience) if done: self._number_episodes += 1 else: self._number_timesteps += 1 # every so often the agent should learn from experiences if self._number_timesteps % self._update_frequency == 0 and self.has_sufficient_experience(): experiences = self._memory.sample() self.learn(experiences) . The code for the training loop remains unchanged from the previous post. . import collections import typing import gym def _train_for_at_most(agent: Agent, env: gym.Env, max_timesteps: int) -&gt; int: &quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot; state = env.reset() score = 0 for t in range(max_timesteps): action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward if done: break return score def _train_until_done(agent: Agent, env: gym.Env) -&gt; float: &quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot; state = env.reset() score = 0 done = False while not done: action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward return score def train(agent: Agent, env: gym.Env, checkpoint_filepath: str, target_score: float, number_episodes: int, maximum_timesteps=None) -&gt; typing.List[float]: &quot;&quot;&quot; Reinforcement learning training loop. Parameters: -- agent (Agent): an agent to train. env (gym.Env): an environment in which to train the agent. checkpoint_filepath (str): filepath used to save the state of the trained agent. number_episodes (int): maximum number of training episodes. maximum_timsteps (int): maximum number of timesteps per episode. Returns: -- scores (list): collection of episode scores from training. &quot;&quot;&quot; scores = [] most_recent_scores = collections.deque(maxlen=100) for i in range(number_episodes): if maximum_timesteps is None: score = _train_until_done(agent, env) else: score = _train_for_at_most(agent, env, maximum_timesteps) scores.append(score) most_recent_scores.append(score) average_score = sum(most_recent_scores) / len(most_recent_scores) if average_score &gt;= target_score: print(f&quot; nEnvironment solved in {i:d} episodes! tAverage Score: {average_score:.2f}&quot;) agent.save(checkpoint_filepath) break if (i + 1) % 100 == 0: print(f&quot; rEpisode {i + 1} tAverage Score: {average_score:.2f}&quot;) return scores . Solving the LunarLander-v2 environment . In the rest of this blog post I will use the Double DQN algorithm to train an agent to solve the LunarLander-v2 environment from OpenAI and the compare it to the the results obtained using the vanilla DQN algorithm. . In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land &quot;gracefully&quot; (and not crash in the landing area!). . A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered &quot;solved&quot; if the lander is able to achieve 200 points (I will actually be more stringent and define &quot;solved&quot; as achieving over 200 points on average in the most recent 100 training episodes). . Action Space . There are four discrete actions available: . Do nothing. | Fire the left orientation engine. | Fire main engine. | Fire the right orientation engine. | Colab specific environment setup . If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment. . !pip install gym[box2d]==0.17.* . import gym env = gym.make(&#39;LunarLander-v2&#39;) _ = env.seed(42) . /Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32 warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;)) . Creating a DeepQAgent . Before creating an instance of the DeepQAgent I need to define an $ epsilon$-decay schedule and choose an optimizer. . Epsilon decay schedule . As was the case with the DQN algorithm, when using the Double DQN algorithm the agent chooses its action using an $ epsilon$-greedy policy. When using an $ epsilon$-greedy policy, with probability $ epsilon$, the agent explores the state space by choosing an action uniformly at random from the set of feasible actions; with probability $1- epsilon$, the agent exploits its current knowledge by choosing the optimal action given that current state. . As the agent learns and acquires additional knowledge about it environment it makes sense to decrease exploration and increase exploitation by decreasing $ epsilon$. In practice, it isn&#39;t a good idea to decrease $ epsilon$ to zero; instead one typically decreases $ epsilon$ over time according to some schedule until it reaches some minimum value. . def power_decay_schedule(episode_number: int, decay_factor: float, minimum_epsilon: float) -&gt; float: &quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot; return max(decay_factor**episode_number, minimum_epsilon) _epsilon_decay_schedule_kwargs = { &quot;decay_factor&quot;: 0.99, &quot;minimum_epsilon&quot;: 1e-2, } epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs) . Choosing an optimizer . As is the case in training any neural network, the choice of optimizer and the tuning of its hyper-parameters (in particular the learning rate) is important. Here I am going to use the Adam optimizer. In my previous post on the DQN algorithm I used RMSProp. In my experiments I found that the Adam optimizer significantly improves the efficiency and stability of both the Double DQN and DQN algorithms compared with RMSProp (on this task at least!). In fact it seemed that the improvements in terms of efficiency and stability from using the Adam optimizer instead of RMSProp optimzer were more important than any gains from using Double DQN instead of DQN. . from torch import optim _optimizer_kwargs = { &quot;lr&quot;: 1e-3, &quot;betas&quot;: (0.9, 0.999), &quot;eps&quot;: 1e-08, &quot;weight_decay&quot;: 0, &quot;amsgrad&quot;: False, } optimizer_fn = lambda parameters: optim.Adam(parameters, **_optimizer_kwargs) . Training the DeepQAgent using Double DQN . Now I am finally ready to train the deep_q_agent. The target score for the LunarLander-v2 environment is 200 points on average for at least 100 consecutive episodes. If the deep_q_agent is able to &quot;solve&quot; the environment, then training will terminate early. . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;alpha&quot;: 1e-3, &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;double_dqn&quot;: True, # True uses Double DQN; False uses DQN &quot;seed&quot;: None, } double_dqn_agent = DeepQAgent(**_agent_kwargs) double_dqn_scores = train(double_dqn_agent, env, &quot;double-dqn-checkpoint.pth&quot;, number_episodes=2000, target_score=200) . Episode 100 Average Score: -170.99 Episode 200 Average Score: -96.48 Episode 300 Average Score: -58.06 Episode 400 Average Score: -37.71 Episode 500 Average Score: -36.62 Episode 600 Average Score: -9.45 Episode 700 Average Score: 85.52 Episode 800 Average Score: 113.89 Episode 900 Average Score: 169.01 Episode 1000 Average Score: 193.31 Episode 1100 Average Score: 151.43 Episode 1200 Average Score: 198.34 Episode 1300 Average Score: 88.76 Environment solved in 1398 episodes! Average Score: 205.27 . Training the DeepQAgent using DQN . Next I will create another DeepQAgent and train it using the original DQN algorithm for comparison. . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;alpha&quot;: 1e-3, &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;double_dqn&quot;: False, # True uses Double DQN; False uses DQN &quot;seed&quot;: None, } dqn_agent = DeepQAgent(**_agent_kwargs) dqn_scores = train(dqn_agent, env, &quot;dqn-checkpoint.pth&quot;, number_episodes=2000, target_score=200) . Episode 100 Average Score: -154.20 Episode 200 Average Score: -52.41 Episode 300 Average Score: 42.07 Episode 400 Average Score: 15.85 Episode 500 Average Score: 149.99 Environment solved in 581 episodes! Average Score: 201.22 . Comparing DQN and Double DQN . To make it a bit easier to compare the overall performance of the two algorithms I will now re-train both agents for the same number of episodes (rather than training for the minimum number of episodes required to achieve a target score). . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;alpha&quot;: 1e-3, &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;double_dqn&quot;: True, &quot;seed&quot;: None, } double_dqn_agent = DeepQAgent(**_agent_kwargs) double_dqn_scores = train(double_dqn_agent, env, &quot;double-dqn-checkpoint.pth&quot;, number_episodes=2000, target_score=float(&quot;inf&quot;), # hack to insure that training never terminates early ) . Episode 100 Average Score: -167.21 Episode 200 Average Score: -126.37 Episode 300 Average Score: -40.27 Episode 400 Average Score: 62.17 Episode 500 Average Score: 198.44 Episode 600 Average Score: 220.72 Episode 700 Average Score: 235.76 Episode 800 Average Score: 239.27 Episode 900 Average Score: 227.32 Episode 1000 Average Score: 238.04 Episode 1100 Average Score: 230.81 Episode 1200 Average Score: 241.14 Episode 1300 Average Score: 241.82 Episode 1400 Average Score: 240.67 Episode 1500 Average Score: 248.40 Episode 1600 Average Score: 255.64 Episode 1700 Average Score: 257.87 Episode 1800 Average Score: 262.54 Episode 1900 Average Score: 252.53 Episode 2000 Average Score: 251.59 . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;alpha&quot;: 1e-3, &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;double_dqn&quot;: False, &quot;seed&quot;: None, } dqn_agent = DeepQAgent(**_agent_kwargs) dqn_scores = train(dqn_agent, env, &quot;dqn-checkpoint.pth&quot;, number_episodes=2000, target_score=float(&quot;inf&quot;)) . Episode 100 Average Score: -168.37 Episode 200 Average Score: -87.11 Episode 300 Average Score: -19.44 Episode 400 Average Score: 20.67 Episode 500 Average Score: 128.34 Episode 600 Average Score: 173.04 Episode 700 Average Score: 146.70 Episode 800 Average Score: 144.91 Episode 900 Average Score: 217.99 Episode 1000 Average Score: 209.41 Episode 1100 Average Score: 214.15 Episode 1200 Average Score: 219.80 Episode 1300 Average Score: 217.97 Episode 1400 Average Score: 242.82 Episode 1500 Average Score: 239.33 Episode 1600 Average Score: 234.26 Episode 1700 Average Score: 221.27 Episode 1800 Average Score: 239.80 Episode 1900 Average Score: 256.01 Episode 2000 Average Score: 266.62 . Plotting the time series of scores . I can use Pandas to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score. . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . dqn_scores = pd.Series(dqn_scores, name=&quot;scores&quot;) double_dqn_scores = pd.Series(double_dqn_scores, name=&quot;scores&quot;) . fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True) _ = dqn_scores.plot(ax=axes[0], label=&quot;DQN Scores&quot;) _ = (dqn_scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=axes[0])) _ = axes[0].legend() _ = axes[0].set_ylabel(&quot;Score&quot;) _ = double_dqn_scores.plot(ax=axes[1], label=&quot;Double DQN Scores&quot;) _ = (double_dqn_scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=axes[1])) _ = axes[1].legend() _ = axes[1].set_ylabel(&quot;Score&quot;) _ = axes[1].set_xlabel(&quot;Episode Number&quot;) . Kernel density plot of the scores . In general, the kernel density plot will be bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent &quot;solved&quot; the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score). . Depending, you may see that the distribution of scores for Double DQN has a significantly higher positive mode and lower negative mode when compared to the distribution for DQN which indicates that the agent trained with Double DQN solved the task more frequently and crashed and burned less frequently than the agent trained with DQN. . fig, ax = plt.subplots(1,1) _ = dqn_scores.plot(kind=&quot;kde&quot;, ax=ax, label=&quot;DQN&quot;) _ = double_dqn_scores.plot(kind=&quot;kde&quot;, ax=ax, label=&quot;Double DQN&quot;) _ = ax.set_xlabel(&quot;Score&quot;) _ = ax.legend() . Where to go from here? . In a future post I plan to cover Prioritized Experience Replay which improves the sampling scheme used by the ExperienceReplayBuffer so as to replay important transitions more frequently which should lead to more efficient learning. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/11/double-dqn.html",
            "relUrl": "/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/11/double-dqn.html",
            "date": " • Apr 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Getting familiar with deep Q-networks",
            "content": "Motivation . I am currently using my COVID-19 imposed quarantine to expand my deep learning skills by completing the Deep Reinforcement Learning Nanodegree from Udacity. This past week I have been working my way through the seminal 2015 Nature paper from researchers at Deepmind Human-level control through deep reinforcement learning (Minh et al 2015). . Why is Minh et al 2015 important? . While Minh et al 2015 was not the first paper to use neural networks to approximate the action-value function, this paper was the first to demonstrate that the same neural network architecture could be trained in a computationally efficient manner to &quot;solve&quot; a large number or different tasks. . The paper also contributed several practical &quot;tricks&quot; for getting deep neural networks to consistently converge during training. This was a non-trivial contribution as issues with training convergence had plaugued previous attempts to use neural networks as function approximators in reinforcement learning tasks and were blocking widespread adoption of deep learning techniques within the reinforcemnt learning community. . Summary of the paper . Minh et al 2015 uses deep (convolutional) neural network to approximate the optimal action-value function . $$ Q^*(s, a) = max_{ pi} mathbb{E} Bigg[ sum_{s=0}^{ infty} gamma^s r_{t+s} | s_t=s, a_t=a, pi Bigg] $$ . which is the maximum sum of rewards $r_t$ discounted by $ gamma$ at each time-step $t$ achievable by a behaviour policy $ pi = P(a|s)$, after making an observation of the state $s$ and taking an action $a$. . Prior to this seminal paper it was well known that standard reinforcement learning algorithms were unstable or even diverged when a non-linear function approximators such as a neural networks were used to represent the action-value function $Q$. Why? . Minh et al 2015 discuss several reasons. . Correlations present in the sequence of observations of the state $s$. In reinforcement learning applications the sequence state observations is a time-series which will almost surely be auto-correlated. But surely this would also be true of any application of deep neural networks to model time series data. | Small updates to $Q$ may significantly change the policy, $ pi$ and therefore change the data distribution. | Correlations between the action-values, $Q$, and the target values $r + gamma max_{a&#39;} Q(s&#39;, a&#39;)$ | In the paper the authors address these issues by using... . a biologically inspired mechanism they refer to as experience replay that randomizes over the data which removes correlations in the sequence of observations of the state $s$ and smoothes over changes in the data distribution (issues 1 and 2 above). | an iterative update rule that adjusts the action-values, $Q$, towards target values, $Q&#39;$ that are only periodically updated thereby reducing correlations with the target (issue 3 above). | . Approximating the action-value function, $Q(s,a)$ . There are several possible ways of approximating the action-value function $Q$ using a neural network. The only input to the DQN architecture is the state representation and the output layer has a separate output for each possible action. The output units correspond to the predicted $Q$-values of the individual actions for the input state. A representaion of the DQN architecture from the paper is reproduced in the figure below. . . The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map $ phi$. The network has four hidden layers: . Convolutional layer with 32 filters (each of which uses an 8 x 8 kernel and a stride of 4) and a ReLU activation function. | Convolutional layer with 64 filters (each of which using a 4 x 4 kernel with stride of 2) and a ReLU activation function. | Convolutional layer with 64 filters (each of which uses a 3 x 3 kernel and a stride of 1) and a ReLU activation function. | Fully-connected (i.e., dense) layer with 512 neurons followed by a ReLU activation function. | . The output layer is another fully-connected layer with a single output for each action. A PyTorch implementation of the DQN architecture would look something like the following. . import typing import torch from torch import nn QNetwork = nn.Module class LambdaLayer(nn.Module): def __init__(self, f): super().__init__() self._f = f def forward(self, X): return self._f(X) def make_deep_q_network_fn(action_size: int) -&gt; typing.Callable[[], QNetwork]: def deep_q_network_fn() -&gt; QNetwork: q_network = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4), nn.ReLU(), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1), nn.ReLU(), LambdaLayer(lambda tensor: tensor.view(tensor.size(0), -1)), nn.Linear(in_features=25024, out_features=512), nn.ReLU(), nn.Linear(in_features=512, out_features=action_size) ) return q_network return deep_q_network_fn . The Loss Function . The $Q$-learning update at iteration $i$ uses the following loss function . $$ mathcal{L_i}( theta_i) = mathbb{E}_{(s, a, r, s&#39;) sim U(D)} Bigg[ bigg(r + gamma max_{a&#39;} Q big(s&#39;, a&#39;; theta_i^{-} big) - Q big(s, a; theta_i big) bigg)^2 Bigg] $$ . where $ gamma$ is the discount factor determining the agent’s horizon, $ theta_i$ are the parameters of the $Q$-network at iteration $i$ and $ theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $ theta_i^{-}$ are only updated with the $Q$-network parameters $ theta_i$ every $C$ steps and are held fixed between individual updates. . Experience Replay . To perform experience replay the authors store the agent&#39;s experiences $e_t$ as represented by the tuple . $$ e_t = (s_t, a_t, r_t, s_{t+1}) $$ . consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences. . $$ D_t = {e1, e2, ..., e_t } $$ . Depending on the task it may note be feasible for the agent to store the entire history of past experiences. . During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s&#39;)$, drawn uniformly at random from the pool of stored samples $D_t$. . The following is my Python implmentation of these ideas. . import collections import typing import numpy as np _field_names = [ &quot;state&quot;, &quot;action&quot;, &quot;reward&quot;, &quot;next_state&quot;, &quot;done&quot; ] Experience = collections.namedtuple(&quot;Experience&quot;, field_names=_field_names) class ExperienceReplayBuffer: &quot;&quot;&quot;Fixed-size buffer to store experience tuples.&quot;&quot;&quot; def __init__(self, batch_size: int, buffer_size: int = None, random_state: np.random.RandomState = None) -&gt; None: &quot;&quot;&quot; Initialize an ExperienceReplayBuffer object. Parameters: -- buffer_size (int): maximum size of buffer batch_size (int): size of each training batch seed (int): random seed &quot;&quot;&quot; self._batch_size = batch_size self._buffer_size = buffer_size self._buffer = collections.deque(maxlen=buffer_size) self._random_state = np.random.RandomState() if random_state is None else random_state def __len__(self) -&gt; int: return len(self._buffer) @property def batch_size(self) -&gt; int: return self._batch_size @property def buffer_size(self) -&gt; int: return self._buffer_size def is_full(self) -&gt; bool: return len(self._buffer) == self._buffer_size def append(self, experience: Experience) -&gt; None: &quot;&quot;&quot;Add a new experience to memory.&quot;&quot;&quot; self._buffer.append(experience) def sample(self) -&gt; typing.List[Experience]: &quot;&quot;&quot;Randomly sample a batch of experiences from memory.&quot;&quot;&quot; idxs = self._random_state.randint(len(self._buffer), size=self._batch_size) experiences = [self._buffer[idx] for idx in idxs] return experiences . The Deep Q-Network Algorithm . The following is Python pseudo-code for the Deep Q-Network (DQN) algorithm. For more fine-grained details of the DQN algorithm see the methods section of Minh et al 2015. . # hyper-parameters batch_size = 32 # number of experience tuples used in computing the gradient descent parameter update. buffer_size = 10000 # number of experience tuples stored in the replay buffer gamma = 0.99 # discount factor used in the Q-learning update target_network_update_frequency = 4 # frequency (measured in parameter updates) with which target network is updated. update_frequency = 4 # frequency (measured in number of timesteps) with which q-network parameters are updated. # initilizing the various data structures replay_buffer = ExperienceReplayBuffer(batch_size, buffer_size, seed) local_q_network = initialize_q_network() target_q_network = initialize_q_network() synchronize_q_networks(target_q_network, local_q_network) for i in range(number_episodes) # initialize the environment state state = env.reset() # simulate a single training episode done = False timesteps = 0 parameter_updates = 0 while not done: # greedy action based on Q(s, a; theta) action = agent.choose_epsilon_greedy_action(state) # update the environment based on the chosen action next_state, reward, done = env.step(action) # agent records experience in its replay buffer experience = (state, action, reward, next_state, done) agent.replay_buffer.append(experience) # agent samples a mini-batch of experiences from its replay buffer experiences = agent.replay_buffer.sample() states, actions, rewards, next_states, dones = experiences # agent learns every update_frequency timesteps if timesteps % update_frequency == 0: # compute the Q^- values using the Q-learning formula target_q_values = q_learning_update(target_q_network, rewards, next_states, dones) # compute the Q values local_q_values = local_q_network(states, actions) # agent updates the parameters theta using gradient descent loss = mean_squared_error(target_q_values, local_q_values) gradient_descent_update(loss) parameter_updates += 1 # every target_network_update_frequency timesteps set theta^- = theta if parameter_updates % target_network_update_frequency == 0: synchronize_q_networks(target_q_network, local_q_network) . Solving the LunarLander-v2 environment . Google Colab Preamble . If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment. . %%bash # install required system dependencies apt-get install -y xvfb x11-utils # install required python dependencies (might need to install additional gym extras depending) pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* . The code in the cell below creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set visible=False. . This code only needs to be run once per session to start the display. . import pyvirtualdisplay _display = pyvirtualdisplay.Display(visible=False, # use False with Xvfb size=(1400, 900)) _ = _display.start() . Binder Preamble . If you are running this code on Binder, then there isn&#39;t really much to do as all the software is pre-installed. However you do still need to run the code in the cell below to creates a virtual display in the background that your Gym Envs can connect to for rendering. You can adjust the size of the virtual buffer as you like but you must set visible=False. . This code only needs to be run once per session to start the display. . import pyvirtualdisplay _display = pyvirtualdisplay.Display(visible=False, # use False with Xvfb size=(1400, 900)) _ = _display.start() . Creating the Gym environment . In the rest of this blog post I will use the DQN algorithm to train an agent to solve the LunarLander-v2 environment from OpenAI. . In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land &quot;gracefully&quot; (and not crash in the landing area!). . A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered &quot;solved&quot; if the lander is able to achieve 200 points (I will actually be more stringent and define &quot;solved&quot; as achieving over 200 points on average in the most recent 100 training episodes). . Action Space . There are four discrete actions available: . Do nothing. | Fire the left orientation engine. | Fire main engine. | Fire the right orientation engine. | import gym env = gym.make(&#39;LunarLander-v2&#39;) _ = env.seed(42) . AttributeError Traceback (most recent call last) &lt;ipython-input-3-9d3fa797859f&gt; in &lt;module&gt; 1 import gym 2 -&gt; 3 env = gym.make(&#39;LunarLander-v2&#39;) 4 _ = env.seed(42) ~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/envs/registration.py in make(id, **kwargs) 140 141 def make(id, **kwargs): --&gt; 142 return registry.make(id, **kwargs) 143 144 def spec(id): ~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/envs/registration.py in make(self, path, **kwargs) 85 logger.info(&#39;Making new env: %s&#39;, path) 86 spec = self.spec(path) &gt; 87 env = spec.make(**kwargs) 88 # We used to have people override _reset/_step rather than 89 # reset/step. Set _gym_disable_underscore_compat = True on ~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/envs/registration.py in make(self, **kwargs) 56 env = self.entry_point(**_kwargs) 57 else: &gt; 58 cls = load(self.entry_point) 59 env = cls(**_kwargs) 60 ~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/envs/registration.py in load(name) 16 mod_name, attr_name = name.split(&#34;:&#34;) 17 mod = importlib.import_module(mod_name) &gt; 18 fn = getattr(mod, attr_name) 19 return fn 20 AttributeError: module &#39;gym.envs.box2d&#39; has no attribute &#39;LunarLander&#39; . Defining a generic Agent and train loop . In the cell below I define a fairly generic training loop for training and Agent to solve a task in a given gym.Env environment. In working through the hands-on portions of the Udacity Deep Reinforcement Learning Nanodegree I found myself writing similar code over and over again to train the agent to solve a task. This is my first attempt to write something that I might be able to reuse on the course going forward. . class Agent: def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot;Rule for choosing an action given the current state of the environment.&quot;&quot;&quot; raise NotImplementedError def save(self, filepath) -&gt; None: &quot;&quot;&quot;Save any important agent state to a file.&quot;&quot;&quot; raise NotImplementedError def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot;Update agent&#39;s state after observing the effect of its action on the environment.&quot;&quot;&quot; raise NotImplmentedError . def _train_for_at_most(agent: Agent, env: gym.Env, max_timesteps: int) -&gt; int: &quot;&quot;&quot;Train agent for a maximum number of timesteps.&quot;&quot;&quot; state = env.reset() score = 0 for t in range(max_timesteps): action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward if done: break return score def _train_until_done(agent: Agent, env: gym.Env) -&gt; float: &quot;&quot;&quot;Train the agent until the current episode is complete.&quot;&quot;&quot; state = env.reset() score = 0 done = False while not done: action = agent.choose_action(state) next_state, reward, done, _ = env.step(action) agent.step(state, action, reward, next_state, done) state = next_state score += reward return score def train(agent: Agent, env: gym.Env, checkpoint_filepath: str, target_score: float, number_episodes: int, maximum_timesteps=None) -&gt; typing.List[float]: &quot;&quot;&quot; Reinforcement learning training loop. Parameters: -- agent (Agent): an agent to train. env (gym.Env): an environment in which to train the agent. checkpoint_filepath (str): filepath used to save the state of the trained agent. number_episodes (int): maximum number of training episodes. maximum_timsteps (int): maximum number of timesteps per episode. Returns: -- scores (list): collection of episode scores from training. &quot;&quot;&quot; scores = [] most_recent_scores = collections.deque(maxlen=100) for i in range(number_episodes): if maximum_timesteps is None: score = _train_until_done(agent, env) else: score = _train_for_at_most(agent, env, maximum_timesteps) scores.append(score) most_recent_scores.append(score) average_score = sum(most_recent_scores) / len(most_recent_scores) if average_score &gt;= target_score: print(f&quot; nEnvironment solved in {i:d} episodes! tAverage Score: {average_score:.2f}&quot;) agent.save(checkpoint_filepath) break if (i + 1) % 100 == 0: print(f&quot; rEpisode {i + 1} tAverage Score: {average_score:.2f}&quot;) return scores . Creating a DeepQAgent . The code in the cell below encapsulates much of the logic of the DQN algorithm in a DeepQAgent class. Since the LunarLander-v2 task is not well suited for convolutional neural networks, the agent uses a simple three layer dense neural network with ReLU activation functions to approximate the action-value function $Q$. . from torch import optim from torch.nn import functional as F class DeepQAgent(Agent): def __init__(self, state_size: int, action_size: int, number_hidden_units: int, optimizer_fn: typing.Callable[[typing.Iterable[torch.nn.Parameter]], optim.Optimizer], batch_size: int, buffer_size: int, epsilon_decay_schedule: typing.Callable[[int], float], alpha: float, gamma: float, update_frequency: int, seed: int = None) -&gt; None: &quot;&quot;&quot; Initialize a DeepQAgent. Parameters: -- state_size (int): the size of the state space. action_size (int): the size of the action space. number_hidden_units (int): number of units in the hidden layers. optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer. batch_size (int): number of experience tuples in each mini-batch. buffer_size (int): maximum number of experience tuples stored in the replay buffer. epsilon_decay_schdule (callable): function that takes episode number and returns epsilon. alpha (float): rate at which the target q-network parameters are updated. gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1). update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated. seed (int): random seed &quot;&quot;&quot; self._state_size = state_size self._action_size = action_size self._device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # set seeds for reproducibility self._random_state = np.random.RandomState() if seed is None else np.random.RandomState(seed) if seed is not None: torch.manual_seed(seed) if torch.cuda.is_available(): torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # initialize agent hyperparameters self._experience_replay_buffer = ExperienceReplayBuffer(batch_size, buffer_size, seed) self._epsilon_decay_schedule = epsilon_decay_schedule self._alpha = alpha self._gamma = gamma # initialize Q-Networks self._update_frequency = update_frequency self._local_q_network = self._initialize_q_network(number_hidden_units) self._target_q_network = self._initialize_q_network(number_hidden_units) self._synchronize_q_networks() # send the networks to the device self._local_q_network.to(self._device) self._target_q_network.to(self._device) # initialize the optimizer self._optimizer = optimizer_fn(self._local_q_network.parameters()) # initialize some counters self._number_episodes = 0 self._number_timesteps = 0 self._number_parameter_updates = 0 def _initialize_q_network(self, number_hidden_units: int) -&gt; nn.Module: &quot;&quot;&quot;Create a neural network for approximating the action-value function.&quot;&quot;&quot; q_network = nn.Sequential( nn.Linear(in_features=self._state_size, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units), nn.ReLU(), nn.Linear(in_features=number_hidden_units, out_features=self._action_size) ) return q_network def _learn_from(self, experiences: typing.List[Experience]) -&gt; None: &quot;&quot;&quot;Heart of the Deep Q-learning algorithm.&quot;&quot;&quot; states, actions, rewards, next_states, dones = (torch.Tensor(vs).to(self._device) for vs in zip(*experiences)) # get max predicted Q values (for next states) from target model next_target_q_values, _ = (self._target_q_network(next_states) .detach() .max(dim=1)) # compute the new Q&#39; values using the Q-learning formula target_q_values = rewards + (self._gamma * next_target_q_values * (1 - dones)) # get expected Q values from local model _index = (actions.long() .unsqueeze(dim=1)) expected_q_values = (self._local_q_network(states) .gather(dim=1, index=_index)) # compute the mean squared loss loss = F.mse_loss(expected_q_values, target_q_values.unsqueeze(dim=1)) # agent updates the parameters theta of Q using gradient descent self._optimizer.zero_grad() loss.backward() self._optimizer.step() self._soft_update_target_q_network_parameters() def _soft_update_target_q_network_parameters(self) -&gt; None: &quot;&quot;&quot;Soft-update of target q-network parameters with the local q-network parameters.&quot;&quot;&quot; for target_param, local_param in zip(self._target_q_network.parameters(), self._local_q_network.parameters()): target_param.data.copy_(self._alpha * local_param.data + (1 - self._alpha) * target_param.data) def _synchronize_q_networks(self) -&gt; None: &quot;&quot;&quot;Synchronize the target_q_network and the local_q_network.&quot;&quot;&quot; _ = self._target_q_network.load_state_dict(self._local_q_network.state_dict()) def _uniform_random_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action uniformly at random.&quot;&quot;&quot; return self._random_state.randint(self._action_size) def _greedy_policy(self, state: torch.Tensor) -&gt; int: &quot;&quot;&quot;Choose an action that maximizes the action_values given the current state.&quot;&quot;&quot; # evaluate the network to compute the action values self._local_q_network.eval() with torch.no_grad(): action_values = self._local_q_network(state) self._local_q_network.train() # choose the greedy action action = (action_values.cpu() # action_values might reside on the GPU! .argmax() .item()) return action def _epsilon_greedy_policy(self, state: torch.Tensor, epsilon: float) -&gt; int: &quot;&quot;&quot;With probability epsilon explore randomly; otherwise exploit knowledge optimally.&quot;&quot;&quot; if self._random_state.random() &lt; epsilon: action = self._uniform_random_policy(state) else: action = self._greedy_policy(state) return action def choose_action(self, state: np.array) -&gt; int: &quot;&quot;&quot; Return the action for given state as per current policy. Parameters: -- state (np.array): current state of the environment. Return: -- action (int): an integer representing the chosen action. &quot;&quot;&quot; # need to reshape state array and convert to tensor state_tensor = (torch.from_numpy(state) .unsqueeze(dim=0) .to(self._device)) # choose uniform at random if agent has insufficient experience if not self.has_sufficient_experience(): action = self._uniform_random_policy(state_tensor) else: epsilon = self._epsilon_decay_schedule(self._number_episodes) action = self._epsilon_greedy_policy(state_tensor, epsilon) return action def has_sufficient_experience(self) -&gt; bool: &quot;&quot;&quot;True if agent has enough experience to train on a batch of samples; False otherwise.&quot;&quot;&quot; return len(self._experience_replay_buffer) &gt;= self._experience_replay_buffer.batch_size def save(self, filepath: str) -&gt; None: &quot;&quot;&quot; Saves the state of the DeepQAgent. Parameters: -- filepath (str): filepath where the serialized state should be saved. Notes: The method uses `torch.save` to serialize the state of the q-network, the optimizer, as well as the dictionary of agent hyperparameters. &quot;&quot;&quot; checkpoint = { &quot;q-network-state&quot;: self._local_q_network.state_dict(), &quot;optimizer-state&quot;: self._optimizer.state_dict(), &quot;agent-hyperparameters&quot;: { &quot;alpha&quot;: self._alpha, &quot;batch_size&quot;: self._experience_replay_buffer.batch_size, &quot;buffer_size&quot;: self._experience_replay_buffer.buffer_size, &quot;gamma&quot;: self._gamma, &quot;update_frequency&quot;: self._update_frequency } } torch.save(checkpoint, filepath) def step(self, state: np.array, action: int, reward: float, next_state: np.array, done: bool) -&gt; None: &quot;&quot;&quot; Updates the agent&#39;s state based on feedback received from the environment. Parameters: -- state (np.array): the previous state of the environment. action (int): the action taken by the agent in the previous state. reward (float): the reward received from the environment. next_state (np.array): the resulting state of the environment following the action. done (bool): True is the training episode is finised; false otherwise. &quot;&quot;&quot; # save experience in the experience replay buffer experience = Experience(state, action, reward, next_state, done) self._experience_replay_buffer.append(experience) if done: self._number_episodes += 1 else: self._number_timesteps += 1 # every so often the agent should learn from experiences if self._number_timesteps % self._update_frequency == 0 and self.has_sufficient_experience(): experiences = self._experience_replay_buffer.sample() self._learn_from(experiences) . Epsilon decay schedule . In the DQN algorithm the agent chooses its action using an $ epsilon$-greedy policy. When using an $ epsilon$-greedy policy, with probability $ epsilon$, the agent explores the state space by choosing an action uniformly at random from the set of feasible actions; with probability $1- epsilon$, the agent exploits its current knowledge by choosing the optimal action given that current state. . As the agent learns and acquires additional knowledge about it environment it makes sense to decrease exploration and increase exploitation by decreasing $ epsilon$. In practice, it isn&#39;t a good idea to decrease $ epsilon$ to zero; instead one typically decreases $ epsilon$ over time according to some schedule until it reaches some minimum value. . The Deepmind researchers used a simple linear decay schedule and set a minimum value of $ epsilon=0.1$. In the cell below I code up a linear decay schedule as well as a power decay schedule that I have seen used in many other practical applications. . def linear_decay_schedule(episode_number: int, slope: float, minimum_epsilon: float) -&gt; float: &quot;&quot;&quot;Simple linear decay schedule used in the Deepmind paper.&quot;&quot;&quot; return max(1 - slope * episode_number, minimum_epsilon) def power_decay_schedule(episode_number: int, decay_factor: float, minimum_epsilon: float) -&gt; float: &quot;&quot;&quot;Power decay schedule found in other practical applications.&quot;&quot;&quot; return max(decay_factor**episode_number, minimum_epsilon) _epsilon_decay_schedule_kwargs = { &quot;decay_factor&quot;: 0.995, &quot;minimum_epsilon&quot;: 1e-2, } epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs) . Choosing an optimizer . As is the case in training any neural network, the choice of optimizer and the tuning of its hyper-parameters (in particular the learning rate) is important. Here I am going to more or less follow the Minh et al 2015 paper and use the RMSProp optimizer. . _optimizer_kwargs = { &quot;lr&quot;: 1e-2, &quot;alpha&quot;: 0.99, &quot;eps&quot;: 1e-08, &quot;weight_decay&quot;: 0, &quot;momentum&quot;: 0, &quot;centered&quot;: False } optimizer_fn = lambda parameters: optim.RMSprop(parameters, **_optimizer_kwargs) . At this point I am ready to create an instance of the DeepQAgent. . _agent_kwargs = { &quot;state_size&quot;: env.observation_space.shape[0], &quot;action_size&quot;: env.action_space.n, &quot;number_hidden_units&quot;: 64, &quot;optimizer_fn&quot;: optimizer_fn, &quot;epsilon_decay_schedule&quot;: epsilon_decay_schedule, &quot;batch_size&quot;: 64, &quot;buffer_size&quot;: 100000, &quot;alpha&quot;: 1e-3, &quot;gamma&quot;: 0.99, &quot;update_frequency&quot;: 4, &quot;seed&quot;: None, } deep_q_agent = DeepQAgent(**_agent_kwargs) . Performance of an un-trained DeepQAgent . The function simulate defined in the cell below can be used to simuate an agent interacting with and environment for one episode. . import matplotlib.pyplot as plt from IPython import display def simulate(agent: Agent, env: gym.Env, ax: plt.Axes) -&gt; None: state = env.reset() img = ax.imshow(env.render(mode=&#39;rgb_array&#39;)) done = False while not done: action = agent.choose_action(state) img.set_data(env.render(mode=&#39;rgb_array&#39;)) plt.axis(&#39;off&#39;) display.display(plt.gcf()) display.clear_output(wait=True) state, reward, done, _ = env.step(action) env.close() . The untrained agent behaves erratically (not quite randomly!) and performs poorly. Lots of room for improvement! . _, ax = plt.subplots(1, 1, figsize=(10, 8)) simulate(deep_q_agent, env, ax) . Training the DeepQAgent . Now I am finally ready to train the deep_q_agent. The target score for the LunarLander-v2 environment is 200 points on average for at least 100 consecutive episodes. If the deep_q_agent is able to &quot;solve&quot; the environment, then training will terminate early. . scores = train(deep_q_agent, env, &quot;checkpoint.pth&quot;, number_episodes=2000, target_score=200) . Episode 100 Average Score: -149.83 Episode 200 Average Score: -103.40 Episode 300 Average Score: -56.31 Episode 400 Average Score: 9.59 Episode 500 Average Score: 82.93 Episode 600 Average Score: 133.17 Episode 700 Average Score: 135.58 Episode 800 Average Score: 128.12 Episode 900 Average Score: 164.59 Episode 1000 Average Score: 180.24 Episode 1100 Average Score: 154.35 Episode 1200 Average Score: 180.69 Episode 1300 Average Score: 180.29 Environment solved in 1388 episodes! Average Score: 201.22 . Analyzing DeepQAgent performance . Now that I have trained the agent, let&#39;s rerun the simulation to see the difference in performance. . _, ax = plt.subplots(1, 1, figsize=(10, 8)) simulate(deep_q_agent, env, ax) . Plotting the time series of scores . I can use Pandas to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score. . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . scores = pd.Series(scores, name=&quot;scores&quot;) . scores.describe() . count 1389.000000 mean 87.963719 std 177.130250 min -910.040623 25% -50.965741 50% 157.388679 75% 247.386481 max 320.713969 Name: scores, dtype: float64 . fig, ax = plt.subplots(1, 1) _ = scores.plot(ax=ax, label=&quot;Scores&quot;) _ = (scores.rolling(window=100) .mean() .rename(&quot;Rolling Average&quot;) .plot(ax=ax)) ax.axhline(200, color=&#39;k&#39;, linestyle=&quot;dashed&quot;, label=&quot;Target Score&quot;) ax.legend() _ = ax.set_xlabel(&quot;Episode Number&quot;) _ = ax.set_ylabel(&quot;Score&quot;) . Kernel density plot of the scores . Kernel density plot of scores is bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent &quot;solved&quot; the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score). . fig, ax = plt.subplots(1,1) _ = scores.plot(kind=&quot;kde&quot;, ax=ax) _ = ax.set_xlabel(&quot;Score&quot;) . Where to go from here? . I am a bit frustrated by lack of stability that I am seeing in my implmentation of the Deep Q algorithm: sometimes the algorithm converges and sometimes not. Perhaps more tuning of hyper-parameters or use of a different optimization algorithm would exhibit better convergence. I have already spent more time than I had allocated on playing around with this agorithm so I am not going to try and fine-tune the hyperparamters or explore alternative optimization algorithms for now. . Rather than spending time tuning hyperparameters I think it would be better use of my time to explore algorithmic improvements. In future posts I plan to cover the following extensions of the DQN algorithm: Double Q-Learning, Prioritized Experience Replay, and Dueling Network Architectures .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html",
            "relUrl": "/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Conda (+ pip) and Docker FTW!",
            "content": "Conda (+ pip) and Docker FTW! . Conda is an open source package and environment management system that runs on Windows, Mac OS and Linux. . Conda can quickly install, run, and update packages and their dependencies. | Conda can create, save, load, and switch between project specific software environments on your local computer. | Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN. | . Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. . While Conda is my default package and environment management solution, not every Python package that I might need to use is available via Conda. Fortunately, Conda plays nicely with pip which is the default Python package management tool. . Why not just use Conda (+ pip)? . While Conda (+ pip) solves most of my day-to-day data science environment and package management issues, incorporating Docker into my Conda (+ pip) development workflow has made it much easier to port my data science workflows from from my laptop/workstation to remote cloud computing resources. Getting Conda (+ pip) to work as expected inside Docker containers turned out to be much more challenging that I expected. . This blog post shows how I eventually combined Conda (+ pip) and Docker. In the following I assume that you have organized your project directory similar to my Python data science project template. In particular, I will assume that you store all Docker related files in a docker sub-directory within your project root directory. . Writing the Dockerfile . The trick to getting Conda (+ pip) and Docker to work smoothly together is to write a good Dockerfile. In this section I will take you step by step through the various pieces of the Dockerfile that I developed. Hopefully you can use this Dockerfile without modification on you next data science project. . Use a standard base image . Every Dockefile has a base or parent image. For the parent image I use Ubuntu 16.04 which is one of the most commonly used flavor of Linux in the data science community (and also happens to be the same OS installed on my workstation). . FROM ubuntu:16.04 . Make bash the default shell . The default shell used to run Dockerfile commands when building Docker images is /bin/sh. Unfortunately /bin/sh is currently not one of the shells supported by the conda init command. Fortunately it is possible to change the default shell used to run Dockerfile commands using the SHELL instruction. . SHELL [ &quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot; ] . Note the use of the --login flag which insures that both ~/.profile and ~/.bashrc are sourced properly. Proper sourcing of both ~/.profile and ~/.bashrc is necessary in order to use various conda commands to build the Conda environment inside the Docker image. . Create a non-root user . It is a Docker security “best practice” to create a non-root user inside your Docker images. My preferred approach to create a non-root user uses build arguments to customize the username, uid, and gidthe non-root user. I use standard defaults for the uid and gid; the default username is set to al-khawarizmi. . # Create a non-root user ARG username=al-khawarizmi ARG uid=1000 ARG gid=100 ENV USER $username ENV UID $uid ENV GID $gid ENV HOME /home/$USER RUN adduser --disabled-password --gecos &quot;Non-root user&quot; --uid $UID --gid $GID --home $HOME $USER . Copy over the config files for the Conda environment . After creating the non-root user I copy over all of the config files that I will need to create the Conda environment (i.e., environment.yml, requirements.txt, postBuild). I also copy over a Bash script that I will use as the Docker ENTRYPOINT (more on this below). . COPY environment.yml requirements.txt /tmp/ RUN chown $UID:$GID /tmp/environment.yml /tmp/requirements.txt COPY postBuild /usr/local/bin/postBuild.sh RUN chown $UID:$GID /usr/local/bin/postBuild.sh &amp;&amp; chmod u+x /usr/local/bin/postBuild.sh COPY docker/entrypoint.sh /usr/local/bin/ RUN chown $UID:$GID /usr/local/bin/entrypoint.sh &amp;&amp; chmod u+x /usr/local/bin/entrypoint.sh . Newer versions of Docker support copying files as a non-root user, however the version of Docker available on DockerHub does not yet support copying as a non-root user so if you want to setup automated builds for your Git repositories you will need to copy everything as root. . Install Miniconda as the non-root user. . After copying over the config files as root, I switch over to the non-root user and install Miniconda. . USER $USER # install miniconda ENV MINICONDA_VERSION 4.8.2 ENV CONDA_DIR $HOME/miniconda3 RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-$MINICONDA_VERSION-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; chmod +x ~/miniconda.sh &amp;&amp; ~/miniconda.sh -b -p $CONDA_DIR &amp;&amp; rm ~/miniconda.sh # make non-activate conda commands available ENV PATH=$CONDA_DIR/bin:$PATH # make conda activate command available from /bin/bash --login shells RUN echo &quot;. $CONDA_DIR/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.profile # make conda activate command available from /bin/bash --interative shells RUN conda init bash . Create a project directory . Next I create a project directory inside the non-root user home directory. The Conda environment will be created in a env sub-directory inside the project directory and all other project files and directories can then be mounted into this directory. . # create a project directory inside user home ENV PROJECT_DIR $HOME/app RUN mkdir $PROJECT_DIR WORKDIR $PROJECT_DIR . Build the Conda environment . Now I am ready to build the Conda environment. Note that I can use nearly the same sequence of conda commands that I would use to build a Conda environment for a project on my laptop or workstation. . # build the conda environment ENV ENV_PREFIX $PWD/env RUN conda update --name base --channel defaults conda &amp;&amp; conda env create --prefix $ENV_PREFIX --file /tmp/environment.yml --force &amp;&amp; conda clean --all --yes # run the postBuild script to install any JupyterLab extensions RUN conda activate $ENV_PREFIX &amp;&amp; /usr/local/bin/postBuild.sh &amp;&amp; conda deactivate . Insure Conda environment is properly activated at runtime . Almost finished! Second to last step is to use an ENTRYPOINT script to insure that the Conda environment is properly activated at runtime. . ENTRYPOINT [ &quot;/usr/local/bin/entrypoint.sh&quot; ] . Here is the /usr/local/bin/entrypoint.sh script for reference. . #!/bin/bash --login set -e conda activate $ENV_PREFIX exec &quot;$@&quot; . Specify a default command for the Docker container . Finally, I use the CMD instruction to specify a default command to run when a Docker container is launched. Since I install JupyerLab in all of my Conda environments I tend to launch a JupyterLab server by default when executing containers. . # default command will be to launch JupyterLab server for development CMD [ &quot;jupyter&quot;, &quot;lab&quot;, &quot;--no-browser&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot; ] . Building the Docker image . The following command builds a new image for your project with a custom $USER (and associated $UID and $GID) as well as a particular $IMAGE_NAME and $IMAGE_TAG. This command should be run within the docker sub-directory of the project as the Docker build context is set to ../ which should be the project root directory. . docker image build --build-arg username=$USER --build-arg uid=$UID --build-arg gid=$GID --file Dockerfile --tag $IMAGE_NAME:$IMAGE_TAG ../ . Running a Docker container . Once the image is built, the following command will run a container based on the image $IMAGE_NAME:$IMAGE_TAG. This command should be run from within the project’s root directory. . docker container run --rm --tty --volume ${pwd}/bin:/home/$USER/app/bin --volume ${pwd}/data:/home/$USER/app/data --volume ${pwd}/doc:/home/$USER/app/doc --volume ${pwd}/notebooks:/home/$USER/app/notebooks --volume ${pwd}/results:/home/$USER/app/results --volume ${pwd}/src:/home/$USER/app/src --publish 8888:8888 $IMAGE_NAME:$IMAGE_TAG . Using Docker Compose . It is quite easy to make typos whilst writing the above docker commands by hand. A less error-prone approach is to use Docker Compose. The above docker commands can be encapsulated into the docker-compose.yml configuration file as follows. . version: &quot;3.7&quot; services: jupyterlab-server: build: args: - username=${USER} - uid=${UID} - gid=${GID} context: ../ dockerfile: docker/Dockerfile ports: - &quot;8888:8888&quot; volumes: - ../bin:/home/${USER}/app/bin - ../data:/home/${USER}/app/data - ../doc:/home/${USER}/app/doc - ../notebooks:/home/${USER}/app/notebooks - ../results:/home/${USER}/app/results - ../src:/home/${USER}/app/src init: true stdin_open: true tty: true . The above docker-compose.yml file relies on variable substitution. to obtain the values for $USER, $UID, and $GID. These values can be stored in an a file called .env as follows. . USER=$USER UID=$UID GID=$GID . You can test your docker-compose.yml file by running the following command in the docker sub-directory of the project. . docker-compose config . This command takes the docker-compose.yml file and substitutes the values provided in the .env file and then returns the result. . Once you are confident that values in the .env file are being substituted properly into the docker-compose.yml file, the following command can be used to bring up a container based on your project’s Docker image and launch the JupyterLab server. This command should also be run from within the docker sub-directory of the project. . docker-compose up --build . When you are done developing and have shutdown the JupyterLab server, the following command tears down the networking infrastructure for the running container. . docker-compose down . Summary . In this post I walked through a Dockerfile that can be used to inject a Conda (+ pip) environment into into a Docker image. I also detailed how to build the resulting image and launch containers using Docker Compose. . If you are looking for a production-quality solution that generalizes the approach outlined above, then I would encourage you to check out jupyter-repo2docker. . jupyter-repo2docker is a tool to build, run, and push Docker images from source code repositories. repo2docker fetches a repository (from GitHub, GitLab, Zenodo, Figshare, Dataverse installations, a Git repository or a local directory) and builds a container image in which the code can be executed. The image build process is based on the configuration files found in the repository. . The Conda (+ pip) and Docker combination has significantly increased my data science development velocity while at the same time increasing the portability and reproducibility of my data science workflows. . Hopefully this post can help you combine these three great tools together on your next data science project! .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "relUrl": "/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Building a Conda environment for Horovod",
            "content": "What is Horovod? . Horovod is an open-source distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Horovod improves the speed, scale, and resource utilization of deep learning training. . In this post I describe how I build Conda environments for my deep learning projects where I plan to use Horovod to enable distributed training across multiple GPUs (either on the same node or spread across multuple nodes). If you like my approach then you can make use of the template repository on GitHub to get started with you rnext Horovod data science project! . Installing the NVIDIA CUDA Toolkit . First thing you need to do is to install the appropriate version of the NVIDIA CUDA Toolkit on your workstation. For this blog post I am using NVIDIA CUDA Toolkit 10.1 (documentation) which works with all three deep learning frameworks that are currently supported by Horovod. . Why not just use the cudatoolkit package? . Typically when installing PyTorch, TensorFlow, or Apache MXNet with GPU support using Conda you simply add the appropriate version cudatoolkit package to your environment.yml file. . Unfortunately, the cudatoolkit package available from conda-forge does not include NVCC and in order to use Horovod with either PyTorch, TensorFlow, or MXNet you need to compile extensions. . What about the cudatoolkit-dev package? . While there are cudatoolkit-dev packages available from conda-forge that do include NVCC, I have had difficult getting these packages to consistently install properly. . Use the nvcc_linux-64 meta-pacakge! . The most robust approach to obtain NVCC and still use Conda to manage all the other dependencies is to install the NVIDIA CUDA Toolkit on your system and then install a meta-package nvcc_linux-64 from conda-forge which configures your Conda environment to use the NVCC installed on the system together with the other CUDA Toolkit components installed inside the Conda environment. . The environment.yml file . I prefer to specify as many dependencies as possible in the Conda environment.yml file and only specify dependencies in requirements.txt that are not available via Conda channels. Check the official Horovod installation guide for details of required dependencies. . Channel Priority . I use the recommended channel priorities. Note that conda-forge has priority over defaults. . name: null channels: - pytorch - conda-forge - defaults . Dependencies . There are a few things worth noting about the dependencies. . Even though I have installed the NVIDIA CUDA Toolkit manually I still use Conda to manage the other required CUDA components such as cudnn and nccl (and the optional cupti). | I use two meta-pacakges, cxx-compiler and nvcc_linux-64, to make sure that suitable C, and C++ compilers are installed and that the resulting Conda environment is aware of the manually installed CUDA Toolkit. | Horovod requires some controller library to coordinate work between the various Horovod processes. Typically this will be some MPI implementation such as OpenMPI. However, rather than specifying the openmpi package directly I instead opt for mpi4py Conda package which provides a cuda-aware build of OpenMPI (where possible). | Horovod also support that Gloo collective communications library that can be used in place of MPI. I include cmake in order to insure that the Horovod extensions for Gloo are built. | Below are the core required dependencies. The complete environment.yml file is available on GitHub. . dependencies: - bokeh=1.4 - cmake=3.16 # insures that the Gloo library extensions will be built - cudnn=7.6 - cupti=10.1 - cxx-compiler=1.0 # meta-pacakge that insures suitable C and C++ compilers are available - jupyterlab=1.2 - mpi4py=3.0 # installs cuda-aware openmpi - nccl=2.5 - nodejs=13 - nvcc_linux-64=10.1 # meta-package that configures environment to be &quot;cuda-aware&quot; - pip=20.0 - pip: - mxnet-cu101mkl==1.6.* # makes sure MXNET is installed prior to horovod - -r file:requirements.txt - python=3.7 - pytorch=1.4 - tensorboard=2.1 - tensorflow-gpu=2.1 - torchvision=0.5 . The requirements.txt File . The requirements.txt file is where all of the pip dependencies, including Horovod itself, are listed for installation. In addition to Horovod I typically will also use pip to install JupyterLab extensions to enable GPU and CPU resource monitoring via jupyterlab-nvdashboard and Tensorboard support via jupyter-tensorboard. . horovod==0.19.* jupyterlab-nvdashboard==0.2.* # server-side component; client-side component installed in postBuild jupyter-tensorboard==0.2.* # make sure horovod is re-compiled if environment is re-built --no-binary=horovod . Note the use of the --no-binary option at the end of the file. Including this option insures that Horovod will be re-built whenever the Conda environment is re-built. . The complete requirements.txt file is available on GitHub. . Building Conda Environment . After adding any necessary dependencies that should be downloaded via conda to the environment.yml file and any dependencies that should be downloaded via pip to the requirements.txt file you create the Conda environment in a sub-directory ./envof your project directory by running the following commands. . export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force . By default Horovod will try and build extensions for all detected frameworks. See the Horovod documentation on environment variables for the details on additional environment variables that can be set prior to building Horovod. . Once the new environment has been created you can activate the environment with the following command. . conda activate $ENV_PREFIX . The postBuild File . If you wish to use any JupyterLab extensions included in the environment.yml and requirements.txt files then you need to rebuild the JupyterLab application using the following commands to source the postBuild script. . conda activate $ENV_PREFIX # optional if environment already active . postBuild . Wrapping it all up in a Bash script . I typically wrap these commands into a shell script ./bin/create-conda-env.sh. Running the shell script will set the Horovod build variables, create the Conda environment, activate the Conda environment, and built JupyterLab with any additional extensions. . #!/bin/bash --login set -e export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force conda activate $ENV_PREFIX . postBuild . I typically put scripts inside a ./bin directory in my project root directory. The script should be run from the project root directory as follows. . ./bin/create-conda-env.sh # assumes that $CUDA_HOME is set properly . Verifying the Conda environment . After building the Conda environment you can check that Horovod has been built with support for the deep learning frameworks TensorFlow, PyTorch, Apache MXNet, and the contollers MPI and Gloo with the following command. . conda activate $ENV_PREFIX # optional if environment already active horovodrun --check-build . You should see output similar to the following. . Horovod v0.19.1: Available Frameworks: [X] TensorFlow [X] PyTorch [X] MXNet Available Controllers: [X] MPI [X] Gloo Available Tensor Operations: [X] NCCL [ ] DDL [ ] CCL [X] MPI [X] Gloo . Listing the contents of the Conda environment . To see the full list of packages installed into the environment run the following command. . conda activate $ENV_PREFIX # optional if environment already active conda list . Updating the Conda environment . If you add (remove) dependencies to (from) the environment.yml file or the requirements.txt file after the environment has already been created, then you can re-create the environment with the following command. . conda env create --prefix $ENV_PREFIX --file environment.yml --force . However, whenever I add new dependencies I prefer to re-run the Bash script which will re-build both the Conda environment and JupyterLab. . ./bin/create-conda-env.sh . Summary . Finding a reproducible process for building Horovod extensions for my deep learning projects was tricky. Key to my solution is the use of meta-packages from conda-forge to insure that the appropriate compilers are installed and that the resulting Conda environment is aware of the system installed NVIDIA CUDA Toolkit. The second key is to use the --no-binary flag in the requirements.txt file to insure that Horovod is re-built whenever the Conda environment is re-built. . If you like my approach then you can make use of the template repository on GitHub to get started with your next Horovod data science project! .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "relUrl": "/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Managing Project-Specific Environments With Conda",
            "content": "Getting Started with Conda . Conda is an open source package and environment management system that runs on Windows, Mac OS and Linux. . Conda can quickly install, run, and update packages and their dependencies. | Conda can create, save, load, and switch between project specific software environments on your local computer. | Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN. | . Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. . Conda? Miniconda? Anaconda? What’s the difference? . Users are often confused about the differences between Conda, Miniconda, and Anaconda. . . I suggest installing Miniconda which combines Conda with Python 3 (and a small number of core systems packages) instead of the full Anaconda distribution. Installing only Miniconda will encourage you to create separate environments for each project (and to install only those packages that you actually need for each project!) which will enhance portability and reproducibility of your research and workflows. . Besides, if you really want a particular version of the full Anaconda distribution you can always create an new conda environment and install it using the following command. . conda create --name anaconda202002 anaconda=2020.02 . Installing Miniconda . Download the 64-bit, Python 3 version of the appropriate Miniconda installer for your operating system from and follow the instructions. I will walk through the steps for installing on Linux systems below as installing on Linux systems is slightly more involved. . Download the 64-bit Python 3 install script for Miniconda. . wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . Run the Miniconda install script. The -b runs the install script in batch mode which doesn’t require manual intervention (and assumes that the user agrees to the terms of the license). . bash Miniconda3-latest-Linux-x86_64.sh -b . Remove the install script. . rm Miniconda3-latest-Linux-x86_64 . Initializing your shell for Conda . After installing Miniconda you next need to configure your preferred shell to be “conda-aware”. . conda init bash source ~/.bashrc (base) $ # now the prompt indicates that the base environment is active! . Updating Conda . It is a good idea to keep your conda installation updated to the most recent version. . conda update --name base conda --yes . Uninstalling Miniconda . Whenever installing new software it is always a good idea to understand how to uninstall the software (just in case you have second thoughts!). Uninstalling Miniconda is fairly straighforward. . Uninitialize your shell to remove Conda related content from ~/.bashrc. . conda init --reverse bash . Remove the entire ~/miniconda3 directory. . rm -rf ~/miniconda3 . Remove the entire ~/.conda directory. . rm -rf ~/.conda . If present, remove your Conda configuration file. . rm ~/.condarc . Conda “Best Practices” . In the following section I detail a minimal set of best practices for using Conda to manage data science environments that I use in my own work. . TLDR; . Here is the basic recipe for using Conda to manage a project specific software stack. . (base) $ mkdir project-dir (base) $ cd project-dir (base) $ nano environment.yml # create the environment file (base) $ conda env create --prefix ./env --file environment.yml (base) $ conda activate ./env # activate the environment (/path/to/env) $ nano environment.yml # forgot to add some deps (/path/to/env) $ conda env update --prefix ./env --file environment.yml --prune) # update the environment (/path/to/env) $ conda deactivate # done working on project (for now!) . New project, new directory . Every new project (no matter how small!) should live in its own directory. A good reference to get started with organizing your project directory is Good Enough Practices for Scientific Computing. . mkdir project-dir cd project-dir . New project, new environment . Now that you have a new project directory you are ready to create a new environment for your project. We will do this in two steps. . Create an environment file that describes the software dependencies (including specific version numbers!) for the project. | Use the newly created environment file to build the software environment. | Here is an example of a typical environment file that could be used to run GPU accelerated, distributed training of deep learning models developed using PyTorch. . name: null channels: - pytorch - conda-forge - defaults dependencies: - cudatoolkit=10.1 - jupyterlab=1.2 - pip=20.0 - python=3.7 - pytorch=1.4 - torchvision=0.5 . Once you have created an environment.yml file inside your project directory you can use the following commands to create the environment as a sub-directory called env inside your project directory. . conda env create --prefix ./env --file environment.yml . Activating an environment . Activating environments is essential to making the software in environments work well (or sometimes at all!). Activation of an environment does two things. . Adds entries to PATH for the environment. | Runs any activation scripts that the environment may contain. | Step 2 is particularly important as activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. . conda activate ./env # activate the environment (/path/to/env) $ # now the prompt indicates which environment is active! . Updating an environment . You are unlikely to know ahead of time which packages (and version numbers!) you will need to use for your research project. For example it may be the case that… . one of your core dependencies just released a new version (dependency version number update). | you need an additional package for data analysis (add a new dependency). | you have found a better visualization package and no longer need to old visualization package (add new dependency and remove old dependency). | . If any of these occurs during the course of your research project, all you need to do is update the contents of your environment.yml file accordingly and then run the following command. . conda env update --prefix ./env --file environment.yml --prune # update the environment . Alternatively, you can simply rebuild the environment from scratch with the following command. . conda env create --prefix ./env --file environment.yml --force . Deactivating an environment . When you are done working on your project it is a good idea to deactivate the current environment. To deactivate the currently active environment use the deactivate command as follows. . conda deactivate # done working on project (for now!) (base) $ # now you are back to the base environment . Interested in Learning More? . For more details on using Conda to manage software stacks for you data science projects, checkout the Introduction to Conda for (Data) Scientists training materials that I have contributed to The Carpentries Incubator. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "relUrl": "/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "date": " • Mar 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}