{
  
    
        "post0": {
            "title": "Conda, pip, and Docker FTW!",
            "content": "Conda, pip, and docker FTW! . Environment management problem . Package management problem . Why not just use Conda (+ pip)? . The Dockerfile . For the parent image I use Ubuntu 16.04 which is one of the most commonly used flavor of Linux in the data science community (and also happens to be the same OS installed on my workstation). . FROM ubuntu:16.04 . Make bash the default shell . The default shell used to run Dockerfile commands when building Docker images is /bin/sh. Unfortunately /bin/sh is currently not one of the shells supported by the conda init command. Fortunately it is possible to change the default shell used to run Dockerfile commands using the SHELL instruction. . SHELL [ &quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot; ] . Note the use of the --login flag which insures that both ~/.profile and ~/.bashrc are sourced properly. Proper sourcing of both ~/.profile and ~/.bashrc is necessary in order to use various conda commands to build the Conda environment inside the Docker image. . Create a non-root user . It is a Docker security “best practice” to create a non-root user inside your Docker images. My preferred approach to create a non-root user uses build arguments to customize the username, uid, and gidthe non-root user. I use standard defaults for the uid and gid; the default username is set to al-khawarizmi in honor of the 9th-century Iranian mathematician Muhammad ibn Musa al-Khwarizmi . # Create a non-root user ARG username=al-khawarizmi ARG uid=1000 ARG gid=100 ENV USER $username ENV UID $uid ENV GID $gid ENV HOME /home/$USER RUN adduser --disabled-password --gecos &quot;Non-root user&quot; --uid $UID --gid $GID --home $HOME $USER . Copy over the config files for the Conda environment . After creating the non-root user I copy over all of the config files that I will need to create the Conda environment (i.e., environment.yml, requirements.txt, postBuild). I also copy over a Bash script that I will use as the Docker ENTRYPOINT (more on this below). . COPY environment.yml requirements.txt /tmp/ RUN chown $UID:$GID /tmp/environment.yml /tmp/requirements.txt COPY postBuild /usr/local/bin/postBuild.sh RUN chown $UID:$GID /usr/local/bin/postBuild.sh &amp;&amp; chmod u+x /usr/local/bin/postBuild.sh COPY docker/entrypoint.sh /usr/local/bin/ RUN chown $UID:$GID /usr/local/bin/entrypoint.sh &amp;&amp; chmod u+x /usr/local/bin/entrypoint.sh . Newer versions of Docker support copying files as a non-root user, however the version of Docker available on DockerHub does not yet support copying as a non-root user so if you want to setup automated builds for your Git repositories you will need to copy everything as root. . Install Miniconda as the non-root user. . After copying over the config files as root, I switch over to the non-root user and install Miniconda. . USER $USER # install miniconda ENV MINICONDA_VERSION 4.8.2 ENV CONDA_DIR $HOME/miniconda3 RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-$MINICONDA_VERSION-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; chmod +x ~/miniconda.sh &amp;&amp; ~/miniconda.sh -b -p $CONDA_DIR &amp;&amp; rm ~/miniconda.sh # make non-activate conda commands available ENV PATH=$CONDA_DIR/bin:$PATH # make conda activate command available from /bin/bash --login shells RUN echo &quot;. $CONDA_DIR/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.profile # make conda activate command available from /bin/bash --interative shells RUN conda init bash . Create a project directory . Next I create a project directory inside the non-root user home directory. The Conda environment will be created in a env sub-directory inside the project directory and all other project files and directories can then be mounted into this directory. . # create a project directory inside user home ENV PROJECT_DIR $HOME/app RUN mkdir $PROJECT_DIR WORKDIR $PROJECT_DIR . Build the Conda environment . Now I am ready to build the Conda environment. Note that I can use nearly the same sequence of conda commands that I would use to build a Conda environment for a project on my laptop or workstation. . # build the conda environment ENV ENV_PREFIX $PWD/env RUN conda update --name base --channel defaults conda &amp;&amp; conda env create --prefix $ENV_PREFIX --file /tmp/environment.yml --force &amp;&amp; conda clean --all --yes # run the postBuild script to install any JupyterLab extensions RUN conda activate $ENV_PREFIX &amp;&amp; /usr/local/bin/postBuild.sh &amp;&amp; conda deactivate . Insure Conda environment is properly activated at runtime . Almost finished! Second to last step is to use an ENTRYPOINT script to insure that the Conda environment is properly activated at runtime. . ENTRYPOINT [ &quot;/usr/local/bin/entrypoint.sh&quot; ] . Here is the /usr/local/bin/entrypoint.sh script for reference. . #!/bin/bash --login set -e conda activate $ENV_PREFIX exec &quot;$@&quot; . Specify a default command for the Docker container . Finally, I use the CMD instruction to specify a default command to run when a Docker container is launched. Since I install JupyerLab in all of my Conda environments I tend to launch a JupyterLab server by default when executing containers. . # default command will be to launch JupyterLab server for development CMD [ &quot;jupyter&quot;, &quot;lab&quot;, &quot;--no-browser&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot; ] .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "relUrl": "/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a Conda environment for Horovod",
            "content": "What is Horovod? . . Horovod is an open-source distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Horovod improves the speed, scale, and resource utilization of deep learning training. . In this post I describe how I build Conda environments for my deep learning projects where I plan to use Horovod to enable distributed training across multiple GPUs (either on the same node or spread across multuple nodes). If you like my approach then you can make use of the template repository on GitHub to get started with you rnext Horovod data science project! . Installing the NVIDIA CUDA Toolkit . First thing you need to do is to install the appropriate version of the NVIDIA CUDA Toolkit on your workstation. For this blog post I am using NVIDIA CUDA Toolkit 10.1 (documentation) which works with all three deep learning frameworks that are currently supported by Horovod. . Why not just use the cudatoolkit package? . Typically when installing PyTorch, TensorFlow, or Apache MXNet with GPU support using Conda you simply add the appropriate version cudatoolkit package to your environment.yml file. . Unfortunately, the cudatoolkit package available from conda-forge does not include NVCC and in order to use Horovod with either PyTorch, TensorFlow, or MXNet you need to compile extensions. . What about the cudatoolkit-dev package? . While there are cudatoolkit-dev packages available from conda-forge that do include NVCC, I have had difficult getting these packages to consistently install properly. . Use the nvcc_linux-64 meta-pacakge! . The most robust approach to obtain NVCC and still use Conda to manage all the other dependencies is to install the NVIDIA CUDA Toolkit on your system and then install a meta-package nvcc_linux-64 from conda-forge which configures your Conda environment to use the NVCC installed on the system together with the other CUDA Toolkit components installed inside the Conda environment. . The environment.yml file . I prefer to specify as many dependencies as possible in the Conda environment.yml file and only specify dependencies in requirements.txt that are not available via Conda channels. Check the official Horovod installation guide for details of required dependencies. . Channel Priority . I use the recommended channel priorities. Note that conda-forge has priority over defaults. . name: null channels: - pytorch - conda-forge - defaults . Dependencies . There are a few things worth noting about the dependencies. . Even though I have installed the NVIDIA CUDA Toolkit manually I still use Conda to manage the other required CUDA components such as cudnn and nccl (and the optional cupti). | I use two meta-pacakges, cxx-compiler and nvcc_linux-64, to make sure that suitable C, and C++ compilers are installed and that the resulting Conda environment is aware of the manually installed CUDA Toolkit. | Horovod requires some controller library to coordinate work between the various Horovod processes. Typically this will be some MPI implementation such as OpenMPI. However, rather than specifying the openmpi package directly I instead opt for mpi4py Conda package which provides a cuda-aware build of OpenMPI (where possible). | Horovod also support that Gloo collective communications library that can be used in place of MPI. I include cmake in order to insure that the Horovod extensions for Gloo are built. | Below are the core required dependencies. The complete environment.yml file is available on GitHub. . dependencies: - bokeh=1.4 - cmake=3.16 # insures that the Gloo library extensions will be built - cudnn=7.6 - cupti=10.1 - cxx-compiler=1.0 # meta-pacakge that insures suitable C and C++ compilers are available - jupyterlab=1.2 - mpi4py=3.0 # installs cuda-aware openmpi - nccl=2.5 - nodejs=13 - nvcc_linux-64=10.1 # meta-package that configures environment to be &quot;cuda-aware&quot; - pip=20.0 - pip: - mxnet-cu101mkl==1.6.* # makes sure MXNET is installed prior to horovod - -r file:requirements.txt - python=3.7 - pytorch=1.4 - tensorboard=2.1 - tensorflow-gpu=2.1 - torchvision=0.5 . The requirements.txt File . The requirements.txt file is where all of the pip dependencies, including Horovod itself, are listed for installation. In addition to Horovod I typically will also use pip to install JupyterLab extensions to enable GPU and CPU resource monitoring via jupyterlab-nvdashboard and Tensorboard support via jupyter-tensorboard. . horovod==0.19.* jupyterlab-nvdashboard==0.2.* # server-side component; client-side component installed in postBuild jupyter-tensorboard==0.2.* # make sure horovod is re-compiled if environment is re-built --no-binary=horovod . Note the use of the --no-binary option at the end of the file. Including this option insures that Horovod will be re-built whenever the Conda environment is re-built. . The complete requirements.txt file is available on GitHub. . Building Conda Environment . After adding any necessary dependencies that should be downloaded via conda to the environment.yml file and any dependencies that should be downloaded via pip to the requirements.txt file you create the Conda environment in a sub-directory ./envof your project directory by running the following commands. . export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force . By default Horovod will try and build extensions for all detected frameworks. See the Horovod documentation on environment variables for the details on additional environment variables that can be set prior to building Horovod. . Once the new environment has been created you can activate the environment with the following command. . conda activate $ENV_PREFIX . The postBuild File . If you wish to use any JupyterLab extensions included in the environment.yml and requirements.txt files then you need to rebuild the JupyterLab application using the following commands to source the postBuild script. . conda activate $ENV_PREFIX # optional if environment already active . postBuild . Wrapping it all up in a Bash script . I typically wrap these commands into a shell script ./bin/create-conda-env.sh. Running the shell script will set the Horovod build variables, create the Conda environment, activate the Conda environment, and built JupyterLab with any additional extensions. . #!/bin/bash --login set -e export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force conda activate $ENV_PREFIX . postBuild . I typically put scripts inside a ./bin directory in my project root directory. The script should be run from the project root directory as follows. . ./bin/create-conda-env.sh # assumes that $CUDA_HOME is set properly . Verifying the Conda environment . After building the Conda environment you can check that Horovod has been built with support for the deep learning frameworks TensorFlow, PyTorch, Apache MXNet, and the contollers MPI and Gloo with the following command. . conda activate $ENV_PREFIX # optional if environment already active horovodrun --check-build . You should see output similar to the following. . Horovod v0.19.1: Available Frameworks: [X] TensorFlow [X] PyTorch [X] MXNet Available Controllers: [X] MPI [X] Gloo Available Tensor Operations: [X] NCCL [ ] DDL [ ] CCL [X] MPI [X] Gloo . Listing the contents of the Conda environment . To see the full list of packages installed into the environment run the following command. . conda activate $ENV_PREFIX # optional if environment already active conda list . Updating the Conda environment . If you add (remove) dependencies to (from) the environment.yml file or the requirements.txt file after the environment has already been created, then you can re-create the environment with the following command. . conda env create --prefix $ENV_PREFIX --file environment.yml --force . However, whenever I add new dependencies I prefer to re-run the Bash script which will re-build both the Conda environment and JupyterLab. . ./bin/create-conda-env.sh . Summary . Finding a reproducible process for building Horovod extensions for my deep learning projects was tricky. Key to my solution is the use of meta-packages from conda-forge to insure that the appropriate compilers are installed and that the resulting Conda environment is aware of the system installed NVIDIA CUDA Toolkit. The second key is to use the --no-binary flag in the requirements.txt file to insure that Horovod is re-built whenever the Conda environment is re-built. . If you like my approach then you can make use of the template repository on GitHub to get started with your next Horovod data science project! .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "relUrl": "/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Managing Project-Specific Environments With Conda",
            "content": "Getting Started with Conda . Conda is an open source package and environment management system that runs on Windows, Mac OS and Linux. . Conda can quickly install, run, and update packages and their dependencies. | Conda can create, save, load, and switch between project specific software environments on your local computer. | Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN. | . Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. . Conda? Miniconda? Anaconda? What’s the difference? . Users are often confused about the differences between Conda, Miniconda, and Anaconda. . . I suggest installing Miniconda which combines Conda with Python 3 (and a small number of core systems packages) instead of the full Anaconda distribution. Installing only Miniconda will encourage you to create separate environments for each project (and to install only those packages that you actually need for each project!) which will enhance portability and reproducibility of your research and workflows. . Besides, if you really want a particular version of the full Anaconda distribution you can always create an new conda environment and install it using the following command. . conda create --name anaconda202002 anaconda=2020.02 . Installing Miniconda . Download the 64-bit, Python 3 version of the appropriate Miniconda installer for your operating system from and follow the instructions. I will walk through the steps for installing on Linux systems below as installing on Linux systems is slightly more involved. . Download the 64-bit Python 3 install script for Miniconda. . wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . Run the Miniconda install script. The -b runs the install script in batch mode which doesn’t require manual intervention (and assumes that the user agrees to the terms of the license). . bash Miniconda3-latest-Linux-x86_64.sh -b . Remove the install script. . rm Miniconda3-latest-Linux-x86_64 . Initializing your shell for Conda . After installing Miniconda you next need to configure your preferred shell to be “conda-aware”. . conda init bash source ~/.bashrc (base) $ # now the prompt indicates that the base environment is active! . Updating Conda . It is a good idea to keep your conda installation updated to the most recent version. . conda update --name base conda --yes . Uninstalling Miniconda . Whenever installing new software it is always a good idea to understand how to uninstall the software (just in case you have second thoughts!). Uninstalling Miniconda is fairly straighforward. . Uninitialize your shell to remove Conda related content from ~/.bashrc. . conda init --reverse bash . Remove the entire ~/miniconda3 directory. . rm -rf ~/miniconda3 . Remove the entire ~/.conda directory. . rm -rf ~/.conda . If present, remove your Conda configuration file. . rm ~/.condarc . Conda “Best Practices” . In the following section I detail a minimal set of best practices for using Conda to manage data science environments that I use in my own work. . TLDR; . Here is the basic recipe for using Conda to manage a project specific software stack. . (base) $ mkdir project-dir (base) $ cd project-dir (base) $ nano environment.yml # create the environment file (base) $ conda env create --prefix ./env --file environment.yml (base) $ conda activate ./env # activate the environment (/path/to/env) $ nano environment.yml # forgot to add some deps (/path/to/env) $ conda env update --prefix ./env --file environment.yml --prune) # update the environment (/path/to/env) $ conda deactivate # done working on project (for now!) . New project, new directory . Every new project (no matter how small!) should live in its own directory. A good reference to get started with organizing your project directory is Good Enough Practices for Scientific Computing. . mkdir project-dir cd project-dir . New project, new environment . Now that you have a new project directory you are ready to create a new environment for your project. We will do this in two steps. . Create an environment file that describes the software dependencies (including specific version numbers!) for the project. | Use the newly created environment file to build the software environment. | Here is an example of a typical environment file that could be used to run GPU accelerated, distributed training of deep learning models developed using PyTorch. . name: null channels: - pytorch - conda-forge - defaults dependencies: - cudatoolkit=10.1 - jupyterlab=1.2 - pip=20.0 - python=3.7 - pytorch=1.4 - torchvision=0.5 . Once you have created an environment.yml file inside your project directory you can use the following commands to create the environment as a sub-directory called env inside your project directory. . conda env create --prefix ./env --file environment.yml . Activating an environment . Activating environments is essential to making the software in environments work well (or sometimes at all!). Activation of an environment does two things. . Adds entries to PATH for the environment. | Runs any activation scripts that the environment may contain. | Step 2 is particularly important as activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. . conda activate ./env # activate the environment (/path/to/env) $ # now the prompt indicates which environment is active! . Updating an environment . You are unlikely to know ahead of time which packages (and version numbers!) you will need to use for your research project. For example it may be the case that… . one of your core dependencies just released a new version (dependency version number update). | you need an additional package for data analysis (add a new dependency). | you have found a better visualization package and no longer need to old visualization package (add new dependency and remove old dependency). | . If any of these occurs during the course of your research project, all you need to do is update the contents of your environment.yml file accordingly and then run the following command. . conda env update --prefix ./env --file environment.yml --prune # update the environment . Alternatively, you can simply rebuild the environment from scratch with the following command. . conda env create --prefix ./env --file environment.yml --force . Deactivating an environment . When you are done working on your project it is a good idea to deactivate the current environment. To deactivate the currently active environment use the deactivate command as follows. . conda deactivate # done working on project (for now!) (base) $ # now you are back to the base environment . Interested in Learning More? . For more details on using Conda to manage software stacks for you data science projects, checkout the Introduction to Conda for (Data) Scientists training materials that I have contributed to The Carpentries Incubator. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "relUrl": "/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}