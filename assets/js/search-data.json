{
  
    
        "post0": {
            "title": "Deep Q-Networks",
            "content": "!conda install --yes --channel pytorch pytorch=1.4 . Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /Users/pughdr/Research/stochastic-expatriate-descent/env added / updated specs: - pytorch=1.4 The following packages will be downloaded: package | build |-- certifi-2019.11.28 | py37_1 156 KB intel-openmp-2020.0 | 166 896 KB mkl-2020.0 | 166 93.5 MB ninja-1.9.0 | py37h04f5b5a_0 90 KB openssl-1.1.1f | h1de35cc_0 2.2 MB pytorch-1.4.0 | py3.7_0 34.5 MB pytorch Total: 131.3 MB The following NEW packages will be INSTALLED: intel-openmp pkgs/main/osx-64::intel-openmp-2020.0-166 mkl pkgs/main/osx-64::mkl-2020.0-166 ninja pkgs/main/osx-64::ninja-1.9.0-py37h04f5b5a_0 pytorch pytorch/osx-64::pytorch-1.4.0-py3.7_0 The following packages will be UPDATED: ca-certificates conda-forge::ca-certificates-2019.11.~ --&gt; pkgs/main::ca-certificates-2020.1.1-0 openssl conda-forge::openssl-1.1.1e-h0b31af3_0 --&gt; pkgs/main::openssl-1.1.1f-h1de35cc_0 The following packages will be SUPERSEDED by a higher-priority channel: certifi conda-forge::certifi-2019.11.28-py37h~ --&gt; pkgs/main::certifi-2019.11.28-py37_1 Downloading and Extracting Packages certifi-2019.11.28 | 156 KB | ##################################### | 100% openssl-1.1.1f | 2.2 MB | ##################################### | 100% intel-openmp-2020.0 | 896 KB | ##################################### | 100% pytorch-1.4.0 | 34.5 MB | ##################################### | 100% mkl-2020.0 | 93.5 MB | ##################################### | 100% ninja-1.9.0 | 90 KB | ##################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done . import torch from torch import nn from torch.nn import functional as F . https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf . Use deep convolutional neural network to approximate that optimal action-value function . $$ Q^*(s, a) = max_{ pi} mathbb{E} Bigg[ sum_{s=0}^{ infty} gamma^s r_{t+s} | s_t=s, a_t=a, pi Bigg] $$ . which is the maximum sum of rewards $r_t$ discounted by $ gamma$ at each time-step $t$ achievable by a behaviour policy $ pi = P(a|s)$, after making an observation of the state $s$ and taking an action $a$. . Reinforcement learning is known to be unstable or even to diverge when a non-linear function approximator such as a neural network is used to represent the $Q$ function. Why? . Correlations present in the sequence of observations of the state $s$. In reinforcement learning applications the sequence state observations is a time-series which will almost surely be auto-correlated. But surely this would also be true of any application of deep neural networks to model time series data. | Small updates to $Q$ may significantly change the policy, $ pi$ and therefore change the data distribution. | Correlations between the action-values, $Q$, and the target values $r + gamma max_{a&#39;} Q(s&#39;, a&#39;)$ | In the paper they address these issue by... . A biologically inspired mechanism they refer to as experience replay that randomizes over the data which removes correlations in the sequence of observations of the state $s$ and smoothes over changes in the data distribution (issues 1 and 2 above). | Using an iterative update rule that adjusts the action-values, $Q$, towards target values that are only periodically updated, thereby reducing correlations with the target (issue 3 above). | . Approximating the state-action function, $Q(s,a)$ . They parameterize an approximate value function $Q(s,a; theta_i)$ using the deep convolutional neural network shown in the figure below. Note that $ theta_i$ are the parameters or weights of the $Q$-network at iteration $i$. . . def create_deep_q_network(state_size: int, action_size: int) -&gt; nn.Module: deep_q_network = nn.Sequential( nn.Conv2d(in_channels=state_size, out_channels=32, kernel_size=8, stride=4), nn.ReLU(), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1), nn.ReLU(), nn.Linear(in_features=64, out_features=512), nn.ReLU(), nn.Linear(in_features=512, out_features=action_size) ) return deep_q_network . Experience Replay . To perform experience replay the authors store the agent&#39;s experiences $e_t$ as represented by the tuple . $$ e_t = (s_t, a_t, r_t, s_{t+1}) $$ . consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences. . $$ D_t = {e1, e2, ..., e_t } $$ . Depending on the task it may note be feasible for the agent to store the entire history of past experiences. . During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s&#39;)$, drawn uniformly at random from the pool of stored samples $D_t$. . The Loss Function . The $Q$-learning update at iteration $i$ uses the following loss function . $$ mathcal{L_i}( theta_i) = mathbb{E}_{(s, a, r, s&#39;) sim U(D)} Bigg[ bigg(r + gamma max_{a&#39;} Q big(s&#39;, a&#39;; theta_i^{-} big) - Q big(s, a; theta_i big) bigg)^2 Bigg] $$ . where $ gamma$ is the discount factor determining the agent’s horizon, $ theta_i$ are the parameters of the $Q$-network at iteration $i$ and $ theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $ theta_i^{-}$ are only updated with the Q$-network parameters $ theta_i$ every $C$ steps and are held fixed between individual updates. . expected_q_network = create_deep_q_network(state_size=4, action_size=4) target_q_network = create_deep_q_network(state_size=4, action_size=4) expected_q_values = ??? target_q_values = ??? loss_fn = F.mse_loss(expected_q_values, target_q_values) . AttributeError Traceback (most recent call last) &lt;ipython-input-14-f483aa612782&gt; in &lt;module&gt; 2 target_q_network = create_deep_q_network(state_size=4, action_size=4) 3 -&gt; 4 loss_fn = F.mse_loss(expected_q_network, target_q_network) ~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction) 2201 See :class:`~torch.nn.MSELoss` for details. 2202 &#34;&#34;&#34; -&gt; 2203 if not (target.size() == input.size()): 2204 warnings.warn(&#34;Using a target size ({}) that is different to the input size ({}). &#34; 2205 &#34;This will likely lead to incorrect results due to broadcasting. &#34; ~/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/torch/nn/modules/module.py in __getattr__(self, name) 574 return modules[name] 575 raise AttributeError(&#34;&#39;{}&#39; object has no attribute &#39;{}&#39;&#34;.format( --&gt; 576 type(self).__name__, name)) 577 578 def __setattr__(self, name, value): AttributeError: &#39;Sequential&#39; object has no attribute &#39;size&#39; .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html",
            "relUrl": "/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Conda (+ pip) and Docker FTW!",
            "content": "Conda (+ pip) and Docker FTW! . Conda is an open source package and environment management system that runs on Windows, Mac OS and Linux. . Conda can quickly install, run, and update packages and their dependencies. | Conda can create, save, load, and switch between project specific software environments on your local computer. | Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN. | . Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. . While Conda is my default package and environment management solution, not every Python package that I might need to use is available via Conda. Fortunately, Conda plays nicely with pip which is the default Python package management tool. . Why not just use Conda (+ pip)? . While Conda (+ pip) solves most of my day-to-day data science environment and package management issues, incorporating Docker into my Conda (+ pip) development workflow has made it much easier to port my data science workflows from from my laptop/workstation to remote cloud computing resources. Getting Conda (+ pip) to work as expected inside Docker containers turned out to be much more challenging that I expected. . This blog post shows how I eventually combined Conda (+ pip) and Docker. In the following I assume that you have organized your project directory similar to my Python data science project template. In particular, I will assume that you store all Docker related files in a docker sub-directory within your project root directory. . Writing the Dockerfile . The trick to getting Conda (+ pip) and Docker to work smoothly together is to write a good Dockerfile. In this section I will take you step by step through the various pieces of the Dockerfile that I developed. Hopefully you can use this Dockerfile without modification on you next data science project. . Use a standard base image . Every Dockefile has a base or parent image. For the parent image I use Ubuntu 16.04 which is one of the most commonly used flavor of Linux in the data science community (and also happens to be the same OS installed on my workstation). . FROM ubuntu:16.04 . Make bash the default shell . The default shell used to run Dockerfile commands when building Docker images is /bin/sh. Unfortunately /bin/sh is currently not one of the shells supported by the conda init command. Fortunately it is possible to change the default shell used to run Dockerfile commands using the SHELL instruction. . SHELL [ &quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot; ] . Note the use of the --login flag which insures that both ~/.profile and ~/.bashrc are sourced properly. Proper sourcing of both ~/.profile and ~/.bashrc is necessary in order to use various conda commands to build the Conda environment inside the Docker image. . Create a non-root user . It is a Docker security “best practice” to create a non-root user inside your Docker images. My preferred approach to create a non-root user uses build arguments to customize the username, uid, and gidthe non-root user. I use standard defaults for the uid and gid; the default username is set to al-khawarizmi. . # Create a non-root user ARG username=al-khawarizmi ARG uid=1000 ARG gid=100 ENV USER $username ENV UID $uid ENV GID $gid ENV HOME /home/$USER RUN adduser --disabled-password --gecos &quot;Non-root user&quot; --uid $UID --gid $GID --home $HOME $USER . Copy over the config files for the Conda environment . After creating the non-root user I copy over all of the config files that I will need to create the Conda environment (i.e., environment.yml, requirements.txt, postBuild). I also copy over a Bash script that I will use as the Docker ENTRYPOINT (more on this below). . COPY environment.yml requirements.txt /tmp/ RUN chown $UID:$GID /tmp/environment.yml /tmp/requirements.txt COPY postBuild /usr/local/bin/postBuild.sh RUN chown $UID:$GID /usr/local/bin/postBuild.sh &amp;&amp; chmod u+x /usr/local/bin/postBuild.sh COPY docker/entrypoint.sh /usr/local/bin/ RUN chown $UID:$GID /usr/local/bin/entrypoint.sh &amp;&amp; chmod u+x /usr/local/bin/entrypoint.sh . Newer versions of Docker support copying files as a non-root user, however the version of Docker available on DockerHub does not yet support copying as a non-root user so if you want to setup automated builds for your Git repositories you will need to copy everything as root. . Install Miniconda as the non-root user. . After copying over the config files as root, I switch over to the non-root user and install Miniconda. . USER $USER # install miniconda ENV MINICONDA_VERSION 4.8.2 ENV CONDA_DIR $HOME/miniconda3 RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-$MINICONDA_VERSION-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; chmod +x ~/miniconda.sh &amp;&amp; ~/miniconda.sh -b -p $CONDA_DIR &amp;&amp; rm ~/miniconda.sh # make non-activate conda commands available ENV PATH=$CONDA_DIR/bin:$PATH # make conda activate command available from /bin/bash --login shells RUN echo &quot;. $CONDA_DIR/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.profile # make conda activate command available from /bin/bash --interative shells RUN conda init bash . Create a project directory . Next I create a project directory inside the non-root user home directory. The Conda environment will be created in a env sub-directory inside the project directory and all other project files and directories can then be mounted into this directory. . # create a project directory inside user home ENV PROJECT_DIR $HOME/app RUN mkdir $PROJECT_DIR WORKDIR $PROJECT_DIR . Build the Conda environment . Now I am ready to build the Conda environment. Note that I can use nearly the same sequence of conda commands that I would use to build a Conda environment for a project on my laptop or workstation. . # build the conda environment ENV ENV_PREFIX $PWD/env RUN conda update --name base --channel defaults conda &amp;&amp; conda env create --prefix $ENV_PREFIX --file /tmp/environment.yml --force &amp;&amp; conda clean --all --yes # run the postBuild script to install any JupyterLab extensions RUN conda activate $ENV_PREFIX &amp;&amp; /usr/local/bin/postBuild.sh &amp;&amp; conda deactivate . Insure Conda environment is properly activated at runtime . Almost finished! Second to last step is to use an ENTRYPOINT script to insure that the Conda environment is properly activated at runtime. . ENTRYPOINT [ &quot;/usr/local/bin/entrypoint.sh&quot; ] . Here is the /usr/local/bin/entrypoint.sh script for reference. . #!/bin/bash --login set -e conda activate $ENV_PREFIX exec &quot;$@&quot; . Specify a default command for the Docker container . Finally, I use the CMD instruction to specify a default command to run when a Docker container is launched. Since I install JupyerLab in all of my Conda environments I tend to launch a JupyterLab server by default when executing containers. . # default command will be to launch JupyterLab server for development CMD [ &quot;jupyter&quot;, &quot;lab&quot;, &quot;--no-browser&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot; ] . Building the Docker image . The following command builds a new image for your project with a custom $USER (and associated $UID and $GID) as well as a particular $IMAGE_NAME and $IMAGE_TAG. This command should be run within the docker sub-directory of the project as the Docker build context is set to ../ which should be the project root directory. . docker image build --build-arg username=$USER --build-arg uid=$UID --build-arg gid=$GID --file Dockerfile --tag $IMAGE_NAME:$IMAGE_TAG ../ . Running a Docker container . Once the image is built, the following command will run a container based on the image $IMAGE_NAME:$IMAGE_TAG. This command should be run from within the project’s root directory. . docker container run --rm --tty --volume ${pwd}/bin:/home/$USER/app/bin --volume ${pwd}/data:/home/$USER/app/data --volume ${pwd}/doc:/home/$USER/app/doc --volume ${pwd}/notebooks:/home/$USER/app/notebooks --volume ${pwd}/results:/home/$USER/app/results --volume ${pwd}/src:/home/$USER/app/src --publish 8888:8888 $IMAGE_NAME:$IMAGE_TAG . Using Docker Compose . It is quite easy to make typos whilst writing the above docker commands by hand. A less error-prone approach is to use Docker Compose. The above docker commands can be encapsulated into the docker-compose.yml configuration file as follows. . version: &quot;3.7&quot; services: jupyterlab-server: build: args: - username=${USER} - uid=${UID} - gid=${GID} context: ../ dockerfile: docker/Dockerfile ports: - &quot;8888:8888&quot; volumes: - ../bin:/home/${USER}/app/bin - ../data:/home/${USER}/app/data - ../doc:/home/${USER}/app/doc - ../notebooks:/home/${USER}/app/notebooks - ../results:/home/${USER}/app/results - ../src:/home/${USER}/app/src init: true stdin_open: true tty: true . The above docker-compose.yml file relies on variable substitution. to obtain the values for $USER, $UID, and $GID. These values can be stored in an a file called .env as follows. . USER=$USER UID=$UID GID=$GID . You can test your docker-compose.yml file by running the following command in the docker sub-directory of the project. . docker-compose config . This command takes the docker-compose.yml file and substitutes the values provided in the .env file and then returns the result. . Once you are confident that values in the .env file are being substituted properly into the docker-compose.yml file, the following command can be used to bring up a container based on your project’s Docker image and launch the JupyterLab server. This command should also be run from within the docker sub-directory of the project. . docker-compose up --build . When you are done developing and have shutdown the JupyterLab server, the following command tears down the networking infrastructure for the running container. . docker-compose down . Summary . In this post I walked through a Dockerfile that can be used to inject a Conda (+ pip) environment into into a Docker image. I also detailed how to build the resulting image and launch containers using Docker Compose. . If you are looking for a production-quality solution that generalizes the approach outlined above, then I would encourage you to check out jupyter-repo2docker. . jupyter-repo2docker is a tool to build, run, and push Docker images from source code repositories. repo2docker fetches a repository (from GitHub, GitLab, Zenodo, Figshare, Dataverse installations, a Git repository or a local directory) and builds a container image in which the code can be executed. The image build process is based on the configuration files found in the repository. . The Conda (+ pip) and Docker combination has significantly increased my data science development velocity while at the same time increasing the portability and reproducibility of my data science workflows. . Hopefully this post can help you combine these three great tools together on your next data science project! .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "relUrl": "/python/conda/docker/data-science/2020/03/31/poor-mans-repo2docker.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Building a Conda environment for Horovod",
            "content": "What is Horovod? . Horovod is an open-source distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Horovod improves the speed, scale, and resource utilization of deep learning training. . In this post I describe how I build Conda environments for my deep learning projects where I plan to use Horovod to enable distributed training across multiple GPUs (either on the same node or spread across multuple nodes). If you like my approach then you can make use of the template repository on GitHub to get started with you rnext Horovod data science project! . Installing the NVIDIA CUDA Toolkit . First thing you need to do is to install the appropriate version of the NVIDIA CUDA Toolkit on your workstation. For this blog post I am using NVIDIA CUDA Toolkit 10.1 (documentation) which works with all three deep learning frameworks that are currently supported by Horovod. . Why not just use the cudatoolkit package? . Typically when installing PyTorch, TensorFlow, or Apache MXNet with GPU support using Conda you simply add the appropriate version cudatoolkit package to your environment.yml file. . Unfortunately, the cudatoolkit package available from conda-forge does not include NVCC and in order to use Horovod with either PyTorch, TensorFlow, or MXNet you need to compile extensions. . What about the cudatoolkit-dev package? . While there are cudatoolkit-dev packages available from conda-forge that do include NVCC, I have had difficult getting these packages to consistently install properly. . Use the nvcc_linux-64 meta-pacakge! . The most robust approach to obtain NVCC and still use Conda to manage all the other dependencies is to install the NVIDIA CUDA Toolkit on your system and then install a meta-package nvcc_linux-64 from conda-forge which configures your Conda environment to use the NVCC installed on the system together with the other CUDA Toolkit components installed inside the Conda environment. . The environment.yml file . I prefer to specify as many dependencies as possible in the Conda environment.yml file and only specify dependencies in requirements.txt that are not available via Conda channels. Check the official Horovod installation guide for details of required dependencies. . Channel Priority . I use the recommended channel priorities. Note that conda-forge has priority over defaults. . name: null channels: - pytorch - conda-forge - defaults . Dependencies . There are a few things worth noting about the dependencies. . Even though I have installed the NVIDIA CUDA Toolkit manually I still use Conda to manage the other required CUDA components such as cudnn and nccl (and the optional cupti). | I use two meta-pacakges, cxx-compiler and nvcc_linux-64, to make sure that suitable C, and C++ compilers are installed and that the resulting Conda environment is aware of the manually installed CUDA Toolkit. | Horovod requires some controller library to coordinate work between the various Horovod processes. Typically this will be some MPI implementation such as OpenMPI. However, rather than specifying the openmpi package directly I instead opt for mpi4py Conda package which provides a cuda-aware build of OpenMPI (where possible). | Horovod also support that Gloo collective communications library that can be used in place of MPI. I include cmake in order to insure that the Horovod extensions for Gloo are built. | Below are the core required dependencies. The complete environment.yml file is available on GitHub. . dependencies: - bokeh=1.4 - cmake=3.16 # insures that the Gloo library extensions will be built - cudnn=7.6 - cupti=10.1 - cxx-compiler=1.0 # meta-pacakge that insures suitable C and C++ compilers are available - jupyterlab=1.2 - mpi4py=3.0 # installs cuda-aware openmpi - nccl=2.5 - nodejs=13 - nvcc_linux-64=10.1 # meta-package that configures environment to be &quot;cuda-aware&quot; - pip=20.0 - pip: - mxnet-cu101mkl==1.6.* # makes sure MXNET is installed prior to horovod - -r file:requirements.txt - python=3.7 - pytorch=1.4 - tensorboard=2.1 - tensorflow-gpu=2.1 - torchvision=0.5 . The requirements.txt File . The requirements.txt file is where all of the pip dependencies, including Horovod itself, are listed for installation. In addition to Horovod I typically will also use pip to install JupyterLab extensions to enable GPU and CPU resource monitoring via jupyterlab-nvdashboard and Tensorboard support via jupyter-tensorboard. . horovod==0.19.* jupyterlab-nvdashboard==0.2.* # server-side component; client-side component installed in postBuild jupyter-tensorboard==0.2.* # make sure horovod is re-compiled if environment is re-built --no-binary=horovod . Note the use of the --no-binary option at the end of the file. Including this option insures that Horovod will be re-built whenever the Conda environment is re-built. . The complete requirements.txt file is available on GitHub. . Building Conda Environment . After adding any necessary dependencies that should be downloaded via conda to the environment.yml file and any dependencies that should be downloaded via pip to the requirements.txt file you create the Conda environment in a sub-directory ./envof your project directory by running the following commands. . export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force . By default Horovod will try and build extensions for all detected frameworks. See the Horovod documentation on environment variables for the details on additional environment variables that can be set prior to building Horovod. . Once the new environment has been created you can activate the environment with the following command. . conda activate $ENV_PREFIX . The postBuild File . If you wish to use any JupyterLab extensions included in the environment.yml and requirements.txt files then you need to rebuild the JupyterLab application using the following commands to source the postBuild script. . conda activate $ENV_PREFIX # optional if environment already active . postBuild . Wrapping it all up in a Bash script . I typically wrap these commands into a shell script ./bin/create-conda-env.sh. Running the shell script will set the Horovod build variables, create the Conda environment, activate the Conda environment, and built JupyterLab with any additional extensions. . #!/bin/bash --login set -e export ENV_PREFIX=$PWD/env export HOROVOD_CUDA_HOME=$CUDA_HOME export HOROVOD_NCCL_HOME=$ENV_PREFIX export HOROVOD_GPU_ALLREDUCE=NCCL export HOROVOD_GPU_BROADCAST=NCCL conda env create --prefix $ENV_PREFIX --file environment.yml --force conda activate $ENV_PREFIX . postBuild . I typically put scripts inside a ./bin directory in my project root directory. The script should be run from the project root directory as follows. . ./bin/create-conda-env.sh # assumes that $CUDA_HOME is set properly . Verifying the Conda environment . After building the Conda environment you can check that Horovod has been built with support for the deep learning frameworks TensorFlow, PyTorch, Apache MXNet, and the contollers MPI and Gloo with the following command. . conda activate $ENV_PREFIX # optional if environment already active horovodrun --check-build . You should see output similar to the following. . Horovod v0.19.1: Available Frameworks: [X] TensorFlow [X] PyTorch [X] MXNet Available Controllers: [X] MPI [X] Gloo Available Tensor Operations: [X] NCCL [ ] DDL [ ] CCL [X] MPI [X] Gloo . Listing the contents of the Conda environment . To see the full list of packages installed into the environment run the following command. . conda activate $ENV_PREFIX # optional if environment already active conda list . Updating the Conda environment . If you add (remove) dependencies to (from) the environment.yml file or the requirements.txt file after the environment has already been created, then you can re-create the environment with the following command. . conda env create --prefix $ENV_PREFIX --file environment.yml --force . However, whenever I add new dependencies I prefer to re-run the Bash script which will re-build both the Conda environment and JupyterLab. . ./bin/create-conda-env.sh . Summary . Finding a reproducible process for building Horovod extensions for my deep learning projects was tricky. Key to my solution is the use of meta-packages from conda-forge to insure that the appropriate compilers are installed and that the resulting Conda environment is aware of the system installed NVIDIA CUDA Toolkit. The second key is to use the --no-binary flag in the requirements.txt file to insure that Horovod is re-built whenever the Conda environment is re-built. . If you like my approach then you can make use of the template repository on GitHub to get started with your next Horovod data science project! .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "relUrl": "/python/conda/deep-learning/pytorch/tensorflow/nvidia/horovod/2020/03/30/horovod-conda-env.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Managing Project-Specific Environments With Conda",
            "content": "Getting Started with Conda . Conda is an open source package and environment management system that runs on Windows, Mac OS and Linux. . Conda can quickly install, run, and update packages and their dependencies. | Conda can create, save, load, and switch between project specific software environments on your local computer. | Although Conda was created for Python programs, Conda can package and distribute software for any language such as R, Ruby, Lua, Scala, Java, JavaScript, C, C++, FORTRAN. | . Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. . Conda? Miniconda? Anaconda? What’s the difference? . Users are often confused about the differences between Conda, Miniconda, and Anaconda. . . I suggest installing Miniconda which combines Conda with Python 3 (and a small number of core systems packages) instead of the full Anaconda distribution. Installing only Miniconda will encourage you to create separate environments for each project (and to install only those packages that you actually need for each project!) which will enhance portability and reproducibility of your research and workflows. . Besides, if you really want a particular version of the full Anaconda distribution you can always create an new conda environment and install it using the following command. . conda create --name anaconda202002 anaconda=2020.02 . Installing Miniconda . Download the 64-bit, Python 3 version of the appropriate Miniconda installer for your operating system from and follow the instructions. I will walk through the steps for installing on Linux systems below as installing on Linux systems is slightly more involved. . Download the 64-bit Python 3 install script for Miniconda. . wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . Run the Miniconda install script. The -b runs the install script in batch mode which doesn’t require manual intervention (and assumes that the user agrees to the terms of the license). . bash Miniconda3-latest-Linux-x86_64.sh -b . Remove the install script. . rm Miniconda3-latest-Linux-x86_64 . Initializing your shell for Conda . After installing Miniconda you next need to configure your preferred shell to be “conda-aware”. . conda init bash source ~/.bashrc (base) $ # now the prompt indicates that the base environment is active! . Updating Conda . It is a good idea to keep your conda installation updated to the most recent version. . conda update --name base conda --yes . Uninstalling Miniconda . Whenever installing new software it is always a good idea to understand how to uninstall the software (just in case you have second thoughts!). Uninstalling Miniconda is fairly straighforward. . Uninitialize your shell to remove Conda related content from ~/.bashrc. . conda init --reverse bash . Remove the entire ~/miniconda3 directory. . rm -rf ~/miniconda3 . Remove the entire ~/.conda directory. . rm -rf ~/.conda . If present, remove your Conda configuration file. . rm ~/.condarc . Conda “Best Practices” . In the following section I detail a minimal set of best practices for using Conda to manage data science environments that I use in my own work. . TLDR; . Here is the basic recipe for using Conda to manage a project specific software stack. . (base) $ mkdir project-dir (base) $ cd project-dir (base) $ nano environment.yml # create the environment file (base) $ conda env create --prefix ./env --file environment.yml (base) $ conda activate ./env # activate the environment (/path/to/env) $ nano environment.yml # forgot to add some deps (/path/to/env) $ conda env update --prefix ./env --file environment.yml --prune) # update the environment (/path/to/env) $ conda deactivate # done working on project (for now!) . New project, new directory . Every new project (no matter how small!) should live in its own directory. A good reference to get started with organizing your project directory is Good Enough Practices for Scientific Computing. . mkdir project-dir cd project-dir . New project, new environment . Now that you have a new project directory you are ready to create a new environment for your project. We will do this in two steps. . Create an environment file that describes the software dependencies (including specific version numbers!) for the project. | Use the newly created environment file to build the software environment. | Here is an example of a typical environment file that could be used to run GPU accelerated, distributed training of deep learning models developed using PyTorch. . name: null channels: - pytorch - conda-forge - defaults dependencies: - cudatoolkit=10.1 - jupyterlab=1.2 - pip=20.0 - python=3.7 - pytorch=1.4 - torchvision=0.5 . Once you have created an environment.yml file inside your project directory you can use the following commands to create the environment as a sub-directory called env inside your project directory. . conda env create --prefix ./env --file environment.yml . Activating an environment . Activating environments is essential to making the software in environments work well (or sometimes at all!). Activation of an environment does two things. . Adds entries to PATH for the environment. | Runs any activation scripts that the environment may contain. | Step 2 is particularly important as activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. . conda activate ./env # activate the environment (/path/to/env) $ # now the prompt indicates which environment is active! . Updating an environment . You are unlikely to know ahead of time which packages (and version numbers!) you will need to use for your research project. For example it may be the case that… . one of your core dependencies just released a new version (dependency version number update). | you need an additional package for data analysis (add a new dependency). | you have found a better visualization package and no longer need to old visualization package (add new dependency and remove old dependency). | . If any of these occurs during the course of your research project, all you need to do is update the contents of your environment.yml file accordingly and then run the following command. . conda env update --prefix ./env --file environment.yml --prune # update the environment . Alternatively, you can simply rebuild the environment from scratch with the following command. . conda env create --prefix ./env --file environment.yml --force . Deactivating an environment . When you are done working on your project it is a good idea to deactivate the current environment. To deactivate the currently active environment use the deactivate command as follows. . conda deactivate # done working on project (for now!) (base) $ # now you are back to the base environment . Interested in Learning More? . For more details on using Conda to manage software stacks for you data science projects, checkout the Introduction to Conda for (Data) Scientists training materials that I have contributed to The Carpentries Incubator. .",
            "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "relUrl": "/python/conda/data-science/machine-learning/deep-learning/2020/03/29/getting-started-with-conda.html",
            "date": " • Mar 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://davidrpugh.github.io/stochastic-expatriate-descent/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}